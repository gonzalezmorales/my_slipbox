[{"id": "yYla7ZKp", "name": "Big Data", "labels": [], "text_md": "Big data consists of information assets characterized by:\n\n- High volume\n- High velocity\n- High variety\n\nProjects that involve a massive amounts of data that cannot be handled by local machines require to host applications and databases on the cloud. \n\n", "links": ["vKQFuLMb"], "quotes": [], "sources": [], "n_links": 1}, {"id": "zNXU0RHP", "name": "Directed graphs (digraphs)", "labels": [], "text_md": "Directed graphs have at least one directed edge \"parent of\". \n\n", "links": ["MV7Xi5dH", "2fx14QVS"], "quotes": [], "sources": [], "n_links": 2}, {"id": "Mv4Ukj5Q", "name": "Network formation: transitive closure", "labels": [], "text_md": "Connect two nodes together if they are already connected to a common neighbor\n\n", "links": ["ifY62dRd"], "quotes": [], "sources": [], "n_links": 1}, {"id": "ifY62dRd", "name": "Network formation: preferential attachment", "labels": [], "text_md": "Mechanism that leads to complex network formation, whereby nodes with more edges get even more edges, forming large hubs in the core, surrounded by poorly connected nodes in the periphery. \n\n", "links": ["4xQthRYr", "MV7Xi5dH", "Mv4Ukj5Q"], "quotes": [], "sources": [], "n_links": 3}, {"id": "4xQthRYr", "name": "Simple vs Complex networks", "labels": [], "text_md": "- Simple networks have regular nodes and edges. \n- Complex networks have a non-trivial structure that often results from decentralized processes with no central control.\n\n", "links": ["MV7Xi5dH", "ifY62dRd"], "quotes": [], "sources": [], "n_links": 2}, {"id": "wJHcEcFj", "name": "Multigraph", "labels": [], "text_md": "Unidirected graphs that can have multiple edges between the same nodes. Parallel edges can represent different types of relationships between the nodes.\n\n", "links": ["kvpUkOPt", "t4mv3eQD"], "quotes": [], "sources": [], "n_links": 2}, {"id": "5uEpgy6B", "name": "NetworkX", "labels": [], "text_md": "Python library for complex network analysis, including construction, exploration, analysis, measurement an visualization of networks.\n\nhttps://networkx.org/documentation/stable/index.html\n\n\n", "links": ["MV7Xi5dH"], "quotes": [], "sources": [], "n_links": 1}, {"id": "MV7Xi5dH", "name": "Network analysis", "labels": [], "text_md": "Exploring quantitative relationships in networks with non-trivial structure.\n\nThe study of complex networks did not start until the late 1800s/ early 1900s due to lack of proper mathematical apparatus (graph theory) and adequate computational tools.\n\n", "links": ["5uEpgy6B", "zNXU0RHP", "4xQthRYr", "ifY62dRd", "t4mv3eQD", "BaBoinzs", "p6ocJHTv"], "quotes": [], "sources": [], "n_links": 7}, {"id": "BaBoinzs", "name": "Network analysis: Modularity", "labels": [], "text_md": "Modularity measures the strength of division of a network into clusters (modules). A network that is partitioned into non-overlapping modules is said to have high modularity if the connections between the nodes within modules are dense, while  the  connections between nodes in across modules are sparse.\n\nIt is defined as the fraction of the edges of a network that fall within the given modules, minus the expected fraction if edges were distributed at random.\n\n", "links": ["MV7Xi5dH", "p6ocJHTv"], "quotes": [], "sources": [], "n_links": 2}, {"id": "p6ocJHTv", "name": "Network analysis: Degree", "labels": [], "text_md": "The degree of a node in a network is the number of immediate neighbors (adjacent nodes).  It is a non-negative integer number. \n\n", "links": ["MV7Xi5dH", "BaBoinzs"], "quotes": [], "sources": [], "n_links": 2}, {"id": "2lr4gGNA", "name": "Meeting management", "labels": [], "text_md": "Meetings can go off tracks due to unanticipated displays of passion from a team member.\n\n\n", "links": ["fOMXYZho"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 1}, {"id": "fOMXYZho", "name": "Conflict management", "labels": [], "text_md": "- Depersonalize conflict, focusing on the disagreement issue rather than on the interested parties. \n- Look beyond the merits of an issue and understand the interests, feasrs, aspirations, and loyalties of the factions around it.\n\n", "links": ["2lr4gGNA", "jwCdelDE", "h1xZIuYx", "PjMcjUgb"], "quotes": [], "sources": ["PoAa8poE"], "n_links": 4}, {"id": "h1xZIuYx", "name": "Investing in people", "labels": [], "text_md": "\"To effectively reenergize their workforces, organizations need to shift their emphasis from getting more out of people to investing more in them, so they are motivated--and able--to bring more of themselves to work every day.\n\nEmployees reward organizations that treat them humanely, fairly and with dignity.  In contrast, loyalty to the organization erodes not only among employees that are treated inhumanely, unfairly or without respect, but also among their colleagues who witness such behavior.  This can lead to serious talent retention problems. \n\n", "links": ["fOMXYZho", "PjMcjUgb"], "quotes": [], "sources": ["iH9FlknB"], "n_links": 2}, {"id": "9JJd01er", "name": "Taking care of oneself during a crisis", "labels": [], "text_md": "Taking care of oneself, both physically and emotionally, is crucial to be able to lead a team's response to a crisis. This includes:\n\n- Finding sanctuaries to reflect on events and regain perspective\n- Reaching out to confidants to articulate own reasons for taking certain actions\n- Bringing more of one's own emotional self to the workplace\n\n", "links": ["84OVqhJD"], "quotes": [], "sources": ["PoAa8poE"], "n_links": 1}, {"id": "84OVqhJD", "name": "Leading in times of trauma", "labels": [], "text_md": "In times of trauma, leaders of an organization can help staff channel their desire to help and get back into their normal routine by fostering their pride in the organization and what they do, and providing context for grieving, meaning and action. \n\n1. *Context for grieving*: Create an environment where people can freely express and discuss the way they feel, and seek or provide comfort\n\n2.  *Context for meaning*:  Communicate and reinforce organizational values, reminding people about the larger purpose of their work and helping make sense of the pain\n\n3. *Context for action*: Create an environment where those who experience or witness pain can imagine a more hopeful future and find ways to alleviate their own and other's suffering.\n\n\n\n", "links": ["9JJd01er", "SRFvQdH5", "bay5lBgW"], "quotes": [], "sources": ["xHjQSgNc"], "n_links": 3}, {"id": "TlYLFIPL", "name": "Product Owner role", "labels": [], "text_md": "The product owner is responsible to manage the backlog of items that form the requirements and the shared understanding of the product's problem and solution.  \n\n", "links": ["47pZOC1r", "QBMWnA4E", "3ETVkGDf"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 3}, {"id": "QyoZwcP9", "name": "Managing own's time and effort", "labels": [], "text_md": "\"It is no fun to find ourselves with too little time and too many deliverable, rushing headlong into panic mode while the hours ebb dep into the night.\" (Goldfedder 2020)\n\nTrying to manage time is a fools errand when it is objectively impossible to fit everything into the limited amount of time that is available in a day, no matter how efficient one is.  \n\nTime is a limited resource and should be allocated in alignment of one's own personal definition of success.\n\nRecognize and excel in what really counts, and aim for less than perfect in everything else.  \n\n- Decline invites to tactical meetings \n- Reduce your involvement in committees\n- Focus on strengths instead of trying to shore up your weaknesses\n\n", "links": ["2uCsbhU6", "qvRxZspp", "HhsYzH6I"], "quotes": [], "sources": ["3iiZ05w1,", "VJ2nXM1R"], "n_links": 3}, {"id": "2uCsbhU6", "name": "Results of bad planning", "labels": [], "text_md": "\"Typical results of bad planning are missed deadlines, unrecognized overhead, and, more often than not, ... desperate acts of last-minute heroism that take on a weirdly optimistic tone (\"I did it and with only three ours of sleep in the last two days!\") \"\n\n\n\n", "links": ["QyoZwcP9"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 1}, {"id": "HhsYzH6I", "name": "Project management: establishing a rhythm", "labels": [], "text_md": "\"Establishing a rhythm is how teams [members] can maintain awareness of each other's workloads, upcoming decisions, and decision outcomes\"\n\nSchedule specific types of meetings at specific intervals:\n\n- **Weekly action meeting** \n  - *Objective:* Unblock the work and set the team up to get their work done for the next seven days.\n  - *Agenda:*\n    1. Check in - Everyone answers: \"What has your attention right now?\"\n    2. Checklist review - Everyone answers \"Yes/No\" to each checklist item\n    3. Metrics review - The team reviews the metrics to see whether we are on the right path\n   4. Project updates - \"What has changed in the project since last week?\"\n   5. Build agenda - Everyone adds topics to the agenda by calling out a placeholder word or phrase.  The team focuses on topics that will unblock the work in the coming week\n   6. Process agenda - Work through the agenda focusing on unblocking work for everyone\n   7. Checkout - Everyone answers: \"What did you notice?\"\n- **Monthly retrospect meeting** \n  - *Objective:* To look back on the past month and discuss necessary changes in the way the team works together.\n\n", "links": ["94L4EBHO", "QyoZwcP9"], "quotes": [], "sources": ["gMK50Cdd"], "n_links": 2}, {"id": "MkZau9Uc", "name": "Waterfall project management", "labels": [], "text_md": "The waterfall model assumes the project can be divided in a linear sequence of phases (requirements analysis, design, implementation, testing, deployment, maintenance).  \n\nIt works well in situations where the requirements and task to be performed in order to meet those requirements are clearly defined in advance. \n\n", "links": ["MVQ6AEt4", "94L4EBHO"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 2}, {"id": "zDox7KV4", "name": "Data migration patterns", "labels": [], "text_md": "**\"Kill and fill\"**: First delete existing data in the target system, and then  replace it with new data \n\n", "links": ["NPfQeP5J"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 1}, {"id": "DdmZ5wq7", "name": "Data integration challenges", "labels": [], "text_md": "The simple task of mobing data from A to B \"can turn quite messy very fast and without warning\". \n\n- Lack of standardization of ETL solutions and tools\n- Mismatch between data from source system, existing ETL pipelines, and target system schema.\n- Insufficient, incomplete, or poorly communicated metadata\n- Data quality issues\n\nNeed to research, document and test inbound data\n\n", "links": ["NPfQeP5J", "OX8Uv2GF"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 2}, {"id": "5sMdUJ5Y", "name": "\"The easiest solution is probably the best\"", "labels": [], "text_md": "Oftentimes spending a week manually entering data could be more efficient that writing code to automate a source-to-target solution for a one off data migration task.  \n\n", "links": ["JExDAHlM", "TBRkKxJ7"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 2}, {"id": "TBRkKxJ7", "name": "Paralysis by analysis", "labels": [], "text_md": "\"When the fear of either making an error, or foregoing a superior solution, outweighs the realistic expectation or potential value of success in a decision made in a timely manner\"\n\n\"When overanalyzing or overthinking a situation prevents progress or decision-making\"\n\n\"The more numerous our options, the more difficult it becomes to choose a single one... and so we end up choosing none at all.\"(P. Bergman, 2010)\n\nParalysis by analysis is common when:\n- Dealing with complex systems and high uncertainty.\n- There is an overload of information from numerous sources \n- There is too little knowledge leads to lack of confidence on the part of team members\n- Vast knowledge and expertise increases the number of options and considerations that appear at every decision point. \n- Excessive focus is placed on perfection and completeness of the analysis phase.\n- The goals and timeframe of the analysis phase are not clearly defined and the expectations surrounding the deliverables are fuzzy.\n- Design and implementation issues are introduced into the analysis phase.\n\nWikipedia: \"In software development, analysis paralysis typically manifests itself through the Waterfall model with exceedingly long phases of project planning, requirements gathering, program design and data modeling, which can create little or no extra value by those steps and risk many revisions.\"\n\n\"Agile software development methodologies explicitly seek to prevent analysis paralysis, by promoting an iterative work cycle that emphasizes working products over product specifications,\"\n\n\n", "links": ["MVQ6AEt4", "94L4EBHO", "JExDAHlM", "5sMdUJ5Y"], "quotes": [], "sources": ["qXafgygU,https://en.wikipedia.org/wiki/Analysis_paralysis,https://www.businessanalystlearnings.com/blog/2014/2/10/managinganalysisparalysis"], "n_links": 4}, {"id": "liKE24lR", "name": "Regularity/Frequency of data migration tasks", "labels": [], "text_md": "- **One-time migration:**\n  An existing data source is moved once into a target destination, often after performing several transformations and tests through code or scripting.\n- **Nightly integration**:\n  Data from external system is loaded on a regular basis (e.g., every night)  into the target system.  This approach is common in data warehouses. \n- **Near real time integration:**\n    Listener services allow for integration to be triggered the moment data is input into the source system \n- **Hypbrid approach**\n\nNightly and real-time integration require the automation of the ETL task, which assumes that the content and structure of the source system is predictable and that exceptions can be adequately handled. \n\n", "links": ["NPfQeP5J", "1cHc8MiO", "4HybrQML", "TvEdCbEX", "OX8Uv2GF"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 5}, {"id": "NPfQeP5J", "name": "Data integration", "labels": [], "text_md": "Data integration is the act of making data from disparate source systems align and conform to common standards, combining and presenting them in a unified view to users, so they are readily available for further value creation. Getting the raw data from various sources ready for modelling and analysis is usually 80 percent of the work.\n\nA key deliverable from a data integration effort is a set of source-to-target mappings (STTM).\n\nData integration is a critical task which should be tackled early in a data analysis project, way before development work begins.\n\nData integration relies on different patterns, including data replication, data virtualization, and ETL, which may or may not involve the physical transfer of data between storage systems. \n\nSuccessful data integration requires a thorough analysis of the overall system architecture, data stores, and business requirements. \n\nGood integration design is repeatable.  It also must include mechanisms for handling exceptions and monitoring the results of data integration tasks (e.g., providing users with regular success and error counts)\n\n", "links": ["DdmZ5wq7", "liKE24lR", "d3ru9pGK", "68VjKPZb", "uKiS7iJH", "1cHc8MiO", "62wtBu4C", "zDox7KV4", "sKuMe1ca", "4HybrQML", "QSfNE2W5", "Aslc8kRG", "OX8Uv2GF"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 13}, {"id": "MJyDsMeW", "name": "Interrupt-driven environments", "labels": [], "text_md": "In an \"interrupt-driven\" environment, teams are not able to reliably plan work beyond one week or so--it's not possible to know what the work will be that far into the future.  At any given moment there is a high probability or receiving a high-priority request that will force to re-assign resources that had already been committed to other tasks.\n\n", "links": ["6bvsiUUz", "eac4gaf6", "Nwuk7TBb"], "quotes": [], "sources": ["lxJxVFm9"], "n_links": 3}, {"id": "lCHnOEAE", "name": "Managing projects that are at risk of failure", "labels": [], "text_md": "Projects at risk of failing are actually very common in most organizations, but most of them recover when action is taken.  \n\n**Why projects become \"at risk\"**\n\n- Requirements (poorly defined, not prioritized, contradictory or not agreed upon)\n- Resources (not available, tied up in conflicts, or not properly planned for)\n- Schedules (unrealistic).\n- Planning (based on incomplete data, bad estimates, and insufficiently detailed).\n- Risks (not properly identified or measured)\n\n**Critical determinants of project recovery**\n\nThe project manager has a large impact on whether a project at risk of failing ultimately succeeds or not, since she or he decides which root causes to address and which actions to undertake.\n\nHaving a standardized project management methodology significantly reduces the likelihood of project failure in an organization. \n\n**Most frequent interventions that lead to successful recovery**\n\n- Improve communication\n- Redefine the scope or business case of the project\n- Right-size resources (add if too little, reduce is too many resources had been allocated)\n- Address technical issues trough in-depth analysis\n- Shift talent (e.g. replace project manager or bring in a consultant)\n\n**Steps to recovery**\n\n1. Review the project's history, purpose, goals, assumptions, and team composition\n2. Facilitate team communication about issues and root causes (without finger pointing)\n3. Address incentives and verify that stakeholders are still vested in the project\n4. Assess tradeoffs between time, cost and scope, subject to reputation, quality and value constraints\n5. Obtain buy in from stakeholders into the proposed solution\n6. Restart the project by communicating learnings from past mistakes, updated plan, roles and responsibilities\n\n\n", "links": ["SRFvQdH5", "bay5lBgW"], "quotes": [], "sources": ["PhyiqCiv"], "n_links": 2}, {"id": "QBMWnA4E", "name": "Scrum", "labels": [], "text_md": "A people-centric framework for organizing and managing work, which promotes frequent and meaningful collaboration based on mutual trust and empowerment of team members. \n\nScrum is well suited for operating in a complex environment, as it empowers self-organizing teams to explore and adapt.  It is particularly useful to manage innovation and new product development projects.\n\nScrum includes 3 main roles:\n\n1. Product owner\n2. Scrum master\n3. Development team\n\nActivities:\n\n- Sprint planning\n- Sprint execution -- including daily scrum\n- Sprint review\n- Sprint retrospective\n- Product backlog grooming\n\nArtifacts:\n \n- Product backlog\n- Sprint backlog\n\nRules\n\n\n\n", "links": ["Nwuk7TBb", "TlYLFIPL", "94L4EBHO", "J4RjLkh4", "PjMcjUgb"], "quotes": [], "sources": ["lxJxVFm9"], "n_links": 5}, {"id": "Nwuk7TBb", "name": "Kanban", "labels": [], "text_md": "Kanban is an approach focused on measuring and visualizing the flow of work through the system in order to identify opportunities for continuous, gradual improvement.  It emphasizes the elimination of over-burden and the reduction of variability in workflows.  \n\nIn contrast to scrum, kanban is well suited to manage \"interrupt-driven\" work environments. The idea is to monitor the \"work in process\" (WIP) at each step, ensuring that teams are not doing more work than they have the capacity to do.  \n\n", "links": ["MJyDsMeW", "6bvsiUUz", "QBMWnA4E", "eac4gaf6"], "quotes": [], "sources": ["lxJxVFm9"], "n_links": 4}, {"id": "sA6Ay1UI", "name": "Network analysis: Snowball sampling", "labels": [], "text_md": "Method for discovering all the nodes and edges of interest, starting from a \"seed\" node, using a breadth-first algorithm.\n\n1. Start with a seed\n2. Then the seed's neighbors\n3. Then the neighbors' neighbors\n4.  ...\n\nThe algorithm must remember which nodes have been already processed and which ones have been discovered but not yet processed (done/to do), keeping track of the distance from the node being currently processed to the seed. \n\n\n\n", "links": ["bzURsW8Z"], "quotes": [], "sources": [], "n_links": 1}, {"id": "IF5MS5t2", "name": "Data matching", "labels": [], "text_md": "Process of finding records in one or more datasets that refer to the same entity.  This process can be deterministic (by matching unique identifiers) or probabilistic (based on some measure of similarity between field values that provide partial identification). \n\n", "links": ["OX8Uv2GF"], "quotes": [], "sources": [], "n_links": 1}, {"id": "oBtYWEfG", "name": "Data profiling", "labels": [], "text_md": "Process of reviewing source data to discover its structure, content and relationships. It involves:\n\n- Calculating descriptive statistics (e.g., min, max, count, sum...).\n- Collecting data types, length and recurring patterns.\n- Tagging data with keywords, descriptions or categories.\n- Assessing data quality.\n- Discovering metadata and assessing its accuracy.\n- Identifying key candidates, foreign-key candidates, functional dependencies, embedded value dependencies, \n- Performing inter-table analysis.\n- Document connection/access details\n  - If the data is available as text file, is it comma delimited, tab-delimited, or something else?\n  - If the data is available from a database, what are the connection details?\n  - If the data is available through a web service, what are the API endpoints and in what is the serialization format of the response message?\n  - What are the authentication mechanisms granting access to the data?\n\n\n**Data profiling good practices**\n\n1. Conduct data profiling at project start to discover if data is suitable for analysis\u2014and make a \u201cgo / no go\u201d decision.\n2. Identify and correct data quality issues in source data, even before starting to move it into target database.\n3. Identify data quality issues that can be corrected by Extract-Transform-Load (ETL), while data is moved from source to target. \n4. Identify any additional manual processing that might be needed.\n5. Identify unanticipated business rules, hierarchical structures and foreign key / private key relationships, use them to fine-tune the ETL process.\n\n**Basic data profiling techniques**\n\n- Distinct count and percent\u2014identifies natural keys, distinct values in each column that can help process inserts and updates. \n- Percent of zero / blank / null values\u2014identifies missing or unknown data. \n- Minimum / maximum / average string length\u2014helps select appropriate data types and sizes in target database. \n\n**Advanced data profiling techniques**\n\n- Key integrity\u2014ensure keys are always present and identify orphan keys\n- Cardinality\u2014check for one-to-one, one-to-many, many-to-many relationships between related data sets. \n- Pattern and frequency distributions\u2014check if data fields are formatted correctly (e.g., emails, postal codes...)\n- Record matching\u2014Identify records that refer to the same entity by analyzing across fields that provide partial identification\n\n", "links": ["cTjLfkWN", "yHy01yYb", "Lr60zbkO", "OX8Uv2GF"], "quotes": [], "sources": ["VJ2nXM1R,https://panoply.io/analyticsstackguide/dataprofilingbestpractices/"], "n_links": 4}, {"id": "OX8Uv2GF", "name": "Components of a data integration process", "labels": [], "text_md": "**Target system** receive records from multiple **source systems**. \n\n**Data profiling** is used to discover and document the structure, content, and relationships among source data\n\n**Data matching** is used to identify records in different source systems that refer to the same entity\n\n**Source-to-target mapping** provides the metadata and transformation rules needed to convert data from each source system into the structure and content required by the target system.\n\n**Business rules for ETL processes** are criteria and conditions for transforming data from source to target, including exception handling \n\n**Testing source data** allows to discover inconsistencies with respect to ETL business rules\n\n**Version control system** records changes to ETL pipelines and data over time, allowing to recall specific versions at any given moment. \n\n", "links": ["IF5MS5t2", "NPfQeP5J", "oBtYWEfG", "DdmZ5wq7", "liKE24lR", "4HybrQML", "MhXSqPKk"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 7}, {"id": "jwCdelDE", "name": "Facilitating collaboration amongst data providers and users", "labels": [], "text_md": "Data integration activities require collaboration across providers and uses of data, both within and across organizations.  Each representative brings unique concerns and opinions, and different knowledge of the overall data ecosystem.  \"Everyone needs to be on the same page when it comes to build a data integration system\"\n\n- Ensure that data are well documented, consistent and of known quality\n- Harmonize the spatial and temporal scales over which data is collected\n- Agree on common metadata requirements\n- Identify tradeoffs and reach compromises regarding data formats, standards and conventions.\n\n", "links": ["uKiS7iJH", "fOMXYZho"], "quotes": [], "sources": [], "n_links": 2}, {"id": "4HybrQML", "name": "Data integration: Single Point of Truth", "labels": [], "text_md": "Single Point of Truth (SPOT) or Master Data Management (MDM) system allows to reconcile data from multiple systems into a single data hub, while allowing users to trace back these reconciled data assets to their original sources. \n\nSuch a system provides a stable access point for analysis work aimed to generate value and insights from a variety of multiple data assets. \n\n", "links": ["Iaba3RcP", "lEHhU8Ma", "CvXRU020", "NPfQeP5J", "liKE24lR", "62wtBu4C", "Aslc8kRG", "OX8Uv2GF"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 8}, {"id": "x97FBvWU", "name": "Managing a crisis communication programme", "labels": [], "text_md": "Managing a crisis communications program requires the same level of dedication and resources typically given to other functions. \n\nIn times of extreme crisis, internal communications take precedence. Key priorities include reestablishing communication with any groups of staff that are stranded or isolated, and rebuilding staff morale.\n\n", "links": ["SRFvQdH5", "YJqwphec", "bay5lBgW"], "quotes": [], "sources": ["m26C5gIR"], "n_links": 3}, {"id": "YJqwphec", "name": "Critical communication infrastructure", "labels": [], "text_md": "Heads of NSOs need to be able to communicate easily with their staff during the crisis.  Staff needs to have adequate connectivity, equipment and infrastructure, not only to be able to work from home, but also to stay in touch with colleagues and stay appraised of current developments. \n\n", "links": ["bay5lBgW", "SRFvQdH5", "hGt3as0b", "BQwQetnc", "xUr89DOT", "x97FBvWU"], "quotes": [], "sources": ["m26C5gIR"], "n_links": 6}, {"id": "hGt3as0b", "name": "Digital collaboration tools", "labels": [], "text_md": "Modern online collaboration tools are increasingly important for virtual and cross-functional teams to achieve greater levels of transparency, optimize resources and deliver results.\n\nThe selection of collaboration tools should be based on business needs and the budget available to the project or organization.\n\nTypical requirements include:\n\n- Team communication:\n  - Chat / instant messaging\n  - Screen sharing\n  - Audio/video conferencing\n  - Discussion forums\n- File and data sharing\n- Project management\n  - Planning\n  - Budget management\n  - Procurement management\n  - Contact management\n  - Calendar management\n  - Note taking\n  - Monitoring and evaluation\n- Code development / versioning\n- Content creation and management\n- Workflow management / automation\n\n", "links": ["yHIpNa5l", "YJqwphec", "rR6VySYD", "duUZSb3j"], "quotes": [], "sources": [], "n_links": 4}, {"id": "yHIpNa5l", "name": "Online collaboration tools", "labels": [], "text_md": "As remote working becomes the rule rather than the exception, modern online collaboration tools are increasingly important for teams to achieve greater levels of transparency, optimize resources and deliver results.\n\nThe selection of collaboration tools should be based on the organization's business needs and budget.\n\nMost online collaboration tools support more than one means of communication:\n- Chat / instant messaging\n- Email\n- Screen sharing\n- Audio/video conferencing\n\n\n\n", "links": ["hGt3as0b", "rR6VySYD"], "quotes": [], "sources": [], "n_links": 2}, {"id": "sKuMe1ca", "name": "Standardizing ETL tools and solutions", "labels": [], "text_md": "The adoption of different, often proprietary ETL tools for data integration in different departments of the same organization often leads to an increase in data silos.\n\nWhen \"one department of an organization, perhaps focusing on its own need, [adopts] a completely different ETL tool than another department[, the result is] an *increase* in data silos, rather than a decrease.\"\n\nOrganizations often work with multiple ETL solutions, each with its own framework, ... \"relegating the entire architecture into a big mess\"\n\n", "links": ["MfTvaMIL", "QSfNE2W5", "uKiS7iJH", "NPfQeP5J"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 4}, {"id": "rR6VySYD", "name": "Virtual teams", "labels": [], "text_md": "A virtual team is composed of members from different cultures and languages\nwho working remotely from one another.  They require intensive use of communication technology and virtual collaboration tools. \n\nThe early stages of a project are particularly challenging for virtual team, as the gathering of requirements often requires face-to-face conversations where body language and nuanced interpretation of voiced guidance are particularly important.   For instance, what would usually take a few minutes to cover in a face-to-face meeting, could take hours to explain in a series of emails or video conversations.  \n\nDelays in having questions answered may lead to a growing backlog.\n\n", "links": ["qEgpEYM3", "hGt3as0b", "BQwQetnc", "kfu8nyho", "yHIpNa5l", "7dELQecN", "lh9J280B"], "quotes": [], "sources": [], "n_links": 7}, {"id": "U3elusdN", "name": "Google Earth Engine", "labels": [], "text_md": "Combines a large catalogue of satellite imagery and geospatial datasets with GIS analysis capabilities to detect changes, map trends and quantify differences on Earth surface\n\n", "links": ["iJY1iH4F", "Cr014GcV"], "quotes": [], "sources": [], "n_links": 2}, {"id": "Ob6auJyZ", "name": "ASP.NET Core", "labels": [], "text_md": "Open source and cross-platform web application framework.  \n- Is useful to build backend applications that interact with databases such a SQL Server.  \n- Works well in cloud platforms such as Microsoft Azure\n\nIt is possible to use React to render our frontend and ASP.NET Core for the backend API.\n\n", "links": ["GRNZb1vP"], "quotes": [], "sources": ["NgZChe2t"], "n_links": 1}, {"id": "QSfNE2W5", "name": "Data integration team", "labels": [], "text_md": "Data integration tasks are a key component of every data innovation project.  One person cannot be responsible for an entire integration effort, including requirements, design, and deployment.  \n\nA data integration team should include the following **roles**:\n\n- Project sponsor: Provides direction and resources\n- Stakeholder: Individual or group with a vested interest in the success of the project\n- Subject matter expert: Individuals knowledgeable of the fundamental technical and/or business details of the day-to-day operations and requirements (their time is limited but precious)\n- Product owner: Main point of contact for all decisions related to the project\n- Project manager: Responsible for overseeing buget, schedule, deliverables and handover to customer\n- Data integration architect: Plans, designs and oversees the construction of data pipelines and tools that enable the movement of data from one point to another.  Works closely with ETL developers as their technical lead.\n- ETL developers: Team responsible of building ETL solutions that satisfy the technical requirements set by the data integration architect and ensure they are free of bugs, errors and defects.\n\nSince the data integration task spans across systems with multiple purposes, it is important to bring in experts from different domain areas, who understand day-to-day processes and know where \"data landmines\" are found. \n\nThe data integration team needs to include people with technical knowledge of different database management systems, cloud platforms, data formats and serializations (e.g., XML, JSON), and communication protocols (e.g. REST). \n\nIn addition, a successful data integration team requires strong communication and the ability to discuss, negotiate and compromise. \n\n\n", "links": ["sKuMe1ca", "NPfQeP5J", "uKiS7iJH", "jU3e4dIS", "MfTvaMIL"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 5}, {"id": "uKiS7iJH", "name": "Role of data integration architect", "labels": [], "text_md": "The integration architect works closely with ETL developers and stakeholder teams gathering and clarifying business and functional requirements, and conducts research to understand business rules and technical specifications of source and target systems. \n\nShe or he plays a key role in facilitating communication and common understanding among strategic and operational members of the broader data integration team.  After the discovery and testing phase, hand off work to developers. \n\n\n", "links": ["jwCdelDE", "NPfQeP5J", "jU3e4dIS", "d3ru9pGK", "68VjKPZb", "62wtBu4C", "sKuMe1ca", "QSfNE2W5", "irmKQb3k"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 9}, {"id": "jU3e4dIS", "name": "Role of data engineer", "labels": [], "text_md": "Data engineers needs a solid technical understanding of data integration and architecture modelling in order to:\n\n1. Develop **ETL pipelines** that are easy to understand, maintain, and replicate by others.  \n2. Implement **data storage and processing environments** that allow data scientists and other analysts to collaborate in the development and testing of estimation models. \n\n\n\n", "links": ["d3ru9pGK", "uKiS7iJH", "Vmp2TR2i", "QSfNE2W5", "irmKQb3k"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 5}, {"id": "bit4HVlF", "name": "Network model of knowledge management", "labels": [], "text_md": "A network model facilitates the management, creation, updating and diffusion of knowledge through digital cross-referencing.\n\n", "links": ["o2aK159Q", "pS33XHDA"], "quotes": [], "sources": [], "n_links": 2}, {"id": "f7nY4e4m", "name": "Outsourcing knowledge work", "labels": [], "text_md": "To understand what sorts of knowledge work can be outsourced, ask:\n\n- Can the task be easily specified and measured?\n- Is delay between value creation and consumption possible?\n- Can the task be done remotely?\n\n", "links": ["sO5SxY53"], "quotes": [], "sources": ["fyK87fjn"], "n_links": 1}, {"id": "G8wbe503", "name": "Content generation using smartphones", "labels": [], "text_md": "\"Smartphone-generated content may offer more ... accurate insights into consumer preferences.\"\n\n\"Tweets composed on smartphones [contain] higher proportions of first-person pronouns, references to family, ... negative emotional words [and display] a less-analytical writing style.\"\n\n\"People associate their smartphones with psychological comfort\"\n\n\n\n", "links": ["bHaKr9ga", "MMjEeFha"], "quotes": [], "sources": ["fyK87fjn"], "n_links": 2}, {"id": "t36XnVjf", "name": "Data security: the \"weakest link\" problem", "labels": [], "text_md": "'Security measures are often thwarted by the \"weakest link\" problem: If just one person responds to an attack, it may succeed.'\n\n", "links": ["odZoIUPn", "kfu8nyho"], "quotes": [], "sources": ["fyK87fjn"], "n_links": 2}, {"id": "odZoIUPn", "name": "Data security: Preventing phishing", "labels": [], "text_md": "\"Phishing accounts for 90% of all data breaches--but an estimated 30% of fraudulent emails are opened nonetheless.\"\n\nNeed to encourage a reflective, analytic approach to cyber security, instead of simply a rules-based training that may result in automatic, careless decision making. \n\n\"Pause if an email requests action; consider the nature, timing, purpose, and appropriateness of the request; and consult a third party about any suspicions.\"\n\n\n\n\n", "links": ["t36XnVjf"], "quotes": [], "sources": ["fyK87fjn"], "n_links": 1}, {"id": "JXu4aYg9", "name": "Impact of COVID-19 lockdown on knowledge workers' productivity", "labels": [], "text_md": "Due to the lockdown, day-to-day schedules of knowledge workers have changed and teams have developed new ways of working. \n\nThe results of a recent survey (Birkinshaw, Cohen and Stach, 2020) found that the COVID-19 lockdown has helped knowledge workers devote more time to interacting with customers and external partners, while reducing time spent on large meetings.  The survey also shows that, during lockdown, knowledge workers are being more intrinsically motivated and taking more personal ownership of their work.\n\nOn the negative side, working remotely makes it more difficult to kick off new projects, resolve internal conflicts, and focus on long-term staff development. \"While time spent on self-education went up...\", e.g., through webinars and online courses, this type of learning is does not encourage the same level of active experimentation and personal reflection as face-to-face interactions. \n\n", "links": ["88RTR3eB", "sO5SxY53"], "quotes": [], "sources": [], "n_links": 2}, {"id": "sO5SxY53", "name": "Knowledge workers", "labels": [], "text_md": "\"Knowledge workers apply subjective judgment to tasks, they decide what to do when, and they can withhold effort ... often without anyone noticing\"\n\nKnowledge workers should be evaluated on their outputs, instead of their inputs.\n\n", "links": ["f7nY4e4m", "JXu4aYg9"], "quotes": [], "sources": ["NFqBQQBi"], "n_links": 2}, {"id": "ePGUzfl7", "name": "Non-relational (No-SQL) databases", "labels": [], "text_md": "Non-relational (No-SQL) databases are \"scheme-agnostic\". They support the storage and manipulation of large volumes of unstructured and semi-structured data.  It is not necessary to specify in advance the types of data that will be stored in a No-SQL database, as it can accommodate changes in data types and data schemas. \n\nNo-SQL databases are designed to distribute data across different nodes; as a consequence, in many cases data consistency is not guaranteed. \n\nSome types of No-SQL databases include:\n\n1. *Graph databases*: They represent data as a network of related nodes, and are particularly suited to analyze relationships between heterogeneous data points.\n\n2. *Document stores*: They store data in XML and JSON format, using the document name as key and the contents of the document as value.   These documents can contain many different value types and can be nested.  They are particularly useful to manage semi-structured data across distributed systems.   Examples include *MongoDB* and *CouchBase*.\n\n3. *Wide-column stores*: This type of database store data in column families or tables.  They are designed to handle very large volumes of distributed data.   Examples include *Cassandra*, *Scylla*, and *HBase*.\n\n4. *Key-value stores*: They store only key-value pairs and provide basic functionality for retrieving the value associated with a known key.  They work best with a simple database schema and when speed is important.  Examples include: *Redis*, *DynamoDB*, *Cosmos*.\n\n", "links": ["TvEdCbEX", "hKNO8E7M", "WV9JV2CP", "Wo49hyYS", "Aslc8kRG"], "quotes": [], "sources": ["HYBQIQ60"], "n_links": 5}, {"id": "MVQ6AEt4", "name": "Project management: Agile vs Waterfall", "labels": [], "text_md": "In practice, a mix of waterfall and agile methodologies is needed. \n\n", "links": ["94L4EBHO", "d700Xe9w", "MkZau9Uc", "TBRkKxJ7"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 4}, {"id": "eeJS3CIE", "name": "Serverless data pipelines", "labels": [], "text_md": "Instead of setting up a data lake and a data warehouse themselves, some organizations are choosing to use managed cloud services from outside vendors for data storage and querying. \n\n", "links": ["vKQFuLMb", "9PrGSBNy", "ulLdtnHy", "dJBcDmmd"], "quotes": [], "sources": ["FuEhNdUH"], "n_links": 4}, {"id": "Wo49hyYS", "name": "Relational databases", "labels": [], "text_md": "Relational databases store data in tables with rows and columns. Each table has a pre-defined schema that strictly defines the number and type of attributes that can be stored in them, as well as the keys to identify and access specific rows.  Changes to the schema of a relational database are difficult and time-consuming, which makes them expensive to setup, maintain and grow. \n\nRelational database management systems (RDBMS)  use structured query language (SQL) statements to query and maintain the tables in a relational database.  \n\nRelational databases are optimized to ensure the relational integrity of the data, and to work with well known and well-structured datasets.  On the other hand, they are ill equipped to handle large, unstructured datasets. \n\n", "links": ["ePGUzfl7", "yerBxsCk", "hKNO8E7M", "WV9JV2CP", "6OomPFuc"], "quotes": [], "sources": ["HYBQIQ60"], "n_links": 5}, {"id": "zBMhHUQc", "name": "Apache Hadoop", "labels": [], "text_md": "Open source framework for distributed storage and processing of large datasets across clusters of computers. \n\nIt is often used in the implementation of data lake architectures. \n\nHadoop consists of four main modules:\n\n1. *Hadoop Distributed File System (HDFS)* \u2013 A distributed file system with high fault tolerance and native support of large datasets.\n2. *Yet Another Resource Negotiator (YARN)* \u2013 Used to manage and monitor cluster nodes and resource usage, and to schedule jobs and tasks.\n3. *MapReduce* \u2013 An implementation of the MapReduce programming model for large-scale parallel computation on data. The **map task** takes input data and converts it into a dataset that can be computed in key value pairs. The output of the map task is consumed by **reduce tasks** to aggregate output and provide the desired result.\n4. *Hadoop Common* \u2013 Common Java libraries\n\n", "links": ["sLFsKmFz", "TvEdCbEX"], "quotes": [], "sources": [], "n_links": 2}, {"id": "sLFsKmFz", "name": "Computer cluster", "labels": [], "text_md": "A network of computers that are connected and work together to accomplish the same task. \n\nWikipedia: \"A computer cluster is a set of loosely or tightly connected computers that work together so that, in many respects, they can be viewed as a single system. Unlike grid computers, computer clusters have each node set to perform the same task, controlled and scheduled by software.\"\n\nComputer clusters provide for improved performance, availability and scalability.\n\n", "links": ["zBMhHUQc"], "quotes": [], "sources": ["HYBQIQ60"], "n_links": 1}, {"id": "9PrGSBNy", "name": "Choosing cloud vendors", "labels": [], "text_md": "Choosing cloud service providers is a key strategic decision in setting up the architecture of a data science project.  This decision should consider a number of factors, including:\n\n- The types of data (structured and non-structured) and the volume of those data types that will need to be stored, accessed and analyzed\n- Whether data will be accessed and processed in batches or in real time\n- Whether storage and analytic capabilities are seamlessly integrated in the same cloud platform (e.g., the ability to deploy ML models directly on a cloud-based data warehouse). \n- Whether the cloud providers support of open-source technologies\n\n## Major cloud vendors include:\n\n- Google Cloud\n  - Storage:\n  - Machine learning: [BigQuery ML](https://cloud.google.com/bigquery-ml/docs/bigqueryml-intro)\n  - ...\n- Amazon Web Services\n  - Storage:\n  - Machine learning: [SageMaker](https://aws.amazon.com/sagemaker/)\n- Microsoft Azure\n  - Storage:\n  - Machine learning: \n- ArcGIS online\n  - Storage:\n  - Machine learning: \n\n", "links": ["eeJS3CIE", "dJBcDmmd", "ulLdtnHy", "Vmp2TR2i", "vKQFuLMb"], "quotes": [], "sources": ["HYBQIQ60"], "n_links": 5}, {"id": "dJBcDmmd", "name": "Cloud service models", "labels": [], "text_md": "Different cloud service models can be categorized into public/private/hybrid cloud and single-provider/multi-provider cloud. \n\n*Public cloud*\n\n- All hardware, software and supporting infrastructure is owned and managed by the cloude service provider\n- Cloud infrastructure is shared with other customers and accessed over the internet.\n\n*Private cloud*\n\n- Services and infrastructure are dedicated exclusively to one organization and maintained in a private network. \n- Can be on-premises or hosted by a third-party service provider\n- Often used by organizations with special security needs\n\n*Hybrid cloud*\n\n- Combines public and private clouds\n\nIn a multi-provider cloud infrastructure, different vendors provide specific cloud services. This approach is often used to avoid vendor lock-in.\n\n", "links": ["Vmp2TR2i", "eeJS3CIE", "vKQFuLMb", "9PrGSBNy"], "quotes": [], "sources": ["HYBQIQ60"], "n_links": 4}, {"id": "7UvCpDNd", "name": "Data pipeline requirements", "labels": [], "text_md": "Some important data pipeline requirements include:\n\n- **Low event latency**: Ability to query the data as soon as it has been collected\n- **Interactive querying**: Support both batch queries and smaller interactive queries allowing data scientists to explore the tables and schemas\n- **Scalability**: Handle increasing data volumes as the project scales\n- **Versioning**: Ability to make changes to the pipeline without bringing it down and losing data\n- **Monitoring**: Generate alerts when data stops coming into, or flowing through, the pipeline.\n- **Testing**: Ability to perform tests on the pipeline without interrupting the work of others\n- **Clear distinction between development and production environments**: The project should not interfere with daily business operations\n\n", "links": ["WnQYbC1N", "MhXSqPKk", "68VjKPZb"], "quotes": [], "sources": ["FuEhNdUH"], "n_links": 3}, {"id": "68VjKPZb", "name": "Architectural design for a data innovation project", "labels": [], "text_md": "At the beginning of a data innovation project, it is necessary design the architecture of the data pipelines and data storage and management environments that will allow the intake of raw data from multiple sources and push them through various processing steps, culminating with the delivery of analytic outputs to the final users.  \n\nThis architectural design involves making decisions regarding technologies, platforms, and data management and analysis tools, including the appropriate mix of on-premises and cloud infrastructure, depending on the type of data inputs and the different analysis and estimation methods that will be used in the project.  \n\nThe engineering of such data pipelines will need to cover both \"development\" and \"production\" environments, as well as a strategy for testing development operations before pushing them into production. \n\nMoreover, the data architecture should provide for data storage and data processing environments and tools that allow the members of cross-functional teams to collaborate in the development, testing, implementation and refinement of various data estimation methods.\n\nSome key requirements for the data architecture include:\n- Easy to maintain by the organization's own staff\n- Easy to replicate by people outside the project team\n\n\n", "links": ["YOp0ve5T", "WnQYbC1N", "NPfQeP5J", "yerBxsCk", "i7m8d4YQ", "uKiS7iJH", "WV9JV2CP", "7UvCpDNd", "MhXSqPKk", "3BmAWMzw", "Aslc8kRG"], "quotes": [], "sources": ["HYBQIQ60"], "n_links": 11}, {"id": "TvEdCbEX", "name": "Data Lakes", "labels": [], "text_md": "A data lake is designed to capture and store multiple sets of raw data at scale for a low cost.  It stores many different types of data in the same repository, and may also allow to perform transformations on the data. \n\nData lakes are designed to handle large volumes of streaming data and to make many different types of data readily available for analysis,  without having to move the data to a separate environment.   They tend to be more ephemeral (data is housed for the short-term) and generate outputs \n\nData lakes are setup without having to first define the data structure, following a **schema on read** principle. This means that  the structure of the data is only defined at the time it is used.   \n\nGiven the large variety of data that can be poured in them, **data lakes require a sound governance framework**, including clear data quality and metadata management protocols so they don't become \"data swamps\". \n\n", "links": ["zBMhHUQc", "WnQYbC1N", "ePGUzfl7", "yerBxsCk", "liKE24lR"], "quotes": [], "sources": ["HYBQIQ60,", "VJ2nXM1R"], "n_links": 5}, {"id": "yerBxsCk", "name": "Hybrid data warehouse / data lake architecture", "labels": [], "text_md": "Setting up both a data warehouse and a data lake allows to work with both structured/persistent and unstructured/ephemeral data in the analytics pipeline.\n\nThe general pattern is to store semi-structured data in a distributed database (the data lake) and run ETL processes to extract the most relevant data to an analytics database (the data warehouse).  But although most analysts interact with the data through the data warehouse, some may work in the data lake environment directly. \n\n", "links": ["Iaba3RcP", "CvXRU020", "lEHhU8Ma", "68VjKPZb", "WV9JV2CP", "Wo49hyYS", "TvEdCbEX"], "quotes": [], "sources": ["HYBQIQ60"], "n_links": 7}, {"id": "WV9JV2CP", "name": "Data storage layer in a data science project", "labels": [], "text_md": "Every data innovation project needs a clear strategy for saving and accessing data.  One of the first steps in a data science project is to design and build the data storage layer, consisting of one or several data warehouses and data lakes that allow to capture, organize and to access structured and un-structured data from multiple sources.\n\nThe data storage layer needs to support the analytic tools and technologies used by the data science team. \n\nThe choice of a data storage solution will determine, for instance, whether data scientists will need to unload the data before working on it, or whether they can perform in-database analytics.\n\nA data storage strategy needs to be based on:\n\n- Types of data that will be used in the project\n- Financial, technical and human resources available for the project\n- Existing data management processes and data storage infrastructure\n- Project priorities (i.e., speed, referential integrity, data security, backups...)\n\n", "links": ["Iaba3RcP", "WnQYbC1N", "CvXRU020", "ePGUzfl7", "yerBxsCk", "68VjKPZb", "Wo49hyYS"], "quotes": [], "sources": ["HYBQIQ60"], "n_links": 7}, {"id": "vKQFuLMb", "name": "The cloud", "labels": [], "text_md": "\"The cloud\" is the networks, services, systems and databases available to clients on the Internet to share, store, access and manipulate data.\n\nEnterprise systems are increasingly switching their focus from on-premise to cloud-based data repositories and processing capabilities. \n\n", "links": ["eeJS3CIE", "yYla7ZKp", "dJBcDmmd", "9PrGSBNy", "ulLdtnHy", "Vmp2TR2i", "MfTvaMIL"], "quotes": [], "sources": [], "n_links": 7}, {"id": "uVrsmP2n", "name": "Synchronous tasks", "labels": [], "text_md": "Instructions are executed immediately in order.  While each operation is being executed, nothing else is happening. \n\n", "links": ["N24sVqGE"], "quotes": [], "sources": [], "n_links": 1}, {"id": "Qbp1VWE4", "name": "Pure functions", "labels": [], "text_md": "A pure function returns a value that is computed based only on the function's arguments.  \n- Takes at least one argument\n- Treats its arguments as immutable data\n- Always returns a value or another function\n- Does not cause side effects\n- Does not set global variables or change anything about the application's state\n\nPure functions are naturally testable.\n\n", "links": ["v0j3xMCw"], "quotes": [], "sources": [], "n_links": 1}, {"id": "h6EOMFkP", "name": "Node.js and JavaScript projects", "labels": [], "text_md": "`Node.js` is an asynchronous, event-driven JavaScript runtime environment  designed to build scalable network applications.\n\nNode.js allows to use JavaScript in web-application development, for both server- and client-side scripts.\n\nTo make sure Node.js is intalled:\n`node -v`\n\n## npm\n\n`npm` is a package manager for the Node.js environment that makes it easier  to publish and share source code of Node.js packages, and is designed to simplify installation, updating, and uninstalling of packages.\n\n`package.json` is a file describing a JavaScript project and its dependencies.  When running `npm install` in the folder that contains the `package.json` file, npm will install all the packages listed in the project.  \n\n- `npm init -y` will initialize a new project and create the `package.json` file.\n- `npm install <package name>` will install a package\n- `npm remove <package name>` will remove a package\n\n### yarn\n\n`yarn` is an alternative to `npm`.  \n\n- `npm install -g yarn` will install yarn globally with npm:\n- `yarn` will install dependencies from `package.json`\n- `yarn add <package-name>`\n- `yarn remove <package-name>`\n\n\n\n", "links": ["WtKaAhds", "UhAj73Yn"], "quotes": [], "sources": [], "n_links": 2}, {"id": "V8p4H53X", "name": "Deconstructing JavaScript Objects", "labels": [], "text_md": "Technique that consists of pulling fields within an object to create local variables for them.\n\n", "links": ["WtKaAhds"], "quotes": [], "sources": [], "n_links": 1}, {"id": "b8UigmQs", "name": "React learning roadmap", "labels": [], "text_md": "1. JavaScript for React\n2. Building a user interface with components\n3. Adding logic with props and state\n4. Use of React Hooks - to reuse stateful logic between components\n\n", "links": ["WtKaAhds", "PLe99ZmC", "zC7DLmEh", "v0j3xMCw", "mJPzl1Wc"], "quotes": [], "sources": [], "n_links": 5}, {"id": "V9r8N6je", "name": "Function expressions in JavaScript", "labels": [], "text_md": "**Function expressions** create functions as a variable.  They are not hoisted -- cannot be invoked before being created. See (https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function#Function_declaration_hoisting)\n\nIn **JavaScript**, functions are \"first-class\" members. It is possible to declare functions with the `var`, `const`, or `let` keywords.  Functions can do the same things that variables can do.  Functions can be added to objects and arrays, sent to functions as arguments, and can be returned from functions.  \n\nNote:\n\n- With the `let` keyword, we can scope a variable to any code block, protecting the value of the global variable\n- The `const` keyword declares a variable that cannot be overwritten. Once declared, its value cannot be changed.\n\n\n", "links": ["WtKaAhds", "v0j3xMCw"], "quotes": [], "sources": [], "n_links": 2}, {"id": "WtKaAhds", "name": "JavaScript for React", "labels": [], "text_md": "- Arrays\n- Objects\n- Functions\n- Functional JavaScript\n\n", "links": ["h6EOMFkP", "v0j3xMCw", "GRNZb1vP", "V9r8N6je", "V8p4H53X", "b8UigmQs"], "quotes": [], "sources": [], "n_links": 6}, {"id": "RpDkqWDR", "name": "Babel - Tool for JavaScript compilation", "labels": [], "text_md": "Babel converts modern JavaScript code to mode widely compatible code before running it in the browser.  It makes possible to use the latest features of JavaScript right away.\n\nSee (https://babeljs.io/repl)\n\n\n", "links": ["UhAj73Yn", "GRNZb1vP"], "quotes": [], "sources": [], "n_links": 2}, {"id": "UhAj73Yn", "name": "React source code", "labels": [], "text_md": "Source code is the set of files that belong to a project that don't run in the browser. \n\n", "links": ["GfeBsLyO", "RpDkqWDR", "h6EOMFkP"], "quotes": [], "sources": [], "n_links": 3}, {"id": "N24sVqGE", "name": "Asynchronous tasks", "labels": [], "text_md": "Their execution does not block the main thread.  \n\n", "links": ["KG4OwoAJ", "YwXkKMRo", "uVrsmP2n"], "quotes": [], "sources": [], "n_links": 3}, {"id": "YwXkKMRo", "name": "JavaScript promises", "labels": [], "text_md": "Promises help make sense of asynchronous behavior in JavaScript.  \n\nA promise is an objects that represents whether an operation is pending, has been completed, or has failed.\n\n-  Simple promises are handled with `fetch` and `.then()`\n- `Asynch` / `Await` is a special syntax to work with promises in a more comfortable fashion.  \n\n", "links": ["N24sVqGE", "PimIKjSr"], "quotes": [], "sources": [], "n_links": 2}, {"id": "PimIKjSr", "name": "Higher-order functions", "labels": [], "text_md": "Higher-order functions are can take functions as arguments, return functions, or both.  They can help handle complexities associated with asynchronicity in JavaScript.\n\nExamples of higher-order functions in JavaScript:\n\n- `Array.map`\n- `Array.filter`\n- `Array.reduce`\n\n\n", "links": ["KG4OwoAJ", "YwXkKMRo", "v0j3xMCw"], "quotes": [], "sources": [], "n_links": 3}, {"id": "oDNGGpzd", "name": "Currying", "labels": [], "text_md": "Currying is a functional technique that involves the use of higher-order functions.\n\n\"A curried function is a function that takes multiple arguments one at a time. Given a function with 3 parameters, the curried version will take one argument and return a function that takes the next argument, which returns a function that takes the third argument. The last function returns the result of applying the function to all of its arguments.\"\n\n\n", "links": ["W0uZ0vW4"], "quotes": [], "sources": ["lZ2UCCUN"], "n_links": 1}, {"id": "KG4OwoAJ", "name": "Recursion", "labels": [], "text_md": "A technique that involve creating a function that recalls itself.\n\n- Often can be used instead of a loop\n- Works well with asynchronous processes (functions can recall themselves when they are ready).\n\n", "links": ["N24sVqGE", "W0uZ0vW4", "PimIKjSr"], "quotes": [], "sources": [], "n_links": 3}, {"id": "W0uZ0vW4", "name": "Function composition and chaining", "labels": [], "text_md": "Composition is the process of putting smaller, pure functions that perform specific tasks back together, to call them in parallel, or compose them into larger functions. \n\nThe 'dot' notation is often used to allow a function to act on the return value of the previous function. It allows to 'pipe' an input value through a sequence of functions.\n\n", "links": ["KG4OwoAJ", "zC7DLmEh", "oDNGGpzd", "v0j3xMCw"], "quotes": [], "sources": [], "n_links": 4}, {"id": "x7XU6ehK", "name": "DOM elements vs React DOM elements", "labels": [], "text_md": "A React DOM element is a description of what the actual DOM element should look like.  It provides instructions for the browser to create a DOM element. \n\nThe DOM API is a collection of objects that JavaScript can use to interact with the browser to modify the DOM.  With react, we do not interact with the DOM directly, but provide instructions for how we want React to build the DOM element.\n\n", "links": ["GRNZb1vP"], "quotes": [], "sources": [], "n_links": 1}, {"id": "wSIAvKsC", "name": "React - Routing", "labels": [], "text_md": "- Routing defines endpoints for the client's requests.  Each route is an endpoint that can be entered into the browser's location bar.\n- React does not come with a standard router\n- The browser's history API allows to change the browser's URL and handle changes in JavaScript. \n\n", "links": ["GRNZb1vP"], "quotes": [], "sources": ["NgZChe2t"], "n_links": 1}, {"id": "zC7DLmEh", "name": "React components", "labels": [], "text_md": "React components are parts that make up a user interface (e.g., buttons, lists, heading...).  They allow the re-use of the same structure, which can be populated many time with different sets of data.\n\nReact applications are data-driven. Components are \"vessels\" for data. The value of a React application depends on the data that flows through its components.\n\nA component is created by writing a function, which returns the re-usable part of a user interface.\n\n\n\n\n", "links": ["PLe99ZmC", "mJPzl1Wc", "GRNZb1vP", "GfeBsLyO", "W0uZ0vW4", "b8UigmQs"], "quotes": [], "sources": [], "n_links": 6}, {"id": "PLe99ZmC", "name": "React hooks", "labels": [], "text_md": "React hooks are a new way of add and share stateful logic across components. When data within the hook changes, they can cause a component to re-render in order to reflect the new data.\n\nHooks contain reusable code logic that is separate from the component tree.\n\nExamples of hooks include:\n\n- `useState`: \n\n  - The value sent to the `useState` function is the default variable of the state variable\n  - The first value of the return array is the state variable\n  - The second value of the return array is a function that can be used to change the state value\n\n\n", "links": ["zC7DLmEh", "b8UigmQs"], "quotes": [], "sources": [], "n_links": 2}, {"id": "mJPzl1Wc", "name": "State in React", "labels": [], "text_md": "The state of a React application is driven by data\n\n- Create stateful components\n- Send state down a component tree\n- Send user interactions back up the component tree\n\n", "links": ["zC7DLmEh", "mJPzl1Wc", "b8UigmQs"], "quotes": [], "sources": [], "n_links": 3}, {"id": "GRNZb1vP", "name": "React", "labels": [], "text_md": "React is a JavaScript library designed to build component-based, high-performing, single-page applications (front ends). It allows to write JavaScript code that looks like HTML and can update the browser DOM, using patterns that are readable, reusable and testable.  \n\nReact needs pre-processing to run in a browser (it needs a build tool like Webpack).\n\nThe major advantage of React is its ability to separate data from UI elements.\n\n", "links": ["WtKaAhds", "x7XU6ehK", "RpDkqWDR", "wSIAvKsC", "uys0AjRS", "zC7DLmEh", "GfeBsLyO", "Ob6auJyZ"], "quotes": [], "sources": [], "n_links": 8}, {"id": "uys0AjRS", "name": "Single-page applications", "labels": [], "text_md": "- The browser initially loads one HTML document.  \n- As users navigate through the site, they actually stay on the same page.\n- JavaScript is used to dynamically update the HTML page (User interfaca) as the user interacts with the application.\n\n\"After the first HTTP request that returns the single HTML page, subsequent HTTP requests are only for data and not HTML markup.\"\n\n\n", "links": ["GRNZb1vP"], "quotes": [], "sources": ["NgZChe2t"], "n_links": 1}, {"id": "GfeBsLyO", "name": "Bundling in React applications", "labels": [], "text_md": "Network performance is improved by having to load only one dependency in the browser: the \"bundle\"\n\nBy bundling all the dependencies into a single file, it is possible to load everything with a single HTTP request, avoiding additional latency\n\n`Webpack` is one of the leading React bundling tools.  It takes all different files (JavaScript, CSS, JSX...) and turns them into a single file for improved modularity and performance,\n\n", "links": ["ISfvwbKG", "UhAj73Yn", "zC7DLmEh", "GRNZb1vP"], "quotes": [], "sources": [], "n_links": 4}, {"id": "ISfvwbKG", "name": "Modularity", "labels": [], "text_md": "Breaking source code into parts (modules) that are easier to work with, especially in a team environment\n\n\n", "links": ["GfeBsLyO", "v0j3xMCw"], "quotes": [], "sources": [], "n_links": 2}, {"id": "hKNO8E7M", "name": "Database management systems (DBMS)", "labels": [], "text_md": "Database management systems support the creation, use and maintenance of databases.  They provide efficient data storage and retrieval, as well as tools for data acquisition, maintenance, formatting and dissemination.\n\n", "links": ["Wo49hyYS", "Aslc8kRG", "ePGUzfl7"], "quotes": [], "sources": ["oLNJUIaY"], "n_links": 3}, {"id": "MhXSqPKk", "name": "Managing ETL pipelines", "labels": [], "text_md": "A core component of any data innovation project is to set up a scalable data architecture, including a set of scalable ETL pipelines that move data from one system to another.  \n\nOne of the main roles of a data engineer is to design, build and run ETL processes (data pipelines) that send input data to data lakes, data warehouses and subscription services for use in the development/compilation of data products.\n\nKey questions that need to be investigated before deploying a data pipeline include:\n\n- Who owns the data pipeline?\n- Which teams will be consuming the data?\n- Who will monitor and maintain the the pipeline? (e.g., will the maintenance of the pipeline be under the responsibility of the data users, or will there be an infrastructure team keeping it operational?\n- What data pipeline and workflow automation tools are best suited to the purpose at hand?\n\n", "links": ["lEHhU8Ma", "d3ru9pGK", "68VjKPZb", "7UvCpDNd", "2GTwGKsm", "Vmp2TR2i", "OX8Uv2GF"], "quotes": [], "sources": ["FuEhNdUH"], "n_links": 7}, {"id": "xjldGfvp", "name": "Types of data moving through ETL pipelines", "labels": [], "text_md": "As data moves through an ETL pipeline, it typically is passes through various transformations, as follows:\n\n1. *Raw data*: Consists of input data encoded in its original format, without a having undergone any processing.  Often it consists of unstructured or semi-structured datasets stored in a file system or in a data lake.\n\n2. *Processed data*: Consists of raw data that has been decoded into specific formats, with a schema applied to it.  It is usually stored in different tables/destinations in the pipeline.\n\n3. *Cooked data*: Consists of processed data that has been aggregated or summarized.  \n\nData scientists typically work wirh processed data and use tools to create cooked data for other teams.\n\n", "links": ["lEHhU8Ma"], "quotes": [], "sources": ["FuEhNdUH"], "n_links": 1}, {"id": "WnQYbC1N", "name": "Stream vs batch ETL pipelines", "labels": [], "text_md": "There are 2 main types of data pipelines:\n\n1. **Stream**: Transactional data is passed along almost as soon as the transaction occurs. As soon as a new record is added into the source database, it\u2019s passed along into the analytical system. Creating and maintaining streaming systems is often more challenging.\n2. **Batch**: The pipeline runs at a specific time interval; data is not live, but is loaded in \"batches\".\n\n\n\n\n", "links": ["lEHhU8Ma", "d3ru9pGK", "68VjKPZb", "WV9JV2CP", "7UvCpDNd", "FOPuLnHI", "2GTwGKsm", "Vmp2TR2i", "2fx14QVS", "TvEdCbEX"], "quotes": [], "sources": [], "n_links": 10}, {"id": "Aslc8kRG", "name": "Data warehouse", "labels": [], "text_md": "Data warehouses are \"logically centralized data repositories where data from operational databases and other sources are integrated, cleaned up, standardized [and stored over the long run] to support business intelligence\".  They constitute \"central locations that data analysts ... can go to, to access all their data\"\n\nData warehouses implement data organization, access, and aggregation methods to support multidimensional views of the integrated data.  They are optimized for dealing with analytical queries (as opposed to transactional queries). In addition, they are designed for ease of understanding, so analysts can connect their analytic and visualization tools to them without having to invest much time understanding the underlying data structures. \n\nThe term Data Warehouse was coined by William Inmon in 1990, which he defined as \"a subject-oriented, integrated, time-variant and non-volatile collection of data in support of management's decision making process.\"   \n- *Subject Oriented:* Provides information about a particular subject.\n- *Integrated:* Merges data from multiple sources into a coherent whole.\n- *Time-variant:* Each data point refers to a specific time period.\n- *Non-volatile:* Data can be added but data is never removed from the warehouse. \n\n(Source: \"What is a Data Warehouse?\" W.H. Inmon, Prism, Volume 1, Number 1, 1995).\n\nData warehouses can run into problems with large volumnes of data -- they may need to truncate older data and keep only summary tables.  An query performance can be an issue.  Using a data warehouse database as the main interface for data is not optimal for some machine learning tools, since the data must be unloaded from the database before it can be operated on. \n\n", "links": ["Iaba3RcP", "CvXRU020", "YOp0ve5T", "ePGUzfl7", "NPfQeP5J", "hKNO8E7M", "68VjKPZb", "6OomPFuc", "4HybrQML", "3BmAWMzw"], "quotes": [], "sources": ["oLNJUIaY"], "n_links": 10}, {"id": "MUnZi5qo", "name": "Unique identifiers best practices", "labels": [], "text_md": "Identifiers are part of the basic data infrastructure of a country or organization. \nThey help structure and link data together.  They are *boundary objects*, i.e., information objects used in different ways by different communities.  Therefore, they enable cross-disciplinary work and collaboration. \n\nTo be useful, they need to be well documented. \n\nUnique identifiers should be:\n\n- unique\n- valid over the entire lifetime of entities\n- issued from a central authority\n- stored in all relevant systems / databases\n- **never** re-issued\n- assigned to all entities\n\n", "links": ["cTjLfkWN"], "quotes": [], "sources": ["onIU0wHP"], "n_links": 1}, {"id": "bzURsW8Z", "name": "Linked Open Data for statistical manuals and handbooks", "labels": [], "text_md": "The objective is to ensure that the meaning and structure of the different elements of methodological documents are available in machine readable form to software applications.\n\n", "links": ["OyHGHNbK", "sA6Ay1UI"], "quotes": [], "sources": [], "n_links": 2}, {"id": "SRFvQdH5", "name": "Importance of organization's mission, vision and values in times of crisis", "labels": [], "text_md": "In times of crisis, the most valuable assets of an organization are its people and its reputation.  An organization cannot start communicating its mission, vision and values during a crisis.  Staff will know what to do only if they have already internalized the organization's guiding principles.  \n\nA strong culture enables an organization to maintain focus in a time of crisis.\n\n\"Our principles will never fail us as lon as we do not fail to live up to them\" (Henry Paulson, Goldman Sachs CEO)\n\n", "links": ["lCHnOEAE", "bay5lBgW", "84OVqhJD", "YJqwphec", "x97FBvWU"], "quotes": [], "sources": ["m26C5gIR"], "n_links": 5}, {"id": "OyHGHNbK", "name": "Open access and interoperability", "labels": [], "text_md": "Providing open access to a dataset or a document is more than just allowing users to obtain a physical or electronic copy of it for their perusal; it requires that their contents be described and structured in a standardized manner, broken down into meaningful elements that can be read and correctly interpreted by different people and by multiple software applications.\n\n", "links": ["bzURsW8Z", "Gvu1WMA8", "cTjLfkWN"], "quotes": [], "sources": [], "n_links": 3}, {"id": "dva8i69T", "name": "Standard classifications", "labels": [], "text_md": "Standard classifications enable the integration of multiple data sets, as well as their consistent analysis and interpretations.  \n\nThey provide the taxonomical basis for managing and describing statistical data, and are fundamental components of key frameworks for the compilation of official statistics (e.g., System of National Accounts).\n\n", "links": ["Gvu1WMA8", "cTjLfkWN"], "quotes": [], "sources": [], "n_links": 2}, {"id": "Gvu1WMA8", "name": "Semantic interoperability", "labels": [], "text_md": "Semantic interoperability is about making visible and accessible to users (both humans and machines), in a standardized manner, the meaning of the various components of a dataset, document or information product, so as to enable the decentralized creation of knowledge and insights through high-value information services.\n\n", "links": ["OyHGHNbK", "dva8i69T"], "quotes": [], "sources": [], "n_links": 2}, {"id": "0zGb0Ntw", "name": "The 2030 Agenda for Sustainable Development", "labels": [], "text_md": "The 2030 Agenda for Sustainable Development is a call for action by all the member states of the United Nations to end poverty and other deprivations, improve health and education, empower women and girls, reduce inequalities, spur economic growth, strengthen universal peace in larger freedom, realize the human rights of all, tackle climate change, and preserve the environment for current and future generations.  \n\nIt recognizes the eradicating poverty as the greatest global challenge and as an indispensable requirement for sustainable development. \n\nThrough 17 Sustainable Development Goals and 169 targets, all member states committed to take bold and transformative steps to shift the world onto a sustainable and resilient path and to leave no one behind,\n\nThe 2030 Agenda cover 5 Areas of action:\n\n- **People:** End poverty and hunger, and ensure everyone can fulfill their potential in dignity and equality\n\n- **Planet:** Protect the planet from degradation, so it can support the needs of the present and future generations.\n\n- **Prosperity:** Ensure everyone's ability to enjoy prosperous and fulfilling lives and economic, social and technological progress in harmony with nature.\n\n- **Peace:** Foster peaceful, just and inclusive societies, free from fear and violence. \n\n- **Partnership:** Mobilize global solidarity among all people, countries, and stakeholders, focused particularly on the needs of the poorest and most vulnerable\n\n", "links": ["3QR1WCvN", "EojLU88D", "GQbIN4UB", "6SXNQUaG", "tnUhXJcI", "5u1sLb2p"], "quotes": [], "sources": [], "n_links": 6}, {"id": "6SXNQUaG", "name": "Inter-linkages across SDGs", "labels": [], "text_md": "The goals and targets of the 2030 Agenda constitute an \"integrated and indivisible\" whole. All the SDGs are interconnected, and each one of them is crucial for the well-being of individuals and societies.\n\nCross-sectoral coordination requires different actors to understand the complex interdependencies across different goals and targets that result form a multitude of trade-offs and synergies among the economic, social and environmental dimensions of sustainable development.\n\nIt is crucial to understand how does making progress made on a particular target reinforce or thwart progress on other targets. Holistic sustainable development solutions are those that maximize synergies and minimizing mutually off-setting effects between different SDGs.\n\n\n\n\n", "links": ["0zGb0Ntw", "62wtBu4C"], "quotes": [], "sources": [], "n_links": 2}, {"id": "62wtBu4C", "name": "Data integration for sustainable development", "labels": [], "text_md": "The integrated use of data and information from multiple sources (both traditional ones, such as census and survey programmes, and non-traditional ones, such as administrative records, satellite imagery, etc.) is crucial to enable all stakeholders to make smarter, evidence-based decisions towards achieving sustainable development.  \n\nThis includes the ability to effectively and efficiently bring together data assets which often have been developed in isolation from each other and only with a narrow set of operational or very short-term needs in mind, for their joint processing, analysis and dissemination throughout the data value chain.  \n\nMoreover, it includes the development of cross-sectoral reporting tools for tactical and strategic decision making and advocacy.\n\n\n\n", "links": ["CvXRU020", "NPfQeP5J", "jt99xW61", "5k2KowcP", "d3ru9pGK", "uKiS7iJH", "6SXNQUaG", "4HybrQML"], "quotes": [], "sources": [], "n_links": 8}, {"id": "3QR1WCvN", "name": "Sustainable development", "labels": [], "text_md": "Sustainable development is \"development that meets the needs of the present without compromising the ability of future generations to meet their own needs\". \n\n\n", "links": ["0zGb0Ntw"], "quotes": [], "sources": [], "n_links": 1}, {"id": "MvmET0Kx", "name": "Supporting local action and innovation", "labels": [], "text_md": "- Policies, budgets, institutions and regulatory frameworks by local governments \n- Local entrepreneurship\n- Community-based initiatives\n- Work of local research, academic and training institutions\n\n\n", "links": ["JiEdFdtg"], "quotes": [], "sources": [], "n_links": 1}, {"id": "NwHayaCP", "name": "Liberalism", "labels": [], "text_md": "Liberalism seeks to provide a way out of the continuous oscillation between authoritarianism and individualism.  It aims to establish a social order that is not built on the basis of irrational dogma, but at the same time, allows for a minimum of stability and order necessary for the preservation and functioning of the state. \n\n", "links": ["GH8W0S4g", "uIHekGyz"], "quotes": [], "sources": ["oabuUCA7"], "n_links": 2}, {"id": "zPySth0x", "name": "What is philosophy", "labels": [], "text_md": "Philosophy combines both scientific and ethical/religious points of view.  \n\nIt sits between science (knowledge) and theology (dogma).  On the one hand, it appeals to reason instead of authority or tradition; on the other, its speculates about things that are not know (or even knowable) with certainty.  \n\nPhilosophy asks questions that science cannot answer (at least not yet). But is does not accept the (unquestionable) answers provided by theology.\n\n", "links": ["V2qxeWls", "rJKUGkhR"], "quotes": [], "sources": ["oabuUCA7"], "n_links": 2}, {"id": "V2qxeWls", "name": "Why study philosophy?", "labels": [], "text_md": "Historian's perspective:\n\n- People's behavior is at least in part determined by their notions of good and evil. \n- \"To understand an era or a people, one has to understand their philosophy.\"\n- There is a two-way causality between human history and the history of philosophy; similarly, one's life situation determines one's own philosophy, and vice-versa.\n\nPersonal perspective:\n\n- Philosophy tries to teach us how to live without being paralyzed by the absence of certainty.\n\n", "links": ["zPySth0x", "uIHekGyz"], "quotes": [], "sources": ["oabuUCA7"], "n_links": 2}, {"id": "rJKUGkhR", "name": "Origins of Philosophy as a discipline", "labels": [], "text_md": "- Philosophy originated as a discipline independent of theology in Greece around the **6th century BC**, with Thales. In its origins, philosophy and science were not different from each other.\n- With the rise of Christianity and the **end of the Roman empire**, philosophy merged back into theology \n- Philosophy's second \"big era\" between the **11th and 14th centuries** under the rule of the catholic church, and ended with the Reformation.\n- It's third era, from the **17th century to date**, is more influenced by science and secular points of view, although traditional religious influences are still present. \n\n", "links": ["zPySth0x", "kSY5JM8j", "SLgwJQDz", "bawDrRBv", "Ov1RUbvE", "GH8W0S4g"], "quotes": [], "sources": ["oabuUCA7"], "n_links": 6}, {"id": "uIHekGyz", "name": "Individual freedom vs social cohesion", "labels": [], "text_md": "In human history, as in the history of philosophy, the pursuit of individual freedom has been always in conflict with the quest for social cohesion \n\n- In ancient Greece, social bonds where established, in varying degrees, through the loyalty and duties _of citizens_ towards the state. \n- After the Greeks where conquered by Rome, a more individualistic ethic emerged (e.g., the stoics identified virtue not so much in the relationship between citizens and the State, but between the soul and God).  \n- With Christianity, the idea became more widespread that an individual's duties towards God take precedence over those towards the state. \n- Liberalism provided for a clear separation between the public and the private spheres.  \n- This separation let, on one hand. to romanticism and individualism, and on the other to doctrines centered around the glorification of the state.\n\n\n", "links": ["GH8W0S4g", "V2qxeWls", "NwHayaCP", "SLgwJQDz"], "quotes": [], "sources": ["oabuUCA7"], "n_links": 4}, {"id": "GH8W0S4g", "name": "Locke", "labels": [], "text_md": "Locke opposed both individualism and the unmitigated submission of the individual to an absolute authority.  According to Russell, this eventually led to doctrines centered around the glorification of the state.\n\n", "links": ["uIHekGyz", "NwHayaCP", "rJKUGkhR"], "quotes": [], "sources": ["oabuUCA7"], "n_links": 3}, {"id": "SLgwJQDz", "name": "Conflict between church and state in the Middle Age", "labels": [], "text_md": "In the conflict between church and state that ensued between the end of the 5th century until middle of the 11th century, the former prevailed. The established philosophy of this era played a key role in this, since although  the secular centers of power kept the monopoly of violence and were not bound by a notion of legality, the authority of kings and barons of germanic descent was dependent on the loyalty and support of their feudal aristocracies, while the church had the monopoly on education and, more importantly, was believed to have the keys to everyone's eternal salvation or damnation.\n\n\n\n", "links": ["uIHekGyz", "bawDrRBv", "rJKUGkhR"], "quotes": [], "sources": ["oabuUCA7"], "n_links": 3}, {"id": "bawDrRBv", "name": "Renaissance and Reformation", "labels": [], "text_md": "The Renaissance and the Reformation destroyed the unity of Christianity around the pope's authority.\n- New knowledge about antique cultures became more widespread\n- Copernicus' astronomy gave Man and the Earth a different place in philosophy\n- The belief in eternal regularity was replaced by scientific inquiry, subjectivism and moral relativism\n- With Machiavelli, politics was understood as the naked pursuit of power.\n- The Reformation was a revolt of the Nordic kings and peoples against the pope's dominance.\n- The authority of the pope was countervailed not by one secular emperor, but by a multitude of nation states (and the strengthening of social bonds within them)\n- In the new protestant philosophy, there is no longer a single earthly intermediary between the soul and God\n- The truth does not come from an authority, but is grasped by each individual through reason\n- This lead to the advent of anarchism, mysticism. \n- There is no one protestant philosophy, but as many philosophies as there are philosophers.\n- Slowly, the seeds are planted for a re-emergence of individualism and pluralism.\n\n", "links": ["rJKUGkhR", "kSY5JM8j", "SLgwJQDz"], "quotes": [], "sources": ["oabuUCA7"], "n_links": 3}, {"id": "kSY5JM8j", "name": "Origins of modern philosophy: Descartes and subjectivism", "labels": [], "text_md": "Modern philosophy begins with Descartes, who attempts to derive the knowledge of the external world from the certain knowledge of the existence of one's self and one's own thoughts.\n\nThe resulting philosophical subjectivism--whose origins can be traced back to the protestant opposition to the authority of the pope--goes hand in hand with the emergence of liberalism and a more general opposition to all kinds of secular government and the gradual rise of political anarchism.\n\n", "links": ["rJKUGkhR", "bawDrRBv"], "quotes": [], "sources": ["oabuUCA7"], "n_links": 2}, {"id": "Ov1RUbvE", "name": "Eighteenth century's sentimentalism (\"Empfindsamkeit\") and romanticism", "labels": [], "text_md": "A deed is valued not against its good consequences or its alignment with moral principles, but against the intensity and \"authenticity\" of the (subjective) sentiments that engender it.  This leads to the romantic cult of heroism (Carlyle and Nietzsche) and passion (Byron).\n\nThe individual is no longer seen as a member of a community, but as an object of aesthetic contemplation.\n\n", "links": ["rJKUGkhR"], "quotes": [], "sources": ["oabuUCA7"], "n_links": 1}, {"id": "tnUhXJcI", "name": "The decade of action", "labels": [], "text_md": "In spite of progress made so far in many places, action towards achieving the SDGs by 2030 is not fast or comprehensive enough.  The **Decade of Action** launched by the UN Secreatry-General in September 2019 calls for accelerating sustainable solutions to the world\u2019s biggest challenges on three levels: \n\n- **Global action**: Secure leadership and resources\n- **Local action**: Design and implement policies, budgets, institutions and regulatory frameworks of governments, cities and local authorities\n- **People action**: Mobilize youth, civil society, the media, the private sector, unions, academia and other stakeholders\n\n", "links": ["0zGb0Ntw", "GQbIN4UB", "5u1sLb2p"], "quotes": [], "sources": ["Tn4oO9hv"], "n_links": 3}, {"id": "ClFwmtgA", "name": "Data visualization", "labels": [], "text_md": "Exploring data and analytic results in visual form helps identify and communicate conclusions in ways that are easily understood, shared and acted upon.\n\n", "links": ["3c7dZcB6", "etUj8mX6", "iJY1iH4F"], "quotes": [], "sources": ["XN9jt6q8"], "n_links": 3}, {"id": "GQbIN4UB", "name": "Role of multi-stakeholder partnerships", "labels": [], "text_md": "Multi-stakeholder partnerships are key to **mobilize support and action** around the 2030 Agenda for Sustainable Development.\n\n*SDG Targets on Multi-stakeholder partnerships:*\n\n- 17.16 Enhance the global partnership for sustainable development, complemented by multi-stakeholder partnerships that mobilize and share knowledge, expertise, technology and financial resources, to support the achievement of the sustainable development goals in all countries, in particular developing countries\n- 17.17 Encourage and promote effective public, public-private and civil society partnerships, building on the experience and resourcing strategies of partnerships\n\nThe [Partnerships for SDGs online platform](https://sustainabledevelopment.un.org/partnerships/) is a global registry of voluntary commitments and multi-stakeholder partnerships maintained by the United Nations to facilitate the global engagement of all stakeholders in the implementation of the 2030 Agenda.\n\n", "links": ["0zGb0Ntw", "tnUhXJcI"], "quotes": [], "sources": [], "n_links": 2}, {"id": "iJY1iH4F", "name": "Geospatial data analysis and visualization", "labels": [], "text_md": "Geospatial data analysis and map visualizations can help identify trends, patterns and relationships between different data sets over geographic space.  The analysis and visualization of geospatially disaggregated data \"enhances the ability to understand and respond to place-based factors\" affecting the phenomenon under study.  It also can lead to \"new questions or ideas about relationships in the data that can be further explored with additional methods\". \n\n", "links": ["ClFwmtgA", "etUj8mX6", "U3elusdN", "qpy6M6vh"], "quotes": [], "sources": ["XN9jt6q8"], "n_links": 4}, {"id": "tltgpLsL", "name": "Levels of disaggregation of administrative units", "labels": [], "text_md": "The hierarchy of political divisions of a country's territory is comprised by administrative units, each of which is delineated by specific geographic boundaries.\n\n- The country is the highest-level administrative unit, and it is referred to as the \"**Level 0**\" administrative unit.  \n- The first level of subdivision of administrative units within a country is referred to as '**Level 1**\".\n- Administrative units at further levels of geographic disaggregation are denoted as \"**Level 2**\", \"**Level 3**\", etc.\n\n\n\n", "links": ["cTjLfkWN", "etUj8mX6", "pXxz1MHR", "qpy6M6vh", "6WFYziDD"], "quotes": [], "sources": ["XN9jt6q8"], "n_links": 5}, {"id": "irmKQb3k", "name": "Roles in data innovation project team", "labels": [], "text_md": "- **Data scientist**: Understands data science modelling techniques\n- **Data engineer**: Understands dataflow architecture\n- **Data platform administrator**: \n- **Database administrator**: Understands individual DBMSs\n- **Data administrator**: Plans and sets policies related to data\n\n", "links": ["mmwn1oEU", "uKiS7iJH", "jU3e4dIS"], "quotes": [], "sources": [], "n_links": 3}, {"id": "PjMcjUgb", "name": "Team charter template", "labels": [], "text_md": "### Purpose\n\n- Why does our team exist?\n- What are we collectively working towards?\n- What are our personal goals?\n- What do we want to do in the next 15,30, 90 days?\n\n### Measures\n\n- What is the ultimate measure of success for the team?\n- How do we track progress towards our goals?\n\n### Roles\n\n- What are the roles we have in our team?\n- What are they responsible for?\n- Who will fill those roles?\n\n### Practices\n\n- How do we want to work together?\n- How do we communicate and meet?\n- What tools do we use?\n- How often do we revisit goals, retrospect, strategize?\n\n### Guardrails\n\n- What is safe to try?\n- How do we make decisions?\n- What rules do we want to put in place?\n- What other commitments do we have outside the team?\n\n", "links": ["1hLRzYSe", "mmwn1oEU", "fOMXYZho", "QBMWnA4E", "J4RjLkh4", "88RTR3eB", "h1xZIuYx"], "quotes": [], "sources": ["gMK50Cdd"], "n_links": 7}, {"id": "1hLRzYSe", "name": "Content Production Team", "labels": [], "text_md": "A content production team is often an example of a cross-functional team, with members coming from different functional areas of the organization that  may include:\n- Content strategist\n- Content manager\n- Content writer(s)\n- Content editor(s)\n- Content analyst\n\n", "links": ["ofQx7aBs", "J4RjLkh4", "MMjEeFha", "Lb8FlQSj", "bHaKr9ga", "S1rUnyfS", "AP2e8YNz", "oWg5LrNe", "PjMcjUgb", "jHpeljkZ", "i54AuJAT"], "quotes": [], "sources": [], "n_links": 11}, {"id": "J4RjLkh4", "name": "Ad-hoc cross functional teams", "labels": [], "text_md": "Ad-hoc cross functional teams seek to integrate skills across organizational boundaries in order to accomplish a specific goal over a limited period of time. They are often called during a critical juncture to deal with a specific problem or explore a new opportunity.\n\nA critical success factor is having clearly defined, realistic outputs, timeline, and exit criteria for each phase of the project.  \n\nAd-hoc  cross functional teams are challenged by complex supervisory relationships and incentive structures, as their members formally report to different functional managers. This increases the need for **horizontal cooperation and coordination** among team members.\n\nTo  avoid turf battles and organizational power politics, the different members of an ad-hoc cross-functional team must be completely **focused on creating value to customers**.  Team members must therefore have adequate **incentives to collaborate** outside their own \"home teams\" in the organization.  This means that their performance needs to be measured and rewarded based on their contributions towards achieving the objectives of the cross-functional project.\n\nPutting together the **right mix of talents** is another key ingredient for the success of a cross-functional team.  Reluctance by managers from different functional areas to get personally involved and to commit their most qualified staff to a cross functional project team often leads to poor results. On the other hand, assigning key staff members to a cross-functional project is a clear signal of the commitment by senior management to the success of the project. \n\nAd-hoc cross functional teams must also have a clearly defined **\"sunsetting\" plan** for incorporating back people and the team outputs, innovations and learnings back into the organization's functional processes and structure when the time comes to dissolve the team.  In particular, it is important to identify opportunities for team members to take on new responsibilities based on the skills they developed while being part of the ad-hoc cross functional team.\n\n", "links": ["Vk1c3Bjw", "wiYBU1Oc", "1hLRzYSe", "5woerok1", "94L4EBHO", "mmwn1oEU", "QBMWnA4E", "PjMcjUgb", "88RTR3eB", "duUZSb3j"], "quotes": [], "sources": ["gMK50Cdd"], "n_links": 10}, {"id": "Lb8FlQSj", "name": "Responsibilities of the content manager", "labels": [], "text_md": "Responsibilities of the content manager include:\n\n- Making day-to-day decisions about what should be pulbished, how and when\n- Explaining to content creators and project owners why a piece of content should be created\n- Finding topics that will perform well in various channels (blog articles, Twitter, email...)\n- Staying on top of the editorial calendar - overseeing that content getes finished and published on time\n- Knowing the workload of each content creator\n\n\n", "links": ["h1lCewmo", "1hLRzYSe", "MMjEeFha", "AP2e8YNz"], "quotes": [], "sources": [], "n_links": 4}, {"id": "oWg5LrNe", "name": "Responsibilities of the content analyst", "labels": [], "text_md": "The content analyst digs into the use analytics to measure content performance and create reports that help determine what works and what doesn't, in order to fine tune:\n- Content creation strategy (what type content should we create?)\n- Content promotion strategy (what content should we promote? What content promotion activities are most effective?)\n\n", "links": ["1hLRzYSe", "AP2e8YNz", "bHaKr9ga", "jHpeljkZ", "i54AuJAT"], "quotes": [], "sources": [], "n_links": 5}, {"id": "S1rUnyfS", "name": "Responsibilities of the content editor", "labels": [], "text_md": "The role of the content editor is to make a writer's good work great, offering guidance and helping authors think differently about their topic and ensuring that only high-quality content gets published.\n\nThe content editor's responsibilities include: \n\n- Review all written materials to ensure that they:\n  - are free of errors\n  - comply with the style guide\n  - reflect the brand's voice \n  - meet the goals set in the content strategy\n- Suggest revisions\n- Approve publication\n\n\n", "links": ["jHpeljkZ", "1hLRzYSe", "AP2e8YNz"], "quotes": [], "sources": [], "n_links": 3}, {"id": "i54AuJAT", "name": "Content promotion strategy", "labels": [], "text_md": "Once new content is published, there is need to promote content through social media and other channels.\n\nThe effectiveness of the content promotion strategy should be monitored through content analytics.  \n\n", "links": ["4INbWGw6", "ofQx7aBs", "1hLRzYSe", "xFkKBQHT", "oWg5LrNe", "bHaKr9ga", "jHpeljkZ", "i54AuJAT"], "quotes": [], "sources": [], "n_links": 8}, {"id": "xFkKBQHT", "name": "Guest blogging", "labels": [], "text_md": "Guest blogs where third parties promote their own content allow to strengthen partnerships while attracting visitors to our website and making it visible to broader audiences.\n\n", "links": ["bHaKr9ga", "Tlb9tDLU", "i54AuJAT"], "quotes": [], "sources": [], "n_links": 3}, {"id": "MMjEeFha", "name": "Skills required in a content manager", "labels": [], "text_md": "- Very good understanding of the needs of users\n- SEO and keyword research: Ability to identify topics and opportunities for improving organic traffic\n- Strong understanding of the organization's communication goals: Ability to generate meaningful topics and organize them in the editorial calendar\n\n", "links": ["G8wbe503", "Lb8FlQSj", "1hLRzYSe", "AP2e8YNz"], "quotes": [], "sources": [], "n_links": 4}, {"id": "4INbWGw6", "name": "COVID-19 response website types of users", "labels": [], "text_md": "Our content strategy needs to cater to multiple types of audiences. We need to know who are our target audiences.\n\n- Chief statisticians from national and international statistical organizations\n- Data experts/practitioners from national and international statistical organizations\n- Data experts/practitioners from UN Country Teams\n- Data experts/practitioners from civil society, academia and the private sector\n- Students at different education levels (high school/undergraduate/graduate)\n- Analysts / policy experts from national governments and international organizations\n- Donors providing funding for statistical capacity building\n\n", "links": ["h1lCewmo", "i54AuJAT"], "quotes": [], "sources": [], "n_links": 2}, {"id": "i7m8d4YQ", "name": "Information architecture for the COVID-19 response platform", "labels": [], "text_md": "Guidance and resources provided through UNSD's COVID-19 response platforms could be organized according to the various phases of the statistical business process:\n- Data Collection\n- Data Validation\n- Data Processing \n- Data Analysis\n- Data Dissemination\n\nThey could also be organized according to the classification of statistical activities.\n\n\n", "links": ["bHaKr9ga", "68VjKPZb"], "quotes": [], "sources": [], "n_links": 2}, {"id": "jHpeljkZ", "name": "Content strategy goals", "labels": [], "text_md": "- Increase *website traffic*\n- Make brand channels a destination for *organic traffic*\n- Increase *brand awareness*\n- Establish *brand authority*\n- Bring **value to users**\n  - Help users identify and understand the main challenges faced by the national and global statistical systems that result from the COVID-19 pandemic\n  - Help global and national statistical systems overcome these challenges\n\n\n", "links": ["1hLRzYSe", "h1lCewmo", "S1rUnyfS", "AP2e8YNz", "oWg5LrNe", "bHaKr9ga", "i54AuJAT"], "quotes": [], "sources": [], "n_links": 7}, {"id": "AP2e8YNz", "name": "Elements of a content strategy", "labels": [], "text_md": "A content strategy includes:\n- A list of **clear goals**\n- A method for measuring success\n- A competitor analysis\n- Persona development\n- A brand style guide\n- A list of types of content to be produced\n- An initial editorial calendar\n\n\n", "links": ["1hLRzYSe", "ofQx7aBs", "tZWRLOUI", "MMjEeFha", "Lb8FlQSj", "S1rUnyfS", "oWg5LrNe", "bHaKr9ga", "jHpeljkZ"], "quotes": [], "sources": [], "n_links": 9}, {"id": "bHaKr9ga", "name": "Types of content", "labels": [], "text_md": "There are many types of content:\n- Blogs\n- Emails\n- Videos\n- Infographics\n- Newsletters\n- Press releases\n- ...\n\n", "links": ["G8wbe503", "1hLRzYSe", "i7m8d4YQ", "xFkKBQHT", "oWg5LrNe", "AP2e8YNz", "jHpeljkZ", "i54AuJAT"], "quotes": [], "sources": [], "n_links": 8}, {"id": "EojLU88D", "name": "Global SDG indicator framework", "labels": [], "text_md": "The 17 goals and 169 targets of the 2030 Agenda for Sustainable Development are monitored and reviewed at the global level through an indicator framework developed by the Inter-agency and Expert Group on SDG Indicators. \n\nThe Global SDG indicator framework was initially agreed upon at the 48th session of the United Nations Statistical Commission in March 2017, and adopted by the General Assembly on 6 July 2017 in its [resolution on the Work of the Statistical Commission pertaining to the 2030 Agenda for Sustainable Development (A/RES/71/313)](https://undocs.org/A/RES/71/313).  \n\nAnnual refinements to the Global SDG Indicator Framework, as well as more comprehensive reviews, are submitted by the IAEG-SDGs to the United Nations Statistical Commission for its approval.  \n\nThe 2020 comprehensive review included 36 major changes to the framework in the form of replacements, revisions, additions and deletions to the framework.  The next comprehensive review is scheduled to take place in 2025.\n\n\n", "links": ["0zGb0Ntw", "5u1sLb2p"], "quotes": [], "sources": [], "n_links": 2}, {"id": "Tlb9tDLU", "name": "Sharing of experiences, good practices and lessons learned", "labels": [], "text_md": "Success in the use of new technologies and sources of data will depend on a wide variety of contextual and operational factors, as well as on the evolving severity and nature of the impact of the COVID-19 epidemic in each country. Managers of national and global statistical programmes affected by the crisis require a platform for rapidly sharing experiences and lessons learned as they navigate a new environment characterized by many uncertainties and risks. This sharing should focus on the essential aspects of planning, management and implementation of re-organization and adaptation strategies. \n\n", "links": ["jj13yRW9", "h1lCewmo", "xFkKBQHT"], "quotes": [], "sources": ["vh5QhUfR"], "n_links": 3}, {"id": "h1lCewmo", "name": "COVID-19 response website - analysis of users' goals", "labels": [], "text_md": "Once we know who our target audiences are, we need to know what are their own goals, and how we will be helping them attain those goals.  \n\nOur main constituency are National Statistical Offices and statistical offices of international organizations, whose main goals are to **(1) keep existing statistical programmes running**, and **(2) respond to new data requirements ** around the COVID-19 crisis from national and local governments, other institutional decision makers, and the public at large.\n\nThese goals translate into specific needs:\n\n- **Mobilize resources** to support regular statistical activities affected by the pandemic and to launch and run new statistical activities to satisfy new data demands\n- **Obtain access to methodological guidance** on how the use of new data sources, methods and technologies for the collection, processing, analysis, dissemination and communication of data and statistics\n  - Create opportunities for peer-to-peer **sharing of experiences** \n  - Provide access to relevant **training materials**\n  - Deliver **expert advice**\n\n- **Identify authoritative, reliable sources of data** to monitor the day-to-day evolution of the health crisis, to assess and monitor social, economic and environmental impact of the pandemic, and to inform recovery policies over the longer term\n- Identify **capacity building needs**\n- **Coordinate initiatives** that respond to the needs of national statistical systems\n  - Understand who is doing what across the global and national statistical systems\n\n\n", "links": ["4INbWGw6", "QJsx3KmY", "jHpeljkZ", "P43SdKCs", "Lb8FlQSj", "53FNlsMn", "Tlb9tDLU"], "quotes": [], "sources": [], "n_links": 7}, {"id": "mRaNOol6", "name": "Complexity of census programmes", "labels": [], "text_md": "Population and housing census programmes are complex data collection operations comprising a series of many interrelated activities. They require contacting and collecting information on the whole population of a country within a limited period of time. \n\n", "links": ["jj13yRW9", "nVgnq2cz"], "quotes": [], "sources": ["vh5QhUfR"], "n_links": 2}, {"id": "5gf8Ujfa", "name": "Testing of new IT systems for data collection", "labels": [], "text_md": "Statistical organizations are under huge pressure to accelerate the process of procuring, developing and deploying new IT systems in order to respond to the challenges of the COVID-19 crisis. \n\nHowever, in order to prevent further disruptions in critical statistical operations, the introduction of new IT systems for data collection, processing and dissemination requires that these systems be thoroughly tested with respect to:\n\n- Functionality\n- Usability\n- Integration\n- Accessibility\n- Security\n- Reliability / stress testing\n\n", "links": ["oRq6vy7w", "smZXjmav", "TsYS8WO6"], "quotes": [], "sources": ["vh5QhUfR"], "n_links": 3}, {"id": "gxrbaZc1", "name": "Small, bottom-up initiatives in response to COVID-19", "labels": [], "text_md": "Small, bottom-up initiatives can play an important role in a statistical organization's response to the disruptions created by the COVID-19 pandemic.   Instead of attempting to develop and implement grand and detailed plans, in a crisis like this it is better to learn quickly and often from the successes and failures of many small-scale experiments.\n\nAdaptability usually comes \"from the accumulation of microadaptations originating throught [the organization] in response to its many microenvironments\".\n\nIn times of crisis, information sharing, listening and learning must take precedence over hierarchy and formal authority, so as to be able to draw from the collective knowledge and skills across diverse functions and locations to generate solutions, \n\n", "links": ["4kDQlxl8", "bay5lBgW"], "quotes": [], "sources": ["OlVDFOgK"], "n_links": 2}, {"id": "4kDQlxl8", "name": "Leading the response of NSOs to the COVID-19 pandemic", "labels": [], "text_md": "The worldwide spread of COVID-19 is having vast, long-term consequences for people, organizations, economies and society at large, which call for a new kind of leadership.  \n\nLeaders from governments and organizations at all levels must not only make decisions to keep people safe and ensure continuity of operations in the middle of the crisis. They must also keep an eye on the long-term recovery and look for opportunities to innovate, create growth and \"build back better\".\n\nEven after the health crisis is over, things will not \"return to normal\", and national and international statistical organizations will have to adapt to a new reality, characterized by new demands from users and lasting changes in the configuration of the entire data value chain. \n\nIn responding to the crisis, heads of national statistical offices cannot default to known approaches, as the unprecedented nature and scale of the challenge surpasses anyone's past experience.  NSO staff at all levels need to quickly adapt and experiment with \"next practices\" in their day-to-day statistical operations, acknowledging that many of the traditional \"best practices\" are no longer relevant. \n\nMoreover, any success in the initial response by NSOs should not be followed by complacency or the illusion of a return to normalcy. Instead, statistical organizations must continue focusing on innovation and sustain their efforts to understand and adaptation to the new reality in the global data ecosystem. \n\nIn this context, heads of statistical organizations must identify the principles and practices that must be preserved in the compilation of official statistics, as well as those that must be abandoned or modified in order to move forward.\n\n\n", "links": ["gxrbaZc1", "xUr89DOT", "bay5lBgW"], "quotes": [], "sources": ["OlVDFOgK"], "n_links": 3}, {"id": "TsYS8WO6", "name": "Challenges of introducing new technologies in response to COVID-19", "labels": [], "text_md": "National Statistical Offices are being challenged to introduce telephone-based interviewing and web-based self-reporting techniques at once for many critical data collection operations--such as population and housing, agricultural, and economic censuses, as well as household, business and other types of surveys. In many cases, they need to do it without the benefit of prior experience and with very limited time to conduct detailed analysis and testing of the different alternatives.   \n\nThe introduction of these new technologies is risky and can be expensive. To make an informed decision on the type of technologies best suited to mitigate disruptions to data collection programmes, National Statistical Offices need to take into account their existing infrastructure, technical capacities and resources. \n\n", "links": ["MfTvaMIL", "qvOtxtiK", "5gf8Ujfa", "A23mYrDu"], "quotes": [], "sources": ["vh5QhUfR"], "n_links": 4}, {"id": "vIDWWZjJ", "name": "Communication strategy with respondents", "labels": [], "text_md": "It is crucial to design and launch as quickly as possible a contact and communication strategy towards respondents in the target population, aimed to maximize high response rates in a new web-based or telephone-based data collection setting.  This include communications soliciting households to complete online questionnaires or to be interviewed by telephone, sending reminders and follow-ups in case of non-response.\n\n", "links": ["A23mYrDu"], "quotes": [], "sources": ["vh5QhUfR"], "n_links": 1}, {"id": "oRq6vy7w", "name": "Re-purposing CAPI infrastructure to conduct CATI data collection", "labels": [], "text_md": "National Statistical Offices that have in place a data collection programme based on computer-assisted personal interviews (CAPI) may consider re-purposing the existing software and hardware infrastructure to support computer-assisted telephone interviews (CATI) instead.  This would allow to leverage the existing devices (tablets, personal digital assistants, smart phones or portable computers) as well as their specialized software, including their ability to instantly transmit data over mobile data networks.  \n\nHowever, this re-purposing is not trivial.  For instance, it requires to integrate CATI operations with existing digital mapping and operational management applications built under the assumption that enumerators/interviewers would be entering the data on the same location as the respondent. As interviewers will now be entering the information from a remote location, this creates new challenges for the automatic geo-coding of questionnaire responses and for the supervision of the interview process. \n\n", "links": ["5dUHywzA", "qvOtxtiK", "5gf8Ujfa", "87In6UYV"], "quotes": [], "sources": ["vh5QhUfR"], "n_links": 4}, {"id": "A23mYrDu", "name": "Challenges of remote interviewing and self-reporting", "labels": [], "text_md": "Even National Statistical Offices that have started using electronic data collection approaches, such as computer-assessed personal interviews (CAPI), still rely heavily on personal interviews, Some of the challenges in moving away from face-to-face interviews to remote interviews and self-reporting methodologies include:\n\n- Implementing new mechanisms to identify, contact, authenticate and communicate with respondents\n- Establish mechanisms to geo-locate responses obtained via remote interviewing and self-reporting.\n- Procure/develop and test IT systems to support computer-assisted telephone interviews and online self-reporting portals\n- Implement mechanisms to support respondents in completing online questionnaires\n- Implement mechanisms to support and oversee telephone-based interview workflows\n- Ensure secure remote access to IT systems and secure data exchange.\n\n", "links": ["vIDWWZjJ", "87In6UYV", "TsYS8WO6"], "quotes": [], "sources": ["vh5QhUfR"], "n_links": 3}, {"id": "qvOtxtiK", "name": "Continuity of statistical programmes: Role of new technologies", "labels": [], "text_md": "It is imperative to leverage innovative technologies and approaches to ensure the continuity of censuses, household surveys, and other major statistical programmes.  This includes making use, to the fullest extent possible, of mobile connectivity, cloud computing, smart mobile devises, and other technological innovations that offer alternative means to ensure that activities around the capturing, validation, processing and dissemination of census and survey data can go on in the new environment of limited mobility of staff and of the population at large. \n\n", "links": ["BQwQetnc", "TsYS8WO6", "oRq6vy7w", "jj13yRW9", "rps4c5Jo"], "quotes": [], "sources": ["vh5QhUfR"], "n_links": 5}, {"id": "SGskQjpC", "name": "Contingency planning needs to be tailored to national circumstances", "labels": [], "text_md": "The response of each National Statistical Office to the COVID-19 crisis needs to be tailored to its own particular circumstances, including the nature and severity of the disruptions caused by the crisis, its existing infrastructure, technical capacities and resources, and other institutional, operational, economic, and socio-cultural factors. \n\n", "links": ["jj13yRW9", "xUr89DOT"], "quotes": [], "sources": ["vh5QhUfR"], "n_links": 2}, {"id": "nVgnq2cz", "name": "Impact of COVID-19 on census programmes", "labels": [], "text_md": "The COVID-19 crisis creates unprecedented and sudden challenges for countries conducting population and housing censuses and other major statistical operations, disrupting data collection, processing, analysis and dissemination activities carried out by National Statistical Offices, forcing them to rapidly develop and adopt alternative ways of implementing them.\n\n", "links": ["mRUJpSCa", "mRaNOol6"], "quotes": [], "sources": ["vh5QhUfR"], "n_links": 2}, {"id": "87In6UYV", "name": "Selecting alternative data collection approaches - time considerations", "labels": [], "text_md": "Time is a key factor in selecting alternative data collection approaches to ensure the continuity of statistical operations:\n\n- Estimated time necessary to procure/develop, test and deploy technical solutions \n- Estimated time that needs to be spent in training of staff in new skills and the use of new technologies\n\n", "links": ["oRq6vy7w", "rps4c5Jo", "xUr89DOT", "A23mYrDu"], "quotes": [], "sources": ["vh5QhUfR"], "n_links": 4}, {"id": "xUr89DOT", "name": "Priority objectives of COVID-19 contingency plans for statistical programmes", "labels": [], "text_md": "Priority areas are:\n\n- Maintain adequate coverage of the target population\n- Ensure high questionnaire- and item-response\n- Guarantee internal consistency, comparability, and overall quality of the data collected\n- Maintain timely data collection, processing and dissemination\n- Minimize response burden on information providers\n- Use resources efficiently / minimize cost of statistical operations in the new environment\n\n", "links": ["4kDQlxl8", "bay5lBgW", "SGskQjpC", "YJqwphec", "jj13yRW9", "87In6UYV"], "quotes": [], "sources": ["vh5QhUfR"], "n_links": 6}, {"id": "URGTzbbu", "name": "Measures by WHO and public health authorities to contain COVID-19", "labels": [], "text_md": "WHO and public health authorities across the world are taking measures to contain the COVID19 epidemic.  All sectors of society, including members of the global and national statistical systems, need to act promptly and in a coordinated manner to respond effectively to the crisis and mitigate its human and economic impacts, as well as to usher a rapid and sustainable recovery.\n\n", "links": ["mRUJpSCa", "jj13yRW9"], "quotes": [], "sources": ["A1MOeNel"], "n_links": 2}, {"id": "5dUHywzA", "name": "Adapting infrastructure and operations of statistical organizations in times of crisis", "labels": [], "text_md": "National and international statistical organizations must adapt to the uncertainties of a new reality characterized by health emergencies, environmental crises, economic recession and political instability. \n\nFor instance, the central information systems of National Statistical Offices need to adapt quickly to effectively manage and monitor statistical operations in the context of the COVID-19 crisis, such as staff recruitment, training, data collection logistics, and supervision and gathering of operational intelligence. \n\n\n\n", "links": ["oRq6vy7w", "jj13yRW9", "bay5lBgW"], "quotes": [], "sources": ["vh5QhUfR"], "n_links": 3}, {"id": "jj13yRW9", "name": "COVID19 contingency plans in statistical organizations", "labels": [], "text_md": "- Determine how existing statistical activities, processes and programmes are being affected by the crisis\n- Find alternative means to reach existing data sources (e.g., computer-assisted self interviews with the use of internet and smart phone, or computer assited telephone interviews), or alternative data sources when existing ones are no longer accessible (e.g., switch to use of administrative sources and webscraping).\n- Develop a plan to switch on-site and field processes to a remote/telecommuting setting:\n    - Processes that can be performed remotely by staff using existing infrastructure (e.g., repurposing CAPI software and equipment to support CATI data collection)\n    - Processes that could be performed remotely after digitizing and/or migrating them to cloud environments (e.g., maintenance of public data dissemination portals)\n    - On-site and field processes that would require establishing mechanisms for secure remote access to central databases and systems in order to be performed out of premises (e.g., processing and analysis of confidential microdata records).\n- Identify priority data needs of governments and other stakeholders to respond to the COVID19 crisis\n\n\n", "links": ["mRUJpSCa", "BQwQetnc", "URGTzbbu", "SGskQjpC", "mRaNOol6", "qvOtxtiK", "88RTR3eB", "5dUHywzA", "Tlb9tDLU", "xUr89DOT"], "quotes": [], "sources": ["Y9KlpXBF"], "n_links": 10}, {"id": "rps4c5Jo", "name": "Impact of population mobility restrictions on data collection programmes", "labels": [], "text_md": "In an effort to contain the spread of the COVID-19 epidemic, many governments are imposing severe restrictions on the mobility of the population, disrupting field data collection operations and threatening the ability of National Statistical Offices to deliver high-quality, timely and cost-effective statistical outputs. \n\nThis results in the urgent need to replace current field data collection operations that rely on face-to-face interviews with alternative remote data collection methodologies, such as telephone personal interviewing or paper or web-based self-reporting methods.  \n\nTo respond to this challenge, many countries need to build quickly the necessary capacity to accelerate the implementation of fully digital data collection technologies instead of traditional paper-based methods.\n\n", "links": ["mRUJpSCa", "qvOtxtiK", "87In6UYV"], "quotes": [], "sources": ["vh5QhUfR"], "n_links": 3}, {"id": "mRUJpSCa", "name": "Impact of COVID19 pandemic on national statistical offices", "labels": [], "text_md": "The measures to contain the spread of the epidemic in many countries include the requirement for very large parts of the population to stay at home and avoid social contact.  There is a substantial risk that the global health emergency will continue disrupting the normal operations of all sectors of society over several months. \n\nThis is disrupting statistical operations around the world, as the staff of statistical organizations are unable to go to their offices or to the field in order to perform their regular tasks.  The crisis is also making it more difficult to engage with the broader statistical community, as workshops, seminars, expert forums, inter-agency coordination meetings and other public events are cancelled. \n\nThe impact on statistical operations include:\n- Disruptions in data collection\n- Disruption of data processing and analysis workflows\n- Delay in publication of statistical outputs\n- Suspension of user engagement events and staff training activities\n\n\n\n", "links": ["BQwQetnc", "URGTzbbu", "nVgnq2cz", "jj13yRW9", "rps4c5Jo"], "quotes": [], "sources": ["Y9KlpXBF"], "n_links": 5}, {"id": "lh9J280B", "name": "Bandwidth requirements for telecommuting", "labels": [], "text_md": "Voice and video-conferencing are essential tools for effective telecommuting. However they require a minimum level of bandwidth that is not always present.\n\n\n", "links": ["qEgpEYM3", "BQwQetnc", "rR6VySYD"], "quotes": [], "sources": ["ZNBpGaEL"], "n_links": 3}, {"id": "smZXjmav", "name": "On-site infrastructure", "labels": [], "text_md": "Many statistical organizations still use server infrastructure located in their own premises to support key functions, such as collecting source data from information providers, giving staff access to data management and data analysis software, and delivering statistical outputs to users.\n\nIf staff is not able to access the software tools and data needed to perform their work, statistical organizations face major disruption in essential workflows. \n\n", "links": ["akoVqiRK", "5gf8Ujfa", "kfu8nyho"], "quotes": [], "sources": ["ZNBpGaEL"], "n_links": 3}, {"id": "5k2KowcP", "name": "Business intelligence vs. transaction data processing", "labels": [], "text_md": "- Transaction processing typically deals with a few records at a time.  It is process-oriented, and relies on current data at the individual-record level.  \n- Business intelligence processing may deal with thousands or millions of records at a time. It is subject/topic-oriented, and relies on historical data at both the individual-record and aggregate/summarized levels. \n\n", "links": ["Iaba3RcP", "62wtBu4C", "UHxZBMU2"], "quotes": [], "sources": ["oLNJUIaY"], "n_links": 3}, {"id": "88RTR3eB", "name": "Business continuity teams", "labels": [], "text_md": "The development and implementation of a business continuity plan requires the establishment of a cross-functional team, composed of:\n\n- Senior managemers\n- Front-line managers\n- Information Technology department\n- Legal department\n\n", "links": ["BQwQetnc", "J4RjLkh4", "JXu4aYg9", "PjMcjUgb", "jj13yRW9", "akoVqiRK"], "quotes": [], "sources": ["ZNBpGaEL"], "n_links": 6}, {"id": "7dELQecN", "name": "Updating telecommuting policy", "labels": [], "text_md": "In many organizations, existing telecommuting policies were written under the assumption that telecommuting was offered only to certain employees under special circumstances.  \n\nIn the face of the COVID-19 crisis, telecommuting will be the most common work arrangement for many organizations over the foreseeable future. \n\nThere is an urgent need to review and adapt existing telecommunication policies so organizations can nimbly adjust to the new situation. \n\n- Telecommuting as the rule and not the exception\n- Less cumbersome process for formal agreements with staff regarding use of corporate infrastructure and services\n\n", "links": ["qEgpEYM3", "rR6VySYD", "kfu8nyho"], "quotes": [], "sources": ["ZNBpGaEL"], "n_links": 3}, {"id": "kfu8nyho", "name": "COVID-19 - Sudden spike in need for telecommuting", "labels": [], "text_md": "To limit the COVID-19 epidemic, organizations are requiring all or most of their staff work from home. This has created a huge challenge of having to manage \"a very large and sudden spike\" in the number of remote workers, even for organizations that already support certain number of telecommuters. National Statistical Offices are facing the prospect of a protracted telecommuting crisis.\n\n> \"Until now, telecommuting has been voluntary or even a reward of sorts. Not it's mandatory...\"  (Rist, 2011)\n\nSuch challenges include:\n- Meeting increased demand for IT help-desk support\n- Enabling staff to assume additional responsibilities regarding device and data security\n- Migrating on-premises workloads to cloud services\n- Providing remote access and remote management solutions for applications that need to remain served form on-premises servers\n- Adapting to remote performance tracking and virtual team collaboration\n\n", "links": ["qEgpEYM3", "t36XnVjf", "rR6VySYD", "7dELQecN", "smZXjmav"], "quotes": [], "sources": ["ZNBpGaEL"], "n_links": 5}, {"id": "ulLdtnHy", "name": "Cloud computing", "labels": [], "text_md": "Cloud computing refers to the delivery of hosted IT services over the internet, usually on a pay-as-you-go basis.  It includes:\n\n- Software as a service (SaaS) \n- Infrastructure as a service (IaaS)\n- Platform as a service (PaaS)\n\nCloud service providers are responsible for all their hardware and software maintenance, and are backed by a large network of servers and staff to ensure the reliability of their services. \n\nDue to its reliance on hardware-independent virtualization technology, cloud computing enables organizations to quickly back up data, applications, and even operating systems to a remote data center, and to deploy them to multiple users distributed in many different locations.\n\nCloud services usually involve lower upfront costs and shorter time commitments, lowering the barriers of entry for organizations that seek to modernize their IT infrastructure, explore the adoption of new tools, or scale up their ability to handle larger volumes of data. Thus, managed cloud services can provide a good solution for new data innovation projects, which start small but need to be able to rapidly grow. \n\n", "links": ["qEgpEYM3", "eeJS3CIE", "9PrGSBNy", "Vmp2TR2i", "3ONs7aFt", "vKQFuLMb"], "quotes": [], "sources": ["HYBQIQ60"], "n_links": 6}, {"id": "MfTvaMIL", "name": "Risks and challenges of adopting cloud applications", "labels": [], "text_md": "Moving systems currently hosted on-premises to cloud services is a very complex task that requires:\n\n- Vendor selection and procurement\n- Data cleaning and migration\n- Addressing user management and authentication issues\n- Data security and network connectivity through remote logins and VPNs\n\n", "links": ["qEgpEYM3", "TsYS8WO6", "sKuMe1ca", "Vmp2TR2i", "QSfNE2W5", "vKQFuLMb", "akoVqiRK"], "quotes": [], "sources": ["ZNBpGaEL"], "n_links": 7}, {"id": "BQwQetnc", "name": "Business continuity of statistical organizations", "labels": [], "text_md": "The current COVID-19 crisis is affecting critical operations of across the entire global statistical system, and national and international statistical organizations need to urgently develop and implement action plans to ensure the continuity of key statistical compilation activities and the continued availability of data to inform emergency mitigation actions by governments and all sectors of society. \n\nSenior management in statistical organizations need to define guidelines, in consultation with front-line managers and IT teams, to deal with the contingency.  This includes establishing new procedures and workflows on issues like:\n\n- **Management of virtual teams (task tracking and performance management)**: Can leaders of NSOs communicate easily with their staff? Is there a centralized location from which critical information is accessible to staff?\n- **Secure remote data access and data exchange**: Can staff stay connected through internet from home?  Are critical data safely stored and securely accessible?\n- **Responding to users' most urgent needs**: Can users reach service and response teams through dedicated phone lines and email address? Are  specific instructions on how to obtain immediate assistance available through social media and the organization's website? \n\n\n\n", "links": ["qEgpEYM3", "LaFIh4fq", "mRUJpSCa", "rR6VySYD", "qvOtxtiK", "YJqwphec", "88RTR3eB", "lh9J280B", "jj13yRW9", "akoVqiRK"], "quotes": [], "sources": [], "n_links": 10}, {"id": "LaFIh4fq", "name": "Remote access and remote management solutions", "labels": [], "text_md": "Business continuity in a situation where most staff have to telecommute requires the implementation of  secure and effective mechanisms for remote access and remote management for any applications in which a cloud computing solution is not feasible, so they need to remain served from infrastructure located on-premises,\n\nThere must effective mechanisms in place to ensure that any on-site applications can be managed as much as possible from off-site. This includes the ability to run the most common application management tasks remotely, including system reboots, data backups, network and system security scans, and password resets. \n\n", "links": ["qEgpEYM3", "BQwQetnc"], "quotes": [], "sources": ["ZNBpGaEL"], "n_links": 2}, {"id": "qEgpEYM3", "name": "Challenges related to the rapid implementation of telecommuting arrangements", "labels": [], "text_md": "- Procure, configure, and distribute computer equipment and software needed for remote collaboration among all staff members, including add-ons to enable secure remote data access and system administration capabilities (e.g., standardized end-point security software on all employee devises).  If staff has already begun telecommuting, it is necessary to work out how to distribute equipment and software to the locations where they are based.\n- Ensure voice connectivity and video-conferencing capabilities (e.g., forwarding calls to staff members' home or mobile phones; enabling soft-phone, voice-over-IP or video conference solutions). It is key to allow teams to regularly and easily talk to, and visually interact with, each other  \n- Ensure remote users have the necessary bandwidth. This may entail finding a way to upgrade the user's phone and/or internet access for a period of time.\n- Make sure there are processes in place to cover most common IT helpdesk support requests form staff members working remotely\n- Enable cloud-based backup services in all remote devices.  \n- Design, implement and implement new workflows\n\n", "links": ["LaFIh4fq", "rR6VySYD", "BQwQetnc", "kfu8nyho", "7dELQecN", "MfTvaMIL", "ulLdtnHy", "lh9J280B"], "quotes": [], "sources": ["ZNBpGaEL"], "n_links": 8}, {"id": "3ONs7aFt", "name": "Software as a Service (SaaS)", "labels": [], "text_md": "Model in which software applications are hosted by a third-party and made available to users over the Internet, usually through a web browser.  Users do not have to install or configure anything, and the underlying cloud hardware is maintained by the service provider. \n\nMost SaaS providers offer flexible, on-demand pricing arrangements as well as tools for user management and data migratoin.\n\n- Examples of SaaS applications:\n  - Email, videoconferencing, and other basic communication tools\n  - File sharing and team collaboration. \n  - Human resource management\n  - Management of relationships with data providers\n  - Management or relationships with data users\n  - Specialized applications (e.g., statistical analysis software, GIS applications)\n  - E-Learning delivery\n\n\n", "links": ["ulLdtnHy"], "quotes": [], "sources": [], "n_links": 1}, {"id": "akoVqiRK", "name": "Cloud computing for business continuity and disaster recovery", "labels": [], "text_md": "In recent years, cloud computing has become increasingly used by statistical organizations that do not have the hardware or personnel necessary to fully deploy software applications onsite, or that need to test new software tools in the context of pilot data-innovation projects.\n\nToday, cloud computing stands out as a key element of a business continuity and disaster recovery plan for statistical organizations, particularly in the face of the disruption national and global statistical systems caused by the COVID-19 crisis. \n\nIn order to leverage cloud computing solutions for disaster recovery and business continuity, statistical organizations require an IT architecture focused on \"automating as many processes as possible in the event of disaster, ensuring that computing resources are switched over quickly to a stable backup and remain operational.\" (\n\n\n", "links": ["88RTR3eB", "MfTvaMIL", "BQwQetnc", "smZXjmav"], "quotes": [], "sources": [], "n_links": 4}, {"id": "66kUhExT", "name": "Importance of CRVS Systems", "labels": [], "text_md": "CRVS Systems are crucially important to:\n- Build a modern public administration\n- Uphold human rights\n- Support national development initiatives\n- Improve service delivery to all people\n\n", "links": ["Gjve1WgL", "8B67V92I", "AMu8kLNJ", "Yx0ZUlq2", "1i1bh7AA"], "quotes": [], "sources": [], "n_links": 5}, {"id": "8B67V92I", "name": "Africa's Programme on Accelerated Improvement of CRVS", "labels": [], "text_md": "Africa's Programme on Accelerated Improvement of Civil Registration and Vital Statistics was created under the directive of African Ministers Responsible for Civil Registration in 2010.  \n\nIts secretariat is based at UN ECA\n\n", "links": ["66kUhExT", "Yx0ZUlq2", "1i1bh7AA"], "quotes": [], "sources": [], "n_links": 3}, {"id": "1i1bh7AA", "name": "CRVS systems as a development imperative", "labels": [], "text_md": "There has been significant progress in recognizing CRVS systems as a development imperative\n\n", "links": ["66kUhExT", "Gjve1WgL", "Yx0ZUlq2", "8B67V92I"], "quotes": [], "sources": [], "n_links": 4}, {"id": "AMu8kLNJ", "name": "How does a good CRVS system look like?", "labels": [], "text_md": "- Universal\n- Continuing / permanent\n- Compulsory\n- Confidential\n- Every vital event (but primarily birth and death) is registered upon occurrence\n- Vital statistics are produced and used to guide policy\n\n\n", "links": ["AMu8kLNJ", "66kUhExT", "yHy01yYb"], "quotes": [], "sources": [], "n_links": 3}, {"id": "Yx0ZUlq2", "name": "Decade for repositioninig CRVS in Africa", "labels": [], "text_md": "2017-2026 has been designated as the \"decade for repositioning CRVS in Africa\" by the Executive Councl of the African Union in Kigali. \n\n", "links": ["66kUhExT", "1i1bh7AA", "8B67V92I"], "quotes": [], "sources": [], "n_links": 3}, {"id": "etUj8mX6", "name": "Map visualizations", "labels": [], "text_md": "Map visualizations allow to:\n- Identify outliers in the spatial distribution of individual variables\n- Identify and highlight spatial correlations across multiple datasets.  \n- Identify and highlight spatial patterns of inequality at the subnational level (e.g., across districts, municipalities, and communities)\n\n", "links": ["lEHhU8Ma", "tltgpLsL", "iJY1iH4F", "3c7dZcB6", "pXxz1MHR", "ClFwmtgA"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 6}, {"id": "Gjve1WgL", "name": "CRVS systems and the 2030 Agenda", "labels": [], "text_md": "The 2030 Agenda for Sustainable Development's pleadge to leave no one behind means that no one should remain invisible.  Target 16.9 reads: \"By 2030, provide legal identity for all, including birth registration\".   \n\nHowever, many developing countries still do not have a comprehensive and complete CRVS system aligned with international standards.\n\n", "links": ["66kUhExT", "1i1bh7AA"], "quotes": [], "sources": [], "n_links": 2}, {"id": "53FNlsMn", "name": "Training in statistical and econometric modeling", "labels": [], "text_md": "Depending on the specific objectives of a data innovation project for the production of official statistics, there is need to provide specialized statistical and econometric modeling training.  For instance:\n\n- Population density estimation\n- Household consumption estimation\n- Crop-yield estimation\n\nTO DO: What are the most needed types of statistical and econometric modeling methods that project teams need to be able to apply?\n\n", "links": ["P43SdKCs", "h1lCewmo", "2Daj1ZPE", "ilCtHTAx"], "quotes": [], "sources": [], "n_links": 4}, {"id": "kCnh0WC4", "name": "Scope of data innovation projects", "labels": [], "text_md": "Every project needs to have a specific and detailed work programme with concrete activities and expected outcomes.\n\nThe objectives and scope of every data innovation project undertaken in a country need to fit withing the regular work programme of the NSOs and the overall institutional setting of the National Statistical System. \n\n", "links": ["QJsx3KmY"], "quotes": [], "sources": [], "n_links": 1}, {"id": "5Y5WIm31", "name": "Poverty maps production: intermediate indicators", "labels": [], "text_md": "Poverty map production usually requires the computation of intermediate indicators to be used as covariates in poverty estimation methods:\n\n- Distance to nearby markets\n- Distance to nearby cities\n- Distance to roads\n- Distance to service delivery points (e.g., schools, health centers...)\n...\n\n", "links": ["vlQ3wkHh", "QJsx3KmY", "XVq7u8bY"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 3}, {"id": "5woerok1", "name": "Establishing a project steering committee", "labels": [], "text_md": "To ensure the success of a cross-functional team, it is useful to establish a steering committee of senior managers, whose role is to ensure that the project is adequately resourced and that the project goals are aligned with functional and high-level organizational goals. \n\nThis **steering committee** should include a representative of each functional area involved, and should be responsible of assigning individual project tasks to the member of the cross-functional team that is better positioned to undertake it.  \n\n", "links": ["mmwn1oEU", "J4RjLkh4"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 2}, {"id": "Z12lJnJp", "name": "Why do we write", "labels": [], "text_md": "We write to \n- **remember** and **organize** ideas\n- **understand** and to **learn**\n- **communicate** insights\n\n", "links": ["pS33XHDA"], "quotes": ["7DxSGUo6"], "sources": ["RnpNCnXf"], "n_links": 1}, {"id": "UXewQVNc", "name": "Reading and note-taking", "labels": [], "text_md": "- Reading and thinking are the main task. \n- The goal is to understand and come up with new ideas. \n- The notes are just the tangible outcome.\n\n\"The ability to express understanding in one's own words is a fundamental competency\", the same as the ability \"to distinguish the important bits of a text from the less important ones.\" (Ahrens, 2017, p. 54). \n\n", "links": ["pS33XHDA", "ZyufNNxO"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 2}, {"id": "3cbc3DFd", "name": "Confirmation bias", "labels": [], "text_md": "The very moment we decide on a hypothesis, our brains automatically go into search mode, scanning our surroundings for supporting data, (p. 79).\n\nCharles Darwin... forced himself to write down (and therefore elaborate on) the arguments that were the most critical of his theories. (p. 80). \n\n", "links": ["pS33XHDA"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 1}, {"id": "ZyufNNxO", "name": "Smart note-taking", "labels": [], "text_md": "According to Ahrens (2017), smart note taking requires:\n- Reading a text with questions in mind and try to relate it to other possible approaches\n- Spotting the limitations of a particular approach\n- Seeing what is *not* mentioned\n- Interpreting particular information within the bigger frame or argument of the text\n- Thinking hard about how the main ideas of the text connect with other ideas from different contexts:  *\"Notes are only as valuable as the ... reference networks they are embedded in.\"* (Ahrens, 2017, p. 108).\n\n", "links": ["FlUbxxop", "UXewQVNc"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 2}, {"id": "f6AE5k9M", "name": "Types of notes", "labels": [], "text_md": "1. **Fleeting notes**: Sever to capture \"raw\" ideas we come across. They are stored in one place for later processing.  They are only useful if reviewed and turned into proper notes within a day or so.\n2. **Literature notes**: Capture bibliographic details and brief description of sources.\n3. **Permanent notes**: Serve to develop ideas based on fleeting notes and literature notes.  Written in precise, clear, and brief full sentences. They can be understood even outside the context they were taken from.\n4. **Project notes**: Are only relevant to one particular project and can be discarded or archived after the project is finished\n\nTypical mistakes:\n- Treat every note as if it belongs to the \"permanent\" category\n- Collect notes only related to specific projects\n- Just collecting unprocessed fleeting notes\n\n", "links": ["o2aK159Q"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 1}, {"id": "o2aK159Q", "name": "Slip box", "labels": [], "text_md": "- The slipbox is a simple external system to organize one's thought, ideas, and collected facts.\n- To be effective, it has to be embedded in one's overarching workflow (daily routine)\n- Video of Prof. Niklas Luhmann: https://www.youtube.com/watch?v=qRSCKSPMuDc&feature=youtu.be&t=37m30s\n\n", "links": ["bit4HVlF", "pS33XHDA", "f6AE5k9M"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 3}, {"id": "pS33XHDA", "name": "Knowledge management systems", "labels": [], "text_md": "- We need to compensate for the limitations of our brains by relying on external structures ('scafolding') to capture ideas and supports our thinking process.\n- Knowledge management systems help keep track of ever-increasing volume of information and relieve brain capacity to focus on what is important\n\n\n", "links": ["UXewQVNc", "bit4HVlF", "o2aK159Q", "6bvsiUUz", "Z12lJnJp", "3cbc3DFd"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 6}, {"id": "d700Xe9w", "name": "Innovation is not a linear process", "labels": [], "text_md": "- The quest for innovation requires to constantly iterate between different tasks\n- The search for meaningful connections is a crucial part of any innovation processes (p. 114).\n- Any attempt to squeeze a non-linear process into a linear order only leads to problems and frustrations\n\n", "links": ["nBYBmeNa", "MVQ6AEt4", "onHn6c4S"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 3}, {"id": "onHn6c4S", "name": "Iterative abstraction / re-specification", "labels": [], "text_md": "- Innovation requires to combine and re-combine ideas, liberating them from their original context by means of an iterative process of abstraction and re-specification. (Ahrens, 2017, p. 123).\n- Abstraction from concrete situations and re-specification allows to apply ideas from one practical context into another.\n\n", "links": ["d700Xe9w", "5WJqHcE5"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 2}, {"id": "QZ4e6WSS", "name": "Organization of data innovation", "labels": [], "text_md": "Success in data innovation depends to a large extent on the adequate organization of workflows for the practical implementation of new sources, technologies and methodologies. \n\nIt requires **breaking down the amorphous task of \"data innovation\" into  separable tasks**, which can be completed within reasonable time, and which are clearly connected to the delivery of specific, tangible outputs, and finally to the achievement of well-defined outcomes.\n\n", "links": ["oTbmWlt2", "94L4EBHO", "QJsx3KmY", "nBYBmeNa"], "quotes": [], "sources": [], "n_links": 4}, {"id": "6bvsiUUz", "name": "Importance of an overarching workflow", "labels": [], "text_md": "- It is crucial to maintain a \"holistic perspective\" so everything that needs to be taken care of is in one place and can be processed in a standardized way\n- Having a simple, overarching and streamlined workflow in place helps to stay in control by focusing on the important things and being able to pick up tasks quickly where they are left off.\n\n\n", "links": ["pS33XHDA", "Nwuk7TBb", "nDxqu3EV", "MJyDsMeW", "XkfA6AFU", "nBYBmeNa", "eac4gaf6"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 7}, {"id": "nDxqu3EV", "name": "Lessons from the shipping container", "labels": [], "text_md": "Ahrens (2017) explains how the initial attempts to introduce the use of the shipping container --a very simple solution--failed as long as ship owners failed to change their infrastructure and routines and to recognize that what mattered was the entire transport chain, from packaging of goods at the point of production to their delivery at the final destination.\n\n> It wasn\u2019t just another way of shipping goods. It was a whole new way of doing business. \n>(Ahrens, 2017, p. 40) \n\nSimilarly, simple innovations in statistical production can only be mainstreamed if they are accompanied by necessary changes and adaptations along the whole data value chain.\n\nFor example... (?)\n\n", "links": ["6bvsiUUz", "OkCaKf6I"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 2}, {"id": "OkCaKf6I", "name": "SDMX is the shipping container of official statistics", "labels": [], "text_md": "Paraphrasing https://trello.com/c/RnpNCnXf, \"\\[SDMX\\] is the shipping container of the \\[official statistics\\] world.  Instead of having different \\[mechanisms for the exchange of\\] different \\[datasets\\], everything goes into the same \\[multi-dimensional schema\\] and is standardized into the same format. (...) Everything is streamlined towards one thing only: \\[statistical data\\] that can be \\[easily exchanged with, and utilized by, users\\].\n\n", "links": ["nDxqu3EV", "6OomPFuc", "UHxZBMU2", "cTjLfkWN"], "quotes": [], "sources": [], "n_links": 4}, {"id": "nBYBmeNa", "name": "Innovation: Plans vs structured workflows", "labels": [], "text_md": "- **Innovation requires flexibility**. Detailed plans often impose too much structure for open-ended research or innovation projects that require flexibility.  \n- **Innovation cannot be predetermined**: Initial ideas are necessarily vague and change when we put them into practice.\n- Accidental encounters make up the majority of what we learn\n- The challenge is to have **overarching workflows** that allow for new ideas and insights to be generated, tested, adapted and mainstreamed\n\n", "links": ["94L4EBHO", "6bvsiUUz", "XkfA6AFU", "d700Xe9w", "QZ4e6WSS", "eac4gaf6", "uhh1kRu3"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 7}, {"id": "uhh1kRu3", "name": "Innovation starts with what you have", "labels": [], "text_md": "- Nobody ever starts from scratch. Innovation projects should start with what you *have*, and not with an unfounded idea about what the data, technology or methods that you are planning to acquire or develop might eventually provide.\n\n", "links": ["CHr0Qnj2", "nBYBmeNa"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 2}, {"id": "CHr0Qnj2", "name": "Prioritize already available data and technology resources", "labels": [], "text_md": "Data innovation projects should prioritize and maximize the use of data sources and tools that are already under the control of the NSO and other members of the National Statistical System.\n\n", "links": ["uhh1kRu3"], "quotes": [], "sources": [], "n_links": 1}, {"id": "XkfA6AFU", "name": "Exergonic vs endergonic workflows", "labels": [], "text_md": "Ahrens (2017) explains that workflows can be characterized as either \"exergonic\" (requiring constant addition of energy to keep them going) or \"endergonic\" (once triggered, they continue by themselves and even release energy).\n\nGood (endergonic) workflows turn into **virtuous cycles** where the experience of becoming better at what we do motivates us to take on the next task (Ahrens, 2017, p.53). \n\nSuch workflows need to include a **learning system** based on actionable **feedback loops**.\n\n", "links": ["6bvsiUUz", "eac4gaf6", "nBYBmeNa"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 3}, {"id": "FlUbxxop", "name": "Be flexible - don't cling to a fixed idea", "labels": [], "text_md": "- Don't cling to an idea if another, more promising gains momentum\n- Follow your interest and always take the path that promises the most insight\n- The more you become interested in something, the more likely it is that you will generate insights from reading, taking notes, and writing about it\n\n", "links": ["ZyufNNxO"], "quotes": ["qh8vQ4LG"], "sources": ["RnpNCnXf"], "n_links": 1}, {"id": "1j2ClWxr", "name": "Understanding the how and the why", "labels": [], "text_md": "Only after we understand why and how new technologies and methods work, are we able to tweak them for our own needs.\n \n", "links": ["U3iX23IM", "DLdh6sOz", "zGHMrgZh", "HZvkseXI"], "quotes": ["dEE3HyCw"], "sources": ["RnpNCnXf"], "n_links": 4}, {"id": "DLdh6sOz", "name": "Nothing is more practical than a good theory", "labels": [], "text_md": "Facts need to \"hang together on a latticework of theory\" in order to provide  insights that can be used to systematically solve real-world problems.\n\n", "links": ["1j2ClWxr"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 1}, {"id": "QGyxiutK", "name": "Key principles of data innovation projects", "labels": [], "text_md": "1. Country ownership\n\nWithout country ownership, and funding, a data innovation project will not survive very long. Decision makers in government and senior NSO officials need to be convinced that the project will help drive the national strategy for the development of statistics and other national policy agendas. \n\n2. Sustainability\n\n3.  Long-term relevance\n\nAdequate planning of data and infrastructure is essential to ensure country ownership, sustainability and long-term relevance. \n\n", "links": ["wFd1FvbC", "1P7GAwhy", "CNiPnq0T", "U3iX23IM", "n3KkkMdF", "zGHMrgZh", "3jLAEhY0"], "quotes": [], "sources": [], "n_links": 7}, {"id": "38O0h16q", "name": "Project champions", "labels": [], "text_md": "To be successful, data innovation projects need to identify champions willing to put their names behind them, who can mobilize resources and institutional support, as well as facilitate collaboration across different organizations.\n\nNational champions are especially crucial to help broker collaboration with key government agencies and partners. \n\n", "links": ["38O0h16q", "2j4pK36q"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 2}, {"id": "47pZOC1r", "name": "Product vs. Process Ownership", "labels": [], "text_md": "\"Owning a process\" requires a different perspective than the more familiar \"owning a product\"\n\n", "links": ["TlYLFIPL", "3ETVkGDf"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 2}, {"id": "3ETVkGDf", "name": "Total cost of ownership", "labels": [], "text_md": "The \"total cost of ownership\" is an estimate of the direct cost of developing and applying a particular system.  It includes:\n\n- Software/hardware costs\n- Operational costs (including maintenance of physical infrastructure)\n- Personnel costs (including hiring and training costs of both permanent and temporary staff)\n\n", "links": ["47pZOC1r", "U3iX23IM", "TlYLFIPL"], "quotes": [], "sources": [], "n_links": 3}, {"id": "U3iX23IM", "name": "National ownership of data innovation projects", "labels": [], "text_md": "National ownership of data innovation projects requires local staff to have the ability and confidence to make the right choices regarding sources, technology and methods in real situations.\n\nCapacity development projects try to make learning easier for NSO staff by prearranging information, sorting it into modules, categories and themes, However, they achieve the opposite if they take away the opportunity to build meaningful connections to their needs and context.  \n\n", "links": ["wFd1FvbC", "QJsx3KmY", "QGyxiutK", "1j2ClWxr", "oTbmWlt2", "3ETVkGDf", "zGHMrgZh"], "quotes": ["dEE3HyCw"], "sources": [], "n_links": 7}, {"id": "wFd1FvbC", "name": "Ensuring national ownership of data innovation projects", "labels": [], "text_md": "**Risk**: \nExternal partners may be perceived by local teams as exerting too much ownership over the process\n\n**Mitigation**: \n- A specific entity within each country (usually the NSO) should be identified and recognized as the **owner** of every data innovation project. \n- This entity should be responsible to ensure the quality of outputs and should be willing to commit staff and invest resources in the project. \n- The main **focal point** for every country project should be within that entity, and all inquiries should be referred to that focal point first. \n- The first publication of all results should be done by the country, and publications by external partners should make reference to national publications\n\n\n\n", "links": ["3c7dZcB6", "U3iX23IM", "QGyxiutK", "3jLAEhY0"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 4}, {"id": "CNiPnq0T", "name": "Long-term relevance of data innovation projects", "labels": [], "text_md": "**Risk:**\nProject outputs and the data sources, methods and technologies used to generate them can become quickly outdated, due to rapid social, economic, environmental or technological changes.\n\n**Mitigation**:\n\n- Setup a continuous maintenance programme to keep IT infrastructure and systems up to date (e.g., software updates)\n- Update and release new results as new data inputs become available\n\n", "links": ["jwQ8qCBg", "QGyxiutK", "P43SdKCs", "1k64R7UM", "oTbmWlt2", "zGHMrgZh"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 6}, {"id": "3jLAEhY0", "name": "Importance of funding from national governments", "labels": [], "text_md": "Over the long term, building institutional capacity requires government funding.\n\nData innovation projects need to secure financial resources at the country level:\n\n- specific budget allocations\n- grants from external donors\n- shared resources from existing projects\n\n", "links": ["wFd1FvbC", "QGyxiutK"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 2}, {"id": "J7uVnj8B", "name": "Data privacy issues in data innovation projects", "labels": [], "text_md": "Individual-records collected from surveys, censuses, administrative sources, and telecommunication providers, etc. are highly sensitive.  \n\nData innovation projects that use any of these sources need to have data privacy and security protocols in place when linking records from different data sources through common identifiers of individuals, households, businesses or geographies. \n\nThis is fundamental to avoid any reputation damage and ensure that organization that owns the project is trusted by all stakeholders, thus being able to keep the project running in the long term.\n\n", "links": ["9h1AmchL"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 1}, {"id": "hp8Foh4V", "name": "Capacity bottlenecks in data innovation projects", "labels": [], "text_md": "Common capacity bottlenecks in data innovation projects include:\n \n- Shortage of **facilities or equipment**\n- Gaps in technical **skills**\n- Lack of **staff time**\n- **Data quality** issues\n- Shortage of modern **software** tools\n\n\n", "links": ["VG23wXyp", "Vk1c3Bjw", "ptDLJoDD", "Vmp2TR2i", "yHy01yYb"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 5}, {"id": "ptDLJoDD", "name": "Facilities and equipment requirements", "labels": [], "text_md": "Data innovation projects require adequate IT infrastructure (e.g. server capacity for computing and data storage, personal computers, as well as broadband internet connectivity) and physical facilities to conduct planning and training workshops, on-site collaboration sessions, etc.\n\n", "links": ["hp8Foh4V"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 1}, {"id": "zGHMrgZh", "name": "Capacity building and sustainability", "labels": [], "text_md": "One of the main objectives of capacity building is to enable local staff to become technically **self-reliant** and to gain **confidence** and a strong sense of **ownership**.\n\nLocal staff should be able to understand and to explain to others not only the final outputs, but all aspects of the process, from beginning to end.\n\nCountry officers need to be able to replicate and update the outputs on a regular basis by their own means.\n\n", "links": ["QGyxiutK", "1j2ClWxr", "U3iX23IM", "HZvkseXI", "d3ru9pGK", "ilCtHTAx", "QRWAHOzF", "CNiPnq0T"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 8}, {"id": "d3ru9pGK", "name": "Building data engineering skills in NSOs", "labels": [], "text_md": "The ability of NSOs to efficiently integrate multiple data inputs into valuable statistics is increasingly dependent on their data infrastructure and \"data engineering\" skills.  \n\nDifferent types of data repositories and architectures require vastly different technical skills.  Statisticians need to develop a broad rang of foundational data engineering skills to  mainstream new data sources, methods and technologies into regular statistical production programmes.\n\nStatisticians are rarely given analysis-ready data that can be directly used in statistical estimation and compilation.  One of the main challenges in official statistics is to design, build and maintain data integration, processing, and dissemination pipelines. \n\nPractical data innovation projects in official statistics require the ability to extract, organize and manipulate raw, unstructured source data, and to transform it into \"clean\" datasets that can be processed and analyzed using by standard software programmes and methodologies.\n\n", "links": ["WnQYbC1N", "NPfQeP5J", "Cp7Quz8B", "uKiS7iJH", "62wtBu4C", "jU3e4dIS", "Vmp2TR2i", "oTbmWlt2", "MhXSqPKk", "zGHMrgZh"], "quotes": [], "sources": ["4z5iFWrh"], "n_links": 10}, {"id": "Cp7Quz8B", "name": "Capacity building in data engineering", "labels": [], "text_md": "Most capacity building in data innovation projects tends to focus on \"high-level\" skills such as artificial-intelligence, machine learning, geoprocessing, or sophisticated estimation methodologies.   \n\nHowever, training workshops in data innovation usually neglect foundational data engineering skills, such as design of table schemas and practical implementation of data pipelines. \n\nAlthough not every statistician needs to become an expert in data engineering, NSOs need to have sufficient in-house data engineering skills to address critical real-life challenges in mainstreaming their data innovation projects into their regular official statistics production processes.\n\n", "links": ["Vmp2TR2i", "d3ru9pGK"], "quotes": [], "sources": ["4z5iFWrh"], "n_links": 2}, {"id": "4RDMe98t", "name": "Technical experts", "labels": [], "text_md": "Data innovation projects need to identify and bring on board technical experts who are deeply knowledgeable of the data inputs, technologies, and methods being pursued, and with ample practical experience in their implementation in the field.\n\nIt is particularly important to partner early on with local experts (e.g., form UN Country Teams, World Bank, government agencies and local universities and think tanks), in order to stimulate interest in the project outcomes and learn more about country needs and priorities. \n\n\n\n", "links": ["2j4pK36q"], "quotes": ["dEE3HyCw"], "sources": ["UCi2QXBJ"], "n_links": 1}, {"id": "Vmp2TR2i", "name": "Data engineering curriculum for official statisticians", "labels": [], "text_md": "In the past, teams working on data innovation projects used to be able to get away with just knowing the basics of data storage and ETL infrastructure. Today, however, data innovation teams need to have a good understanding of how different types of databases can be set up, accessed and integrated, and how to setup and orchestrate the infrastructure and data management environments needed to develop, test and deploy different analytic methods. \n\nData innovation training curriculum for official statisticians should develop data engineering skills on the following topics:\n\n- **Data management and integration** \n    - Basic data modelling\n    - Transcoding and record-linking methods\n    - Statistical disclosure control methods\n    - Data warehousing / ETL pipeline design patterns and techniques\n- **Cloud computing**\n  - Understanding distributed computing\n  - How to evaluate and implement different cloud service models\n\n\n   \n\n", "links": ["2Daj1ZPE", "dJBcDmmd", "MfTvaMIL", "vKQFuLMb", "Iaba3RcP", "ilCtHTAx", "9PrGSBNy", "lEHhU8Ma", "WnQYbC1N", "QJsx3KmY", "1P7GAwhy", "Cp7Quz8B", "d3ru9pGK", "ulLdtnHy", "MhXSqPKk", "P43SdKCs", "JiEdFdtg", "hp8Foh4V", "UHxZBMU2", "jU3e4dIS"], "quotes": [], "sources": [], "n_links": 20}, {"id": "5W8MgJOp", "name": "Data Partitioning", "labels": [], "text_md": "**Data Partitioning** - breaking up data into independent, self-contained chunks, instead of storing in a single table or file.\n\nIt is \"a practice that enables more efficient querying and data backfilling\" (Chang, 2018b)\n\n", "links": ["FOPuLnHI"], "quotes": [], "sources": ["oIarYHfE"], "n_links": 1}, {"id": "Iaba3RcP", "name": "Key role of data warehouses", "labels": [], "text_md": "- \"A data warehouse is a place where raw data is transformed and stored in query-able forms\"\n- \"Data warehouses are both the engine and the fuel that enable higher level analytics\"\n- \"Without...foundational warehouses, every activity related to data science becomes either too expensive or not scalable\"\n\n", "links": ["CvXRU020", "yerBxsCk", "5k2KowcP", "WV9JV2CP", "6OomPFuc", "Vmp2TR2i", "4HybrQML", "Aslc8kRG"], "quotes": [], "sources": ["4z5iFWrh"], "n_links": 8}, {"id": "UHxZBMU2", "name": "Data Modelling", "labels": [], "text_md": "**Data Modeling** is \"a design process where one carefully defines table schemas and data relations to capture business metrics and dimensions\". (Chang, 2018b)\n\nData modelling is about optimizing data structures for the purpose at hand. \n\n- Data modelling patterns used in business intelligence or for analytic purposes often involve sacrificing data normalization (i.e., accepting more data redundancy and more complex ETL pipelines to maintain) in order to facilitate data queries from tables where metrics and dimensions are already pre-joined.\n\n", "links": ["lEHhU8Ma", "5k2KowcP", "OkCaKf6I", "6OomPFuc", "Vmp2TR2i"], "quotes": [], "sources": ["oIarYHfE"], "n_links": 5}, {"id": "6OomPFuc", "name": "Star schema", "labels": [], "text_md": "Data warehouses generally implement a simple **star schema**, which consists of normalized fact and dimension tables that can be easily used to build denormalized tables for analytic purposes.\n\nA star schema design helps balance between ETL maintainability and ease of analytics.\n\n*Fact tables* - Contain the business metrics of interest\n*Dimension tables* - Contain slowly changing attributes (often organized in a hierarchical structure) that can be joined with the fact tables\n\n", "links": ["Iaba3RcP", "OkCaKf6I", "UHxZBMU2", "Wo49hyYS", "3BmAWMzw", "Aslc8kRG"], "quotes": [], "sources": ["oIarYHfE"], "n_links": 6}, {"id": "oTbmWlt2", "name": "Mainstreaming data innovation projects", "labels": [], "text_md": "It is crucial to ensure that the objectives and scope of national data innovation projects fit with the priorities of National Strategies for the Development of Statistics, with the regular work programme of NSOs and with the overall institutional setting of the National Statistical system.\n\nFor instance, there has to be a clear link between project outputs and SDG reporting platforms and VNR processes to ensure sustained demand from policy makers and other key stakeholders.\n\n", "links": ["U3iX23IM", "d3ru9pGK", "n3KkkMdF", "QZ4e6WSS", "5u1sLb2p", "CNiPnq0T"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 6}, {"id": "n3KkkMdF", "name": "Mainstreaming projects:  sustainability", "labels": [], "text_md": "When data innovation projects are viewed only as ad hoc activities, it is very difficult to secure long-term resources.   To be sustainable, data innovation projects need to be aligned with over-arching strategic plans and be embedded within the organization's existing multi-year programme of activities. This includes ensuring that the project activities are funded through regular budget sources and that they are adequately staffed over the long term. \n\n\n\n", "links": ["oTbmWlt2", "QGyxiutK"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 2}, {"id": "5WJqHcE5", "name": "Abstraction and scalability", "labels": [], "text_md": "Complexity increases when NSOs seek to mainstream data innovations beyond pilot projects run by a small team.  \"This makes enforcing ETL best practices, upholding data quality, and standardizing workflows increasingly challenging.\" \n\nIdentifying and automating common ETL patterns into standard workflows allows to leverage the power of abstraction in order to address scalability challenges.\n\nThis requires both governance and re-usable technologies\n- repositories (git)\n- notebooks (Jupyter)\n- containers (Docker)\n\n", "links": ["onHn6c4S"], "quotes": [], "sources": ["HKijdzxQ"], "n_links": 1}, {"id": "ilCtHTAx", "name": "Training for data innovation projects", "labels": [], "text_md": "Data innovation teams need to build a wide range of **data analysis and management** skills, including:\n- data engineering\n- data science\n- IT infrastructure engineering\n- statistical and econometric modeling\n- GIS analysis\n\nIt also requires building **\"soft\" skills**, such as project management, fund-raising, and communication.\n\n\n\n", "links": ["P43SdKCs", "JiEdFdtg", "Vmp2TR2i", "53FNlsMn", "zGHMrgZh"], "quotes": [], "sources": [], "n_links": 5}, {"id": "P43SdKCs", "name": "Training materials for national staff", "labels": [], "text_md": "To enable national staff to reproduce the outputs of data innovation projects and to utilize innovative data sources, technologies and methods on a sustainable basis, it is crucial to develop training materials and knowledge resources tailored to their own needs and context.\n\n- In their own language\n- Applicable in their existing technological infrastructure\n\n", "links": ["h1lCewmo", "JiEdFdtg", "ilCtHTAx", "Vmp2TR2i", "53FNlsMn", "CNiPnq0T"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 6}, {"id": "eac4gaf6", "name": "Why workflows become complicated", "labels": [], "text_md": "- Workflows become clogged over time as we try to apply a variety of new approaches and techniques, each promising to make something easier or better, but which combined have the opposite effects.\n- When new techniques are used without regard to the overarching workflow, \"nothing really fits together\", every little step suddenly becomes its own project, and it becomes very difficult to get things done.\n\n\n", "links": ["Nwuk7TBb", "JExDAHlM", "6bvsiUUz", "MJyDsMeW", "XkfA6AFU", "nBYBmeNa"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 6}, {"id": "JExDAHlM", "name": "Simplicity is paramount", "labels": [], "text_md": "- Big transformations start with simple ideas.  What matters is how well these simple ideas fit in the overall workflows of a system or organization.\n- To avoid undesired side effects, it is important to focus on small units of work.\n\n\n", "links": ["TBRkKxJ7", "5sMdUJ5Y", "3c7dZcB6", "vLPsC8l0", "eac4gaf6"], "quotes": [], "sources": ["RnpNCnXf"], "n_links": 5}, {"id": "JiEdFdtg", "name": "Engaging local universities and training institutes", "labels": [], "text_md": "To promote sustainability and national ownership, data innovation project teams need to partner with local universities or training institutes in order to train analysts at national statistical offices and other government agencies.\n\n", "links": ["QJsx3KmY", "P43SdKCs", "ilCtHTAx", "2Daj1ZPE", "MvmET0Kx", "Vmp2TR2i"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 6}, {"id": "jwQ8qCBg", "name": "Loss of expertise through staff attrition", "labels": [], "text_md": "**Risk**: \n\nLoss of expertise through staff attrition or turnover\n\n**Mitigation**: \n\n- Maintain focus on institutional capacity rather than on individuals \n- Promote knowledge sharing and team collaboration (e.g., working in pairs and peer reviews)\n\n", "links": ["QRWAHOzF", "CNiPnq0T"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 2}, {"id": "HZvkseXI", "name": "Delivering methodology as a blackbox", "labels": [], "text_md": "Some aspects of data innovation projects are highly technical.  There is a risk of \"delivering methodology as a black box\", making it impossible for local teams to maintain and update the project results in the long run.\n\n", "links": ["1j2ClWxr", "zGHMrgZh"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 2}, {"id": "QRWAHOzF", "name": "Working in pairs to ensure sustainable skill building", "labels": [], "text_md": "In the face of high staff turnover rates, organizations face the challenge of retaining over the long term the skills acquired during training activities and hands-on implementation activities. \n\nTo address this challenge, a good practice is to ensure that in every activity related to the implementation of a project feature, at least two team members take the lead and are jointly responsible for the deliverable, and that they work via pairing and review each other's work,\n\n", "links": ["jwQ8qCBg", "zGHMrgZh"], "quotes": [], "sources": [], "n_links": 2}, {"id": "5u1sLb2p", "name": "Objectives of data innovation for Sustainable Development", "labels": [], "text_md": "- Data innovation projects should enable countries to *regularly* produce better, more timely and more disaggregated data to inform policies and decisions that contribute to achieving the 2030 Agenda for Sustainable Development\n- Measures of success: Data is available and openly accessible online to policy and decision makers in easy-to-use formats and presentations.\n  - Frequency (at least once a year)\n  - Time lag (less than 2 years)\n  - Geographic coverage (national coverage)\n  - Geographic disaggregation (at least 3rd level)\n  - Coverage of specific population groups (women, disabled, youth, elderly)\n\n", "links": ["0zGb0Ntw", "QJsx3KmY", "jt99xW61", "EojLU88D", "ttc01lbK", "oTbmWlt2", "tnUhXJcI"], "quotes": [], "sources": [], "n_links": 7}, {"id": "vlQ3wkHh", "name": "Gaps in poverty data", "labels": [], "text_md": "Traditional poverty measures are generally available at low frequency intervals (e.g., every ten to five years) and only at highly aggregated levels of granularity\n\n>*As of \\[ \\], \\[  \\] countries have not poverty data at all for the period 2005-2019, and \\[   \\] have only one data point.   For half of in the global database, the most recent data point is for the year \\[  \\] or earlier. *\n\n\n", "links": ["jt99xW61", "5Y5WIm31"], "quotes": [], "sources": [], "n_links": 2}, {"id": "jt99xW61", "name": "Objectives of Data4Now country projects", "labels": [], "text_md": "The Data4Now initiative aims to provide timely and disaggregated information needed by policy and decision makers to better design development strategies and deliver public policies to achieve the 2030 Development Agenda.  \n\nIt mobilizes resources and partnerships to support countries in the use of innovative technologies, data, and methods to obtain insights on key aspects of sustainable development, such as poverty, food security, education, health, disaster-risk resilience, etc.\n\nIts addresses all aspects of the data lifecycle, from data integration and data engineering to the use of data science methods and data communication and visualization tools.\n\nCollaboration in multi-functional teams covering different areas of expertise allows participating countries to leverage synergies and interlinkages between different domain areas.  For instance: \"How are drought patterns and commodity prices correlated with crop yields, poverty estimates, and population movements?\"\n\nThe initiative promotes the systematic use of sound data quality frameworks and data ethics principles in data innovation projects. \n\n", "links": ["vlQ3wkHh", "62wtBu4C", "5u1sLb2p"], "quotes": [], "sources": [], "n_links": 3}, {"id": "1k64R7UM", "name": "Policy relevance of data innovation projects", "labels": [], "text_md": "To ensure broad support to data innovation projects, it is crucial to identify specific elements in the policy process where the outputs of the project will provide immediate value.\n\nIn order for the project to become part of the regular statistical production process, there most be a continuous search for new areas in the policy-making process that could benefit from it. \n\n", "links": ["CNiPnq0T"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 1}, {"id": "ttc01lbK", "name": "Defining data disaggregation priorities", "labels": [], "text_md": "The definition of national data priorities include the definition of priority dimensions for which disaggregated data needs to be available:\n\n- Key population groups (e.g., women, urban/rural population, indigenous groups, youth, elderly population, people living with disabilities, ...)\n- Required level of granularity in geographic disaggregation\n\n", "links": ["5u1sLb2p", "qpy6M6vh"], "quotes": [], "sources": [], "n_links": 2}, {"id": "QJsx3KmY", "name": "Data innovation road map", "labels": [], "text_md": "A road map of consisting of country-specific activities, intermediate outputs and expected outcomes\n\n**Pre-production:**\n\n1. Define **scope**\n1. Identify **key stakeholders**\n1. Build **suport** \n1. Reach out to **potential users**\n1. Secure **resources**\n1. Establish **project team**\n1. Procure/setup/configure the necessary **hardware and software** tools\n1. Secure **access to data** inputs\n1. Design, build and run source data integration process\n1. Assess **quality of source data**\n1. Transform original source data into **analysis-ready datasets**\n1. Set up **data inputs clearinghouse**\n1. Provide **practical training**\n\n**Production:**\n\n1. Compute intermediate indicators to be used as (geospatial) covariates in statistical estimation models (e.g., distance to service-delivery points, distance to roads, elevation, ...)\n\n\n**Post-production:**\n\n1. Communicate project outputs to key audiences \n\n", "links": ["CvXRU020", "2j4pK36q", "2Daj1ZPE", "kCnh0WC4", "QZ4e6WSS", "5u1sLb2p", "yHy01yYb", "VG23wXyp", "5Y5WIm31", "3c7dZcB6", "Cr014GcV", "9h1AmchL", "h1lCewmo", "1P7GAwhy", "XVq7u8bY", "Vmp2TR2i", "U3iX23IM", "JiEdFdtg", "mmwn1oEU"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 19}, {"id": "XVq7u8bY", "name": "Poverty map production: basic steps", "labels": [], "text_md": "Estimation of small-area poverty maps usually follows methodology developed by the World Bank over more than 20 years, combining census data with household survey data. This methodology usually includes the following general steps:\n\n1. Verify quality and comparability of Household survey data and census data\n2. Verify quality of additional geospatial co-variates\n3. Estimate a model of household consumption, based on data sample from household survey (using only variables that are available in both survey and census data) and geospatial co-variates.\n4. Apply estimated parameters to census data and geospatial co-variates in order to estimate consumption per capita for all households in the census\n5. Apply appropriate poverty lines to estimate poverty rates at various levels of aggregation\n6. Build confidence intervals using standard errors\n7. Use GIS to produce visualizations of the results\n\n\n", "links": ["QJsx3KmY", "nqqQlCkb", "5Y5WIm31"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 3}, {"id": "nqqQlCkb", "name": "Poverty maps production: data sources", "labels": [], "text_md": "The production of small-area poverty estimates usually relies on the following two major data sources of household welfare:\n\n1. Detailed household surveys which collect a measure of welfare (typically consumption per capita)\n2. A national census or large national survey that includes a significant share of the country's population\n\nOther data sources that may be used to approximate individual welfare (in approximately real time):\n\n- Individual consumption of mobile phone services\n- Measures of mobility (e.g., derived from mobile phone records)\n- Social network metrics (e.g., derived from social media or mobile phone records)\n- Financial transactions (e.g., derived from mobile phone or credit card records)\n\n", "links": ["XVq7u8bY", "njfyBuRv", "exFglBfe", "RHCUOmoR", "Lr60zbkO", "vWWASKo8"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 6}, {"id": "qpy6M6vh", "name": "Geo-spatial disaggregation", "labels": [], "text_md": "The geographic scale at which data are disaggregated affects the type of information that can be derived from it.  \n\nData collected at more granular level of geographic detail often provide more useful information about patterns of geographic variation and correlation, and can be more flexibly aggregated into broader geographic areas suitable for different types of analysis.\n\nExamples of geographic scales:\n\n- Administrative unit (level 0, level 1, level 2...)\n- Populated place\n- Point location\n- Grid location\n- Area/Line features\n\nDefining a minimum level of geospatial disaggregation facilitates data integration and analysis.  Such minimum level of geospatial disaggregation should be established considering factors such as:\n\n- **Analytic objectives** (e.g., informing policies or decisions at national or local levels)\n- **Country context** (e.g., how large are the different levels of administrative units)\n- Expected **variability** within and across different units at each level of disaggregation (e.g., too much variability within units / too little variability between units may require more granular disaggregation)\n- Type of **data collection process** (e.g., earth observation, on-site measurements, or administrative/business records). \n- Potential **privacy** implications (e.g., whether combining data at a specific level of geographic disaggregation with other datasets could lead to re-identification of personal or individual-level information)\n- **Resources** (e.g., expertise and technology required to collect, process, and analyze data at a specific level of geographic desaggregation).\n\n", "links": ["bs0lGShz", "tltgpLsL", "iJY1iH4F", "pXxz1MHR", "ttc01lbK"], "quotes": [], "sources": ["XN9jt6q8"], "n_links": 5}, {"id": "vWWASKo8", "name": "Additional geo-referenced information used in poverty estimation", "labels": [], "text_md": "- Points of service delivery (e.g., schools, health centers, boreholes...) and their attributes (e.g., number of beds in hospitals; number of health personnel)\n- Networks of infrastructure (e.g., roads, electricity, water...) and their attributes (e.g., condition/quality of roads)\n- Natural features (e.g., elevation, agroclimatic characteristics...)\n\n", "links": ["nqqQlCkb"], "quotes": [], "sources": [], "n_links": 1}, {"id": "exFglBfe", "name": "Challenges in using mobile phone records", "labels": [], "text_md": "- Mobile phone data are typically biased and not representative of the entire population of interest, due to factors such as unequal phone penetration and differences in market share by various carriers\n- Working with mobile phone records requires additional privacy protection measures. \n\n", "links": ["nqqQlCkb"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 1}, {"id": "njfyBuRv", "name": "Challenges in household survey data", "labels": [], "text_md": "Household surveys provide rich information on the living standards and other characteristics of a sample of households.  However, their sample size is typically not large enough to obtain reliable estimates of poverty levels below the first sub-national administrative unit.\n\n", "links": ["nqqQlCkb"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 1}, {"id": "94L4EBHO", "name": "Agile project management", "labels": [], "text_md": "The deliverables (user stories) of a project are specified collaboratively over short, time-blocked iterations called sprints.  \n\nMembers of self-organized, cross-functional teams work together on common tasks, instead of working in isolation. Agility emphasizes frequent communication and empowerment of team members, as well as flexibility to change course at any stage of the project in order to meet the evolving stakeholders' needs. \n\nAgile project management avoids big up-front architecture design decisions, seeking instead to combine a small measure of up-front design with a healthy dose of emergent, just-in-time design.  Teams are encouraged to quickly explore new ideas and approaches and to learn fast whether a potential solution is viable or not. \n\nAn iterative process allows customers to discover and refine their own requirements as they gain information and knowledge in successive sprints. At the start of each sprint, the team decides which high-priority deliverables to work on so as to ensure constant forward momentum.  Each sprint includes activities for designing, building, testing, reviewing and launching specific product features, and the focus is on delivering working features at the end of each iteration that contribute to the final product. \n\n\n", "links": ["wiYBU1Oc", "TBRkKxJ7", "HhsYzH6I", "MVQ6AEt4", "MkZau9Uc", "QBMWnA4E", "QZ4e6WSS", "nBYBmeNa", "J4RjLkh4", "duUZSb3j"], "quotes": [], "sources": ["VJ2nXM1R"], "n_links": 10}, {"id": "2j4pK36q", "name": "Key stakeholders in data innovation projects", "labels": [], "text_md": "In every data innovation project, it is very important to identify from the beginning a broad range of actual and potential stakeholders who may become involved in various phases of the project. This includes:\n\n- **Data providers** (line ministries, land administration, mobile phone companies, industry regulators, business associations, local governments, civil society organizations, space agencies, tech companies...)\n- **Technology providers** (e.g., national data centers, intl. organizations, private sector)\n- **Knowledge and expertise providers** (e.g., international organizations, research institutes, universities...)\n- **Funding providers** (e.g., ministry of planning/finance, intl. donors)\n- **Data users** (e.g., line ministries, parliament, local governments, international organizations, civil society organizations, intl. donors)\n\nIt is also crucial to establish from the beginning both institutional and personal links with these stakeholders, and to involve them early on in the planning and execution of project activities.\n\n", "links": ["4RDMe98t", "38O0h16q", "QJsx3KmY", "UejoKjiC"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 4}, {"id": "VG23wXyp", "name": "Hardware and software tools", "labels": [], "text_md": "Data innovation projects need to procure, setup and configure a number of hardware and software tools\n\n- Statistical software packages and libraries\n- Database management tools/environments\n- Data visualization tools\n- GIS tools\n\n", "links": ["hp8Foh4V", "QJsx3KmY"], "quotes": [], "sources": [], "n_links": 2}, {"id": "mmwn1oEU", "name": "Establishing a national project team", "labels": [], "text_md": "Each national data innovation project requires dedicated personal, both from the NSO and from other participating agencies, to carry out the work at the country level.  The national data innovation project team should consist of:\n\n- A national project coordinator (bridge between technical team at NSO and other stakeholders)\n- Two statisticians (closely familiar with source data and statistical analysis prcesses)\n- One econometricians (with expertise in nocasting and/or small area estimation techniques)\n- One GIS expert (with expertise in GIS analysis and map visualization tools)\n- One IT focal point (with admin rights and familiar with data security protocols)\n- Two policy experts\n\n", "links": ["Vk1c3Bjw", "wiYBU1Oc", "QJsx3KmY", "5woerok1", "J4RjLkh4", "PjMcjUgb", "irmKQb3k", "duUZSb3j"], "quotes": [], "sources": [], "n_links": 8}, {"id": "duUZSb3j", "name": "Collaborative data innovation projects", "labels": [], "text_md": "Collaboration across multiple organizations towards producing common outputs and achieving a shared outcomes allows data innovation projects to draw a wide range of data assets, skills and resources. \n\nData innovation projects require the collaboration of multiple government agencies and partners, with multi-stakeholder, multi-disciplinary teams working together to:\n\n- address methodological and institutional challenges\n- process and analyze different data inputs\n- share best practices\n- promote experimentation and co-creation of information products\n\n\n\n", "links": ["hGt3as0b", "94L4EBHO", "mmwn1oEU", "J4RjLkh4"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 4}, {"id": "Vk1c3Bjw", "name": "Allocation of staff to data innovation projects", "labels": [], "text_md": "Risk: Lack of time availability of skilled technical staff\n\nMitigation: To be successful, any data innovation project must have dedicated staff, who must be freed up from other duties to participate in it (trainings, data analysis, coordination) \n\n", "links": ["hp8Foh4V", "J4RjLkh4", "mmwn1oEU"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 3}, {"id": "1P7GAwhy", "name": "Sustainable access to source data", "labels": [], "text_md": "To be sustainable, producers of statistics need to have the capacity to regularly access to the necessary input data from external sources.\n\nThis requires:\n- Effective legal data exchange arrangements \n- Adequate incentives and business models\n- Appropriate technical data exchange standards and protocols\n- Good data exchange infrastructure, including connectivity and bandwidth.\n\n", "links": ["Vmp2TR2i", "QJsx3KmY", "QGyxiutK"], "quotes": [], "sources": [], "n_links": 3}, {"id": "lEHhU8Ma", "name": "ETL - Making data analysis-ready", "labels": [], "text_md": "The **'extract-tranform-load' (ETL)** pattern consists of the following steps:\n1. Extracting input data from their original sources\n2. Transforming them it into usable data structures through transcoding, filtering, joining, and aggregation operations\n3. Uploading the transformed data onto a controlled data management environment (such as a data warehouse)\n\nETL pipelines are used to **transform raw data into analysis-ready data**, i.e., making data from heterogeneous sources available to data analysis at a central location, following a specific schema, so they can easily run queries using SQL, feed the data into generic statistical packages, etc.\n\n**ETL tools** allow to pull or receive data from a source system,  perform modifications through a sequence of processing steps, and push the transformed data into a target destination.  \n\n**ETL scheduling** allows to run an ETL process at specific intervals to load raw data into a target system.\n\nOperational databases and other sources of data need to be cleaned, standardized and integrated.  A measure of success for the intermediate goal of preparing a set of analysis-ready data inputs is the ability to combine them in exploratory data visualizations, including map visualizations, \n\n\n", "links": ["CvXRU020", "WnQYbC1N", "2fx14QVS", "yerBxsCk", "etUj8mX6", "xjldGfvp", "1cHc8MiO", "FOPuLnHI", "UHxZBMU2", "2GTwGKsm", "Vmp2TR2i", "4HybrQML", "MhXSqPKk"], "quotes": [], "sources": ["4z5iFWrh"], "n_links": 13}, {"id": "FOPuLnHI", "name": "ETL principles", "labels": [], "text_md": "Principles of good ETL pipelines:\n\n- Partition data tables\n- Load data incrementally\n- Use imutable data tables - so queries return the same result when run against the same business logic and time range\n- Parameterize backfilling logic\n- Run early and frequent data checks: Write data into a staging table first, validate data quality, and only then push to final production table\n- Build alerts and monitoring system\n\n\n", "links": ["WnQYbC1N", "lEHhU8Ma", "Jwlw5emQ", "5W8MgJOp"], "quotes": [], "sources": ["oIarYHfE"], "n_links": 4}, {"id": "CvXRU020", "name": "Logical warehouse of data inputs", "labels": [], "text_md": "Most data innovation projects rely on a large variety of source data generated or compiled by many different governmental and non-governmental organizations.  These sources include many large sets of structured, semi-structured and unstructured data, ranging from earth observation data, call detail records, and sensor data, to microdata from administrative records and data from sample surveys or census programmes.\n\nAs data sources grow, performing data analytics with multiple databases can become inefficient and costly. Thus, as part of a data innovation project, it is necessary to establish \"logical warehouse\" of data inputs, a single point of entry providing access to **analysis-ready, geo-referenced data inputs** from multiple sources, organized according to a simple and commonly agreed taxonomy, such as the **fundamental geospatial data themes** \n\nA lot of work may be required to \"condition\" the different data inputs in order to make them ready for use and analysis, including the adoption of data **interoperability standards and best practices** across different data sources and systems.  \n\nUnderlying this logical warehouse of source data can be a mixture of enterprise data warehouses and data lakes, which work together to provide access to \"immutable' data inputs and allow to trace transformations at a specific step in the pipeline,  thus enabling to test and reproduce estimation results.  \n\n", "links": ["Iaba3RcP", "lEHhU8Ma", "QJsx3KmY", "cTjLfkWN", "yerBxsCk", "WV9JV2CP", "1cHc8MiO", "62wtBu4C", "Cr014GcV", "bR2Woaf4", "4HybrQML", "Aslc8kRG"], "quotes": [], "sources": ["HYBQIQ60"], "n_links": 12}, {"id": "bR2Woaf4", "name": "Types of source data required for nowcasting", "labels": [], "text_md": "Nowcasting is about producing \"near real-time\" estimates.  This means being able to project in over time the values of variables measured in the past.\n\nThis in turn requires to leverage any \"panel components\" in input data form household surveys, administrative records, etc., which track the same individuals, households or statistical units with repeated measurement of the same variable(s) at different moments in time.\n\n", "links": ["CvXRU020", "Lr60zbkO"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 2}, {"id": "Lr60zbkO", "name": "Understanding multiple sources of data", "labels": [], "text_md": "No single individual (or organization) is ever familiar with all the attributes and caveats of all the data inputs that are required in a data innovation project.   Therefore, it is crucial to involve from the beginning all relevant experts who have helped produced various data inputs.\n\n", "links": ["cTjLfkWN", "oBtYWEfG", "nqqQlCkb", "bR2Woaf4", "6WFYziDD", "yHy01yYb"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 6}, {"id": "cTjLfkWN", "name": "How to improve interoperability across data sources", "labels": [], "text_md": "To make various data sources ready-to-use, it is necessary to transform them to ensure they are interoperable. This includes:\n\n- Use of canonical data models\n- Geo-reference all data inputs using common boundaries and use consistent location-identification codes \n- Use common vocabularies, classifications and code lists\n- Develop of standardized API documentation\n\n\n", "links": ["CvXRU020", "tltgpLsL", "OyHGHNbK", "MUnZi5qo", "oBtYWEfG", "OkCaKf6I", "pXxz1MHR", "dva8i69T", "Lr60zbkO", "6WFYziDD", "yHy01yYb"], "quotes": [], "sources": [], "n_links": 11}, {"id": "yHy01yYb", "name": "Assessing quality of source data", "labels": [], "text_md": "- Verify quality of individual data sources\n  - Outlier detection\n  - Completeness of data / missing observations\n  - Sample bias / representativity\n  - Interoperability\n  - Internal consistency\n  - Completeness and clarity of reference metadata\n- Verify comparability / consistency across data sources\n\n", "links": ["QJsx3KmY", "cTjLfkWN", "oBtYWEfG", "hp8Foh4V", "AMu8kLNJ", "Lr60zbkO", "9h1AmchL", "6WFYziDD"], "quotes": [], "sources": [], "n_links": 8}, {"id": "6WFYziDD", "name": "Comparability across data sources", "labels": [], "text_md": "The comparability of variables measured across different data sources is a key data quality issue.  \n\nChanges over time that may adversely impact comparability of variables across different data sets include:\n\n- Changes in the definition and coverage of reference geographic areas (e.g., creation or re-drawing of boundaries of new administrative units)\n- Changes in statistical classifications\n- Changes in sampling methodology\n- Changes in definition of reference time periods (e.g., calendar vs. fiscal year)\n- Changes in survey questions or measurement instruments\n\n", "links": ["tltgpLsL", "yHy01yYb", "cTjLfkWN", "Lr60zbkO"], "quotes": [], "sources": [], "n_links": 4}, {"id": "9h1AmchL", "name": "Establishing trust in results of data innovation projects", "labels": [], "text_md": "It is crucial to establish trust in the results of data innovation projects, particularly from key users such as policy and decision makers.\n\nThe quality of estimation results \"is only as good as the quality of the input data and the methodologies employed.\"  Therefore, the reliability of the sources and methods involved in data innovation projects is central to their widespread acceptance and use. \n\nThis highlights the importance of **transparent and participatory validation**:\n\n- Validate quality of data inputs\n- Validate methodology\n- Ensure that data production process are reproducible by independent reviewers\n- Check for internal and external consistency of results\n- Compare pre-existing users' perceptions with project results\n\n\n", "links": ["QJsx3KmY", "UejoKjiC", "3c7dZcB6", "1cHc8MiO", "yHy01yYb", "rKAEb7YK", "J7uVnj8B"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 7}, {"id": "3c7dZcB6", "name": "Communicating results of data innovation projects", "labels": [], "text_md": "**Risk:**\n\n- Outputs from data innovation projects do not reach their intended users, and remain unused by policy and decision makers\n- Key insights and caveats of project results are not presented in formats that are easy to understand by different types of technical and non-technical users \n- Potential users are not aware of project outputs and/or their value to inform policy and decision making\n\n**Mitigation:**\n\n- Communicate clearly and frequently any limitation of the source data and of the applicability of outputs, with a view to avoiding the building of unrealistic expectations\n- Explain possible applications and demonstrate the value of resulrs at an early stage\n- Develop a results communication strategy aimed to maximize the reach of the outputs and their impact. This strategy may include, among other things:\n  - A **social media campaign** to highlight main results and outputs, and direct user to data and resources to understand the underlying methodologies. \n  - **Online dissemination of results** through the official data dissemination platforms of the NSO and participating government agencies, including: \n    - Downloadable **datasets**\n    - Online **maps and visualizations**\n    - Data stories / **narratives** that put data outputs in context and make them easier to understand for policy and decision makers\n  - Production and distribution of project **reports** in multiple formats\n  - User outreach **events**\n\n", "links": ["wFd1FvbC", "QJsx3KmY", "etUj8mX6", "JExDAHlM", "ClFwmtgA", "9h1AmchL", "tZWRLOUI"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 7}, {"id": "tZWRLOUI", "name": "Elements of a results communication strategy", "labels": [], "text_md": "- Identify potential **users** and how the results of an innovation project help them achieve their goals.\n- Determine the **types of information products / formats / media** that are best suited to the needs of these users\n- Identify / develop **dissemination outlets** that can effectively bring the product of the project to the intended users.\n- Develop a clear policy specifying the terms under which users can access and utilize of both the results of the project and the underlying data inputs\n- Develop complete, concise and clear user documentation explaining the methodology and providing guidance on the appropriate interpretation of the results\n- Make all results available in all the languages in which the key user groups work (e.g., all languages in which the government works)\n\n", "links": ["3c7dZcB6", "AP2e8YNz"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 2}, {"id": "rKAEb7YK", "name": "Official statistics is there for the truth", "labels": [], "text_md": "(paraphrasing Alexander von Humboldt):\n\nOfficial statistics is only there for the truth, and truth is always a public matter.  Everything within official statistics aims at some kind of publication.  Every statistical publication is a public claim on truth.\n\n", "links": ["9h1AmchL"], "quotes": ["falIut3E"], "sources": [], "n_links": 1}, {"id": "Cr014GcV", "name": "Fundamental Geospatial Data Themes", "labels": [], "text_md": "The 14 Global Fundamental Geospatial Data Themes adopted by UNGGIM at its seventh session under decision 7/104 provide a \"taxonomy\" of the geospatial data assets that are needed to enable \"the measurement, monitoring and management of sustainable development in a consistent way over time and to facilitate evidence-based decision making and policy-making.\" \n\nThey can be used to facilitate global geospatial information management and the implementation of the integrated geospatial information framework.\n\n1. Global geodetic reference frame* \n2. Addresses\n3. Buildings and settlements\n4. Elevation and depth\n5. Functional areas\n6. Geographic names\n7. Geology and soils\n8. Land cover and land use\n9. Land parcels\n10. Orthoimagery\n11. Physical infrastructure\n12. Population distribution (including population characteristics)\n13. Transport networks\n14. Water\n15. Economy*\n\n(*) Additional theme not included in the 14 GFGDT\n\n\n", "links": ["U3elusdN", "RHCUOmoR", "QJsx3KmY", "CvXRU020"], "quotes": [], "sources": ["ewmJWk6r"], "n_links": 4}, {"id": "RHCUOmoR", "name": "Use of EO in poverty mapping", "labels": [], "text_md": "Poverty mapping projects typically use geospatial datasets derived from high-resolution EO imagery, including:\n\n- Land cover\n- Land use\n- Objects (cars, buildings, ...)\n- Night-time lights\n- Road networks\n- ...\n\n", "links": ["Cr014GcV", "nqqQlCkb"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 2}, {"id": "pXxz1MHR", "name": "Geo-referencing", "labels": [], "text_md": "Geo-referencing makes it relatively easy to bring together, overlay and analyze information from multiple sources based on different units of analysis. \n\n", "links": ["bs0lGShz", "tltgpLsL", "cTjLfkWN", "etUj8mX6", "qpy6M6vh"], "quotes": [], "sources": ["UCi2QXBJ"], "n_links": 5}, {"id": "bs0lGShz", "name": "Geo-referenced data", "labels": [], "text_md": "Geo-referenced data provides identifiers of geospatial location for each individual observation\n\nDefining common geographic identifiers allows different datasets to be linked and analyzed together. \n\n", "links": ["pXxz1MHR", "qpy6M6vh"], "quotes": [], "sources": ["XN9jt6q8"], "n_links": 2}, {"id": "1cHc8MiO", "name": "Reproducibility of processes", "labels": [], "text_md": "The ability to reproduce processes is crucial to generate trust in their outputs.\n\nReproducibility requires immutable data and versioned logic (adoption of functional programming).\n\nReproducibility allows to schedule repeatable processes to auto-execute on specific intervals\n\n", "links": ["lEHhU8Ma", "CvXRU020", "NPfQeP5J", "liKE24lR", "v0j3xMCw", "vLPsC8l0", "9h1AmchL"], "quotes": [], "sources": ["fj461XsQ"], "n_links": 7}, {"id": "vLPsC8l0", "name": "Pure tasks", "labels": [], "text_md": "- **Pure tasks** produce the same result every time they are run.\n- **Overwrite approach**: \"Re-executing a pure task with the same input parameters should overwrite any previous output that could have been left out from a previous run of the same task.\"\n- Tasks can become \"purified\" by breaking them down into smaller tasks, each of which targets a single output. \n\n", "links": ["1cHc8MiO", "JExDAHlM"], "quotes": [], "sources": ["fj461XsQ"], "n_links": 2}, {"id": "7V81uNHT", "name": "Declarative programming", "labels": [], "text_md": "Style of programming that prioritizes *what* should happen over *how* it should happen. \n\nCode is easier to reason about because the code itself describes what is happening. \n\n", "links": ["v0j3xMCw"], "quotes": [], "sources": [], "n_links": 1}, {"id": "v0j3xMCw", "name": "Functional programming", "labels": [], "text_md": "A programming paradigm \"that treats computation as the evaluation of mathematical functions\", where the output of each computation depends only on the arguments passed to it (thus isolating logic changes from data/state changes)\n\nThe goal is to break the application logic into smaller functions, where each function is focused on a single task. Functions are then 'composed' into larger functions.\n\nAdvantages of functional programming:\n\n- Improve modularity: Individual functions can be written and tested in isolation \\(without having to understand external context\\)\n- Enhance reproducibility of results\n- Making code easier to understand\n- Making outputs easier to predict\n\nFunctional programming is part of declarative programming.\n\nSee also (https://en.wikipedia.org/wiki/Functional_programming)\n\n", "links": ["YOp0ve5T", "WtKaAhds", "ISfvwbKG", "3BmAWMzw", "7V81uNHT", "PimIKjSr", "1cHc8MiO", "Qbp1VWE4", "V9r8N6je", "W0uZ0vW4", "b8UigmQs"], "quotes": [], "sources": ["fj461XsQ"], "n_links": 11}, {"id": "Jwlw5emQ", "name": "A persistent and immutable staging area", "labels": [], "text_md": "By accumulating and persisting all source data in a stating area (keeping it forever unchanged) one can have shorter retention policy on derived tables, \"knowing that it\u2019s possible to backfill historical data at will.\"\n\n", "links": ["FOPuLnHI"], "quotes": [], "sources": ["fj461XsQ"], "n_links": 1}, {"id": "YOp0ve5T", "name": "Changing data warehouse logic over time", "labels": [], "text_md": "Changes in data warehouse logic over time should be either \n- expressed with data (in the form of \"parameter tables\") using effective dates\n- captured in source control, so they can applied conditionally, allowing to build the full state of the data warehouse throughout all time periods, or\n\nBeauchemin (2018) illustrates this with the example of introducing a change in the way taxes are calculated in year t.  If a users \"back-fills\" data for year t-1, the change in tax calculation method should not be applied. \n\n", "links": ["Aslc8kRG", "3BmAWMzw", "68VjKPZb", "v0j3xMCw"], "quotes": [], "sources": ["fj461XsQ"], "n_links": 4}, {"id": "3BmAWMzw", "name": "Changing data warehouse dimensions over time", "labels": [], "text_md": "In order to model changing dimensions in functional data warehouses without mutating data, one could use a collection of \"dimension snapshots\", whit each snapshot containing the full dimensions available at a specific point of time.\n\n \n\n\n", "links": ["YOp0ve5T", "68VjKPZb", "v0j3xMCw", "6OomPFuc", "Aslc8kRG"], "quotes": [], "sources": ["fj461XsQ"], "n_links": 5}, {"id": "2GTwGKsm", "name": "ETL Frameworks and tools", "labels": [], "text_md": "Developing and implementing ETL (extract-tranform-load) processes is \"time-consuming, brittle, and often unrewarding\" (Beauchemin, 2018)\n\nThe data flows of real-life statistical production processes can be very complex, and ETL frameworks and tools help address common problems in building ETL pipelines:\n\n- Documentation - Succinctly describe the data flow\n- Automation/Scheduling\n- Monitoring - Track the progress of long running processes and alert when errors arise\n- Backfilling - Ability to re-process historical data\n\nETL Frameworks are often implemented in Python (e.g., Airflow and Luigi).  Examples include:\n\n  - **Google's Cloud Composer**: Python-based workflow orchestration service to connect data, processing, and services between on-premises and the public cloud. Built on Apache Airflow. Pipelines are configured as directed acyclic graphs (DAGs). Includes library of connectors and multiple graphical representations of workflows.  Allows to set up a continuous integration/continuous deployment (CI/CD) pipeline for processing data on Google Cloud.\n\n  - **Google's Cloud Dataflow**: Unified, serverless stream and batch data processing service.  Allows for flexible resource scheduling (FlexRS).\n\nThere are plenty of \"drag and drop\" data pipeline and workflow automation tools that do not require knowledge of the underlying code.:\n\n- **Informatica** - Offers a portfolio of data integration products as well as tools for master data management, data quality, data cataloging, and API management. \n\n- **SQL Server Integration Services (SSIS)** - Microsoft tool for data integration tied to SQL Server.\n\n- **Stitch** - cloud-based ETL platform \n\n- **Google's Cloud Data Fusion**: Cloud-native, scalable data integration service with a visual point-and-click interface, enabling code-free deployment of ETL/ELT data pipelines.  Library of 150+ preconfigured connectors and transformations, plus ability to create custom connections and transformations that can be validated, shared, and reused across teams.\n\n", "links": ["WnQYbC1N", "lEHhU8Ma", "MhXSqPKk", "2fx14QVS"], "quotes": [], "sources": ["fj461XsQ,", "4z5iFWrh"], "n_links": 4}, {"id": "kvpUkOPt", "name": "Graph", "labels": [], "text_md": "In graph theory, a graph is a set of discrete objects (nodes) possibly joined to each other in binary or reflexive relationships (edges).   It is a relational form of organizing and representing discrete data, where nodes and edges can be decorated with additional properties known as \"attributes\".\n\nA binary relationship represents a relationship between two nodes.  A reflexive relationship represents a relationship of a node with itself.\n\nEdges can be thought of as representing pairs of connected nodes (dyads). \n\nGraphs can be used to describe networks (systems of interconnected objects).  \n\n", "links": ["wJHcEcFj", "t4mv3eQD", "2fx14QVS"], "quotes": [], "sources": [], "n_links": 3}, {"id": "t4mv3eQD", "name": "Network analysis: Undirected graphs", "labels": [], "text_md": "Edges can be traversed in either direction (an edge from A to B is the same as an edge from B to A).\n\nCan have self-loops (reflexive relationship)\n\n\n", "links": ["wJHcEcFj", "kvpUkOPt", "2fx14QVS", "MV7Xi5dH"], "quotes": [], "sources": [], "n_links": 4}, {"id": "2fx14QVS", "name": "Representing ETL jobs as Directed Acyclic Graphs (DAG)", "labels": [], "text_md": "It is often useful to visualize complex ETL data flows using a directed acyclic graph, where each node corresponds to a task (which only needs to be performed once), and arrows represent dependencies between tasks. \n\n", "links": ["lEHhU8Ma", "WnQYbC1N", "zNXU0RHP", "t4mv3eQD", "2GTwGKsm", "kvpUkOPt"], "quotes": [], "sources": ["oIarYHfE"], "n_links": 6}, {"id": "UejoKjiC", "name": "User engagement in data innovation projects", "labels": [], "text_md": "It is important to build strong ties of collaboration between technical team and the user community. Representatives from key user groups need to be invited to be part of all project activities, from the planning stage on.  \n\n", "links": ["2j4pK36q", "9h1AmchL"], "quotes": [], "sources": [], "n_links": 2}, {"id": "bay5lBgW", "name": "Need for crisis management skills", "labels": [], "text_md": "The current situation demands strong crisis management skills and expertise, and agility in planning, designing and implementing innovative approaches to carry out critical statistical operations. \n\n\n", "links": ["4kDQlxl8", "lCHnOEAE", "SRFvQdH5", "84OVqhJD", "YJqwphec", "gxrbaZc1", "5dUHywzA", "xUr89DOT", "x97FBvWU"], "quotes": [], "sources": ["vh5QhUfR"], "n_links": 9}, {"id": "ofQx7aBs", "name": "Responsibilities of the social media manager", "labels": [], "text_md": "- Plan the social media calendar\n- Write and schedule social media posts\n- Determine the strategy for responding to and interacting with social media users\n\n", "links": ["1hLRzYSe", "i54AuJAT", "AP2e8YNz"], "quotes": [], "sources": [], "n_links": 3}, {"id": "wiYBU1Oc", "name": "Responsibilities of the project coordinator in a cross-functional team", "labels": [], "text_md": "- Create and track against a project plan\n- Drive team members towards the completion of milestones and project deliverables\n- Budget project resources and expenditures\n- Manage risks\n- Find ways to prevent time-sharing overload of team members between the project and their regular functional duties.\n\n", "links": ["94L4EBHO", "mmwn1oEU", "J4RjLkh4"], "quotes": [], "sources": [], "n_links": 3}, {"id": "2Daj1ZPE", "name": "Data science curriculum for official statisticians", "labels": [], "text_md": "- **Programming and coding**\n    - Python and/or R\n    - SQL\n- **GIS**\n    - Feature extraction from EO imagery\n    - Map visualizatoins\n- **Statistical / econometric estimation methods**\n    - Population-density estimation methods\n    - Small-area estimation methods\n    - Crop-yield estimation methods\n    - Household consumption estimation methods\n    - Poverty maps\n\n", "links": ["Vmp2TR2i", "53FNlsMn", "JiEdFdtg", "QJsx3KmY"], "quotes": [], "sources": [], "n_links": 4}, {"id": "qvRxZspp", "name": "When there is too much to do", "labels": [], "text_md": "\"when we have too much to do we can freeze. Spinning without traction, we move fast but don't make progress on the things that are creating our stress.  Because when there is so much competing for attention, we don't know where to begin and so we don't begin anywhere\".\n\n", "links": ["QyoZwcP9"], "quotes": [], "sources": ["qXafgygU"], "n_links": 1}]