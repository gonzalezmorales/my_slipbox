[
    {
        "level": 1,
        "id": "oWg5LrNe",
        "title": "Responsibilities of the content analyst",
        "text": "The content analyst digs into the use analytics to measure content performance and create reports that help determine what works and what doesn't, in order to fine tune:\n- Content creation strategy (what type content should we create?)\n- Content promotion strategy (what content should we promote? What content promotion activities are most effective?)\n\n"
    },
    {
        "level": 2,
        "id": "i54AuJAT",
        "title": "Content promotion strategy",
        "text": "Once new content is published, there is need to promote content through social media and other channels.\n\nThe effectiveness of the content promotion strategy should be monitored through content analytics.  \n\n"
    },
    {
        "level": 3,
        "id": "xFkKBQHT",
        "title": "Guest blogging",
        "text": "Guest blogs where third parties promote their own content allow to strengthen partnerships while attracting visitors to our website and making it visible to broader audiences.\n\n"
    },
    {
        "level": 3,
        "id": "ofQx7aBs",
        "title": "Responsibilities of the social media manager",
        "text": "- Plan the social media calendar\n- Write and schedule social media posts\n- Determine the strategy for responding to and interacting with social media users\n\n"
    },
    {
        "level": 3,
        "id": "jHpeljkZ",
        "title": "Content strategy goals",
        "text": "- Increase *website traffic*\n- Make brand channels a destination for *organic traffic*\n- Increase *brand awareness*\n- Establish *brand authority*\n- Bring **value to users**\n  - Help users identify and understand the main challenges faced by the national and global statistical systems that result from the COVID-19 pandemic\n  - Help global and national statistical systems overcome these challenges\n\n\n"
    },
    {
        "level": 3,
        "id": "bHaKr9ga",
        "title": "Types of content",
        "text": "There are many types of content:\n- Blogs\n- Emails\n- Videos\n- Infographics\n- Newsletters\n- Press releases\n- ...\n\n"
    },
    {
        "level": 3,
        "id": "1hLRzYSe",
        "title": "Content Production Team",
        "text": "A content production team is often an example of a cross-functional team, with members coming from different functional areas of the organization that  may include:\n- Content strategist\n- Content manager\n- Content writer(s)\n- Content editor(s)\n- Content analyst\n\n"
    },
    {
        "level": 3,
        "id": "4INbWGw6",
        "title": "COVID-19 response website types of users",
        "text": "Our content strategy needs to cater to multiple types of audiences. We need to know who are our target audiences.\n\n- Chief statisticians from national and international statistical organizations\n- Data experts/practitioners from national and international statistical organizations\n- Data experts/practitioners from UN Country Teams\n- Data experts/practitioners from civil society, academia and the private sector\n- Students at different education levels (high school/undergraduate/graduate)\n- Analysts / policy experts from national governments and international organizations\n- Donors providing funding for statistical capacity building\n\n"
    },
    {
        "level": 2,
        "id": "jHpeljkZ",
        "title": "Content strategy goals",
        "text": "- Increase *website traffic*\n- Make brand channels a destination for *organic traffic*\n- Increase *brand awareness*\n- Establish *brand authority*\n- Bring **value to users**\n  - Help users identify and understand the main challenges faced by the national and global statistical systems that result from the COVID-19 pandemic\n  - Help global and national statistical systems overcome these challenges\n\n\n"
    },
    {
        "level": 3,
        "id": "S1rUnyfS",
        "title": "Responsibilities of the content editor",
        "text": "The role of the content editor is to make a writer's good work great, offering guidance and helping authors think differently about their topic and ensuring that only high-quality content gets published.\n\nThe content editor's responsibilities include: \n\n- Review all written materials to ensure that they:\n  - are free of errors\n  - comply with the style guide\n  - reflect the brand's voice \n  - meet the goals set in the content strategy\n- Suggest revisions\n- Approve publication\n\n\n"
    },
    {
        "level": 3,
        "id": "AP2e8YNz",
        "title": "Elements of a content strategy",
        "text": "A content strategy includes:\n- A list of **clear goals**\n- A method for measuring success\n- A competitor analysis\n- Persona development\n- A brand style guide\n- A list of types of content to be produced\n- An initial editorial calendar\n\n\n"
    },
    {
        "level": 3,
        "id": "h1lCewmo",
        "title": "COVID-19 response website - analysis of users' goals",
        "text": "Once we know who our target audiences are, we need to know what are their own goals, and how we will be helping them attain those goals.  \n\nOur main constituency are National Statistical Offices and statistical offices of international organizations, whose main goals are to **(1) keep existing statistical programmes running**, and **(2) respond to new data requirements ** around the COVID-19 crisis from national and local governments, other institutional decision makers, and the public at large.\n\nThese goals translate into specific needs:\n\n- **Mobilize resources** to support regular statistical activities affected by the pandemic and to launch and run new statistical activities to satisfy new data demands\n- **Obtain access to methodological guidance** on how the use of new data sources, methods and technologies for the collection, processing, analysis, dissemination and communication of data and statistics\n  - Create opportunities for peer-to-peer **sharing of experiences** \n  - Provide access to relevant **training materials**\n  - Deliver **expert advice**\n\n- **Identify authoritative, reliable sources of data** to monitor the day-to-day evolution of the health crisis, to assess and monitor social, economic and environmental impact of the pandemic, and to inform recovery policies over the longer term\n- Identify **capacity building needs**\n- **Coordinate initiatives** that respond to the needs of national statistical systems\n  - Understand who is doing what across the global and national statistical systems\n\n\n"
    },
    {
        "level": 2,
        "id": "bHaKr9ga",
        "title": "Types of content",
        "text": "There are many types of content:\n- Blogs\n- Emails\n- Videos\n- Infographics\n- Newsletters\n- Press releases\n- ...\n\n"
    },
    {
        "level": 3,
        "id": "G8wbe503",
        "title": "Content generation using smartphones",
        "text": "\"Smartphone-generated content may offer more ... accurate insights into consumer preferences.\"\n\n\"Tweets composed on smartphones [contain] higher proportions of first-person pronouns, references to family, ... negative emotional words [and display] a less-analytical writing style.\"\n\n\"People associate their smartphones with psychological comfort\"\n\n\n\n"
    },
    {
        "level": 3,
        "id": "i7m8d4YQ",
        "title": "Information architecture for the COVID-19 response platform",
        "text": "Guidance and resources provided through UNSD's COVID-19 response platforms could be organized according to the various phases of the statistical business process:\n- Data Collection\n- Data Validation\n- Data Processing \n- Data Analysis\n- Data Dissemination\n\nThey could also be organized according to the classification of statistical activities.\n\n\n"
    },
    {
        "level": 2,
        "id": "1hLRzYSe",
        "title": "Content Production Team",
        "text": "A content production team is often an example of a cross-functional team, with members coming from different functional areas of the organization that  may include:\n- Content strategist\n- Content manager\n- Content writer(s)\n- Content editor(s)\n- Content analyst\n\n"
    },
    {
        "level": 3,
        "id": "PjMcjUgb",
        "title": "Team charter template",
        "text": "### Purpose\n\n- Why does our team exist?\n- What are we collectively working towards?\n- What are our personal goals?\n- What do we want to do in the next 15,30, 90 days?\n\n### Measures\n\n- What is the ultimate measure of success for the team?\n- How do we track progress towards our goals?\n\n### Roles\n\n- What are the roles we have in our team?\n- What are they responsible for?\n- Who will fill those roles?\n\n### Practices\n\n- How do we want to work together?\n- How do we communicate and meet?\n- What tools do we use?\n- How often do we revisit goals, retrospect, strategize?\n\n### Guardrails\n\n- What is safe to try?\n- How do we make decisions?\n- What rules do we want to put in place?\n- What other commitments do we have outside the team?\n\n"
    },
    {
        "level": 3,
        "id": "MMjEeFha",
        "title": "Skills required in a content manager",
        "text": "- Very good understanding of the needs of users\n- SEO and keyword research: Ability to identify topics and opportunities for improving organic traffic\n- Strong understanding of the organization's communication goals: Ability to generate meaningful topics and organize them in the editorial calendar\n\n"
    },
    {
        "level": 3,
        "id": "Lb8FlQSj",
        "title": "Responsibilities of the content manager",
        "text": "Responsibilities of the content manager include:\n\n- Making day-to-day decisions about what should be pulbished, how and when\n- Explaining to content creators and project owners why a piece of content should be created\n- Finding topics that will perform well in various channels (blog articles, Twitter, email...)\n- Staying on top of the editorial calendar - overseeing that content getes finished and published on time\n- Knowing the workload of each content creator\n\n\n"
    },
    {
        "level": 3,
        "id": "J4RjLkh4",
        "title": "Ad-hoc cross functional teams",
        "text": "Ad-hoc cross functional teams seek to integrate skills across organizational boundaries in order to accomplish a specific goal over a limited period of time. They are often called during a critical juncture to deal with a specific problem or explore a new opportunity.\n\nA critical success factor is having clearly defined, realistic outputs, timeline, and exit criteria for each phase of the project.  \n\nAd-hoc  cross functional teams are challenged by complex supervisory relationships and incentive structures, as their members formally report to different functional managers. This increases the need for **horizontal cooperation and coordination** among team members.\n\nTo  avoid turf battles and organizational power politics, the different members of an ad-hoc cross-functional team must be completely **focused on creating value to customers**.  Team members must therefore have adequate **incentives to collaborate** outside their own \"home teams\" in the organization.  This means that their performance needs to be measured and rewarded based on their contributions towards achieving the objectives of the cross-functional project.\n\nPutting together the **right mix of talents** is another key ingredient for the success of a cross-functional team.  Reluctance by managers from different functional areas to get personally involved and to commit their most qualified staff to a cross functional project team often leads to poor results. On the other hand, assigning key staff members to a cross-functional project is a clear signal of the commitment by senior management to the success of the project. \n\nAd-hoc cross functional teams must also have a clearly defined **\"sunsetting\" plan** for incorporating back people and the team outputs, innovations and learnings back into the organization's functional processes and structure when the time comes to dissolve the team.  In particular, it is important to identify opportunities for team members to take on new responsibilities based on the skills they developed while being part of the ad-hoc cross functional team.\n\n"
    },
    {
        "level": 2,
        "id": "AP2e8YNz",
        "title": "Elements of a content strategy",
        "text": "A content strategy includes:\n- A list of **clear goals**\n- A method for measuring success\n- A competitor analysis\n- Persona development\n- A brand style guide\n- A list of types of content to be produced\n- An initial editorial calendar\n\n\n"
    },
    {
        "level": 3,
        "id": "tZWRLOUI",
        "title": "Elements of a results communication strategy",
        "text": "- Identify potential **users** and how the results of an innovation project help them achieve their goals.\n- Determine the **types of information products / formats / media** that are best suited to the needs of these users\n- Identify / develop **dissemination outlets** that can effectively bring the product of the project to the intended users.\n- Develop a clear policy specifying the terms under which users can access and utilize of both the results of the project and the underlying data inputs\n- Develop complete, concise and clear user documentation explaining the methodology and providing guidance on the appropriate interpretation of the results\n- Make all results available in all the languages in which the key user groups work (e.g., all languages in which the government works)\n\n"
    },
    {
        "level": 1,
        "id": "9PrGSBNy",
        "title": "Choosing cloud vendors",
        "text": "Choosing cloud service providers is a key strategic decision in setting up the architecture of a data science project.  This decision should consider a number of factors, including:\n\n- The types of data (structured and non-structured) and the volume of those data types that will need to be stored, accessed and analyzed\n- Whether data will be accessed and processed in batches or in real time\n- Whether storage and analytic capabilities are seamlessly integrated in the same cloud platform (e.g., the ability to deploy ML models directly on a cloud-based data warehouse). \n- Whether the cloud providers support of open-source technologies\n\n## Major cloud vendors include:\n\n- Google Cloud\n  - Storage:\n  - Machine learning: [BigQuery ML](https://cloud.google.com/bigquery-ml/docs/bigqueryml-intro)\n  - ...\n- Amazon Web Services\n  - Storage:\n  - Machine learning: [SageMaker](https://aws.amazon.com/sagemaker/)\n- Microsoft Azure\n  - Storage:\n  - Machine learning: \n- ArcGIS online\n  - Storage:\n  - Machine learning: \n\n"
    },
    {
        "level": 2,
        "id": "eeJS3CIE",
        "title": "Serverless data pipelines",
        "text": "Instead of setting up a data lake and a data warehouse themselves, some organizations are choosing to use managed cloud services from outside vendors for data storage and querying. \n\n"
    },
    {
        "level": 3,
        "id": "vKQFuLMb",
        "title": "The cloud",
        "text": "\"The cloud\" is the networks, services, systems and databases available to clients on the Internet to share, store, access and manipulate data.\n\nEnterprise systems are increasingly switching their focus from on-premise to cloud-based data repositories and processing capabilities. \n\n"
    },
    {
        "level": 3,
        "id": "dJBcDmmd",
        "title": "Cloud service models",
        "text": "Different cloud service models can be categorized into public/private/hybrid cloud and single-provider/multi-provider cloud. \n\n*Public cloud*\n\n- All hardware, software and supporting infrastructure is owned and managed by the cloude service provider\n- Cloud infrastructure is shared with other customers and accessed over the internet.\n\n*Private cloud*\n\n- Services and infrastructure are dedicated exclusively to one organization and maintained in a private network. \n- Can be on-premises or hosted by a third-party service provider\n- Often used by organizations with special security needs\n\n*Hybrid cloud*\n\n- Combines public and private clouds\n\nIn a multi-provider cloud infrastructure, different vendors provide specific cloud services. This approach is often used to avoid vendor lock-in.\n\n"
    },
    {
        "level": 3,
        "id": "ulLdtnHy",
        "title": "Cloud computing",
        "text": "Cloud computing refers to the delivery of hosted IT services over the internet, usually on a pay-as-you-go basis.  It includes:\n\n- Software as a service (SaaS) \n- Infrastructure as a service (IaaS)\n- Platform as a service (PaaS)\n\nCloud service providers are responsible for all their hardware and software maintenance, and are backed by a large network of servers and staff to ensure the reliability of their services. \n\nDue to its reliance on hardware-independent virtualization technology, cloud computing enables organizations to quickly back up data, applications, and even operating systems to a remote data center, and to deploy them to multiple users distributed in many different locations.\n\nCloud services usually involve lower upfront costs and shorter time commitments, lowering the barriers of entry for organizations that seek to modernize their IT infrastructure, explore the adoption of new tools, or scale up their ability to handle larger volumes of data. Thus, managed cloud services can provide a good solution for new data innovation projects, which start small but need to be able to rapidly grow. \n\n"
    },
    {
        "level": 2,
        "id": "dJBcDmmd",
        "title": "Cloud service models",
        "text": "Different cloud service models can be categorized into public/private/hybrid cloud and single-provider/multi-provider cloud. \n\n*Public cloud*\n\n- All hardware, software and supporting infrastructure is owned and managed by the cloude service provider\n- Cloud infrastructure is shared with other customers and accessed over the internet.\n\n*Private cloud*\n\n- Services and infrastructure are dedicated exclusively to one organization and maintained in a private network. \n- Can be on-premises or hosted by a third-party service provider\n- Often used by organizations with special security needs\n\n*Hybrid cloud*\n\n- Combines public and private clouds\n\nIn a multi-provider cloud infrastructure, different vendors provide specific cloud services. This approach is often used to avoid vendor lock-in.\n\n"
    },
    {
        "level": 3,
        "id": "Vmp2TR2i",
        "title": "Data engineering curriculum for official statisticians",
        "text": "In the past, teams working on data innovation projects used to be able to get away with just knowing the basics of data storage and ETL infrastructure. Today, however, data innovation teams need to have a good understanding of how different types of databases can be set up, accessed and integrated, and how to setup and orchestrate the infrastructure and data management environments needed to develop, test and deploy different analytic methods. \n\nData innovation training curriculum for official statisticians should develop data engineering skills on the following topics:\n\n- **Data management and integration** \n    - Basic data modelling\n    - Transcoding and record-linking methods\n    - Statistical disclosure control methods\n    - Data warehousing / ETL pipeline design patterns and techniques\n- **Cloud computing**\n  - Understanding distributed computing\n  - How to evaluate and implement different cloud service models\n\n\n   \n\n"
    },
    {
        "level": 2,
        "id": "Vmp2TR2i",
        "title": "Data engineering curriculum for official statisticians",
        "text": "In the past, teams working on data innovation projects used to be able to get away with just knowing the basics of data storage and ETL infrastructure. Today, however, data innovation teams need to have a good understanding of how different types of databases can be set up, accessed and integrated, and how to setup and orchestrate the infrastructure and data management environments needed to develop, test and deploy different analytic methods. \n\nData innovation training curriculum for official statisticians should develop data engineering skills on the following topics:\n\n- **Data management and integration** \n    - Basic data modelling\n    - Transcoding and record-linking methods\n    - Statistical disclosure control methods\n    - Data warehousing / ETL pipeline design patterns and techniques\n- **Cloud computing**\n  - Understanding distributed computing\n  - How to evaluate and implement different cloud service models\n\n\n   \n\n"
    },
    {
        "level": 3,
        "id": "P43SdKCs",
        "title": "Training materials for national staff",
        "text": "To enable national staff to reproduce the outputs of data innovation projects and to utilize innovative data sources, technologies and methods on a sustainable basis, it is crucial to develop training materials and knowledge resources tailored to their own needs and context.\n\n- In their own language\n- Applicable in their existing technological infrastructure\n\n"
    },
    {
        "level": 3,
        "id": "Iaba3RcP",
        "title": "Key role of data warehouses",
        "text": "A data warehouse is a place where data from different sources is stored in query-able forms, ready to be used by different users and applications to enable higher level analytics.  \n\nData warehouses make data science activities scalable. \n\n"
    },
    {
        "level": 3,
        "id": "2Daj1ZPE",
        "title": "Data science curriculum for official statisticians",
        "text": "- **Programming and coding**\n    - Python and/or R\n    - SQL\n- **GIS**\n    - Feature extraction from EO imagery\n    - Map visualizatoins\n- **Statistical / econometric estimation methods**\n    - Population-density estimation methods\n    - Small-area estimation methods\n    - Crop-yield estimation methods\n    - Household consumption estimation methods\n    - Poverty maps\n\n"
    },
    {
        "level": 3,
        "id": "hp8Foh4V",
        "title": "Capacity bottlenecks in data innovation projects",
        "text": "Common capacity bottlenecks in data innovation projects include:\n \n- Shortage of **facilities or equipment**\n- Gaps in technical **skills**\n- Lack of **staff time**\n- **Data quality** issues\n- Shortage of modern **software** tools\n\n\n"
    },
    {
        "level": 3,
        "id": "1P7GAwhy",
        "title": "Sustainable access to source data",
        "text": "To be sustainable, producers of statistics need to have the capacity to regularly access to the necessary input data from external sources.\n\nThis requires:\n- Effective legal data exchange arrangements \n- Adequate incentives and business models\n- Appropriate technical data exchange standards and protocols\n- Good data exchange infrastructure, including connectivity and bandwidth.\n\n"
    },
    {
        "level": 3,
        "id": "jU3e4dIS",
        "title": "Role of data engineer",
        "text": "Data engineers needs a solid technical understanding of data integration and architecture modelling in order to:\n\n1. Develop **ETL pipelines** that are easy to understand, maintain, and replicate by others.  \n2. Implement **data storage and processing environments** that allow data scientists and other analysts to collaborate in the development and testing of estimation models. \n\n\n\n"
    },
    {
        "level": 3,
        "id": "WnQYbC1N",
        "title": "Stream vs batch ETL pipelines",
        "text": "There are 2 main types of data pipelines:\n\n1. **Stream**: Transactional data is passed along almost as soon as the transaction occurs. As soon as a new record is added into the source database, it\u2019s passed along into the analytical system. Creating and maintaining streaming systems is often more challenging.\n2. **Batch**: The pipeline runs at a specific time interval; data is not live, but is loaded in \"batches\".\n\n\n\n\n"
    },
    {
        "level": 3,
        "id": "JiEdFdtg",
        "title": "Engaging local universities and training institutes",
        "text": "To promote sustainability and national ownership, data innovation project teams need to partner with local universities or training institutes in order to train analysts at national statistical offices and other government agencies.\n\n"
    },
    {
        "level": 3,
        "id": "Cp7Quz8B",
        "title": "Capacity building in data engineering",
        "text": "Most capacity building in data innovation projects tends to focus on \"high-level\" skills such as artificial-intelligence, machine learning, geoprocessing, or sophisticated estimation methodologies.   \n\nHowever, training workshops in data innovation usually neglect foundational data engineering skills, such as design of table schemas and practical implementation of data pipelines. \n\nAlthough not every statistician needs to become an expert in data engineering, NSOs need to have sufficient in-house data engineering skills to address critical real-life challenges in mainstreaming their data innovation projects into their regular official statistics production processes.\n\n"
    },
    {
        "level": 3,
        "id": "MhXSqPKk",
        "title": "Managing ETL pipelines",
        "text": "A core component of any data innovation project is to set up a scalable data architecture, including a set of scalable ETL pipelines that move data from one system to another.  \n\nOne of the main roles of a data engineer is to design, build and run ETL processes (data pipelines) that send input data to data lakes, data warehouses and subscription services for use in the development/compilation of data products.\n\nKey questions that need to be investigated before deploying a data pipeline include:\n\n- Who owns the data pipeline?\n- Which teams will be consuming the data?\n- Who will monitor and maintain the the pipeline? (e.g., will the maintenance of the pipeline be under the responsibility of the data users, or will there be an infrastructure team keeping it operational?\n- What data pipeline and workflow automation tools are best suited to the purpose at hand?\n\n"
    },
    {
        "level": 3,
        "id": "QJsx3KmY",
        "title": "Data innovation road map",
        "text": "A road map of consisting of country-specific activities, intermediate outputs and expected outcomes\n\n**Pre-production:**\n\n1. Define **scope**\n1. Identify **key stakeholders**\n1. Build **suport** \n1. Reach out to **potential users**\n1. Secure **resources**\n1. Establish **project team**\n1. Procure/setup/configure the necessary **hardware and software** tools\n1. Secure **access to data** inputs\n1. Design, build and run source data integration process\n1. Assess **quality of source data**\n1. Transform original source data into **analysis-ready datasets**\n1. Set up **data inputs clearinghouse**\n1. Provide **practical training**\n\n**Production:**\n\n1. Compute intermediate indicators to be used as (geospatial) covariates in statistical estimation models (e.g., distance to service-delivery points, distance to roads, elevation, ...)\n\n\n**Post-production:**\n\n1. Communicate project outputs to key audiences \n\n"
    },
    {
        "level": 3,
        "id": "lEHhU8Ma",
        "title": "ETL - Making data analysis-ready",
        "text": "The **'extract-tranform-load' (ETL)** pattern consists of the following steps:\n1. Extracting input data from their original sources\n2. Transforming them it into usable data structures through transcoding, filtering, joining, and aggregation operations\n3. Uploading the transformed data onto a controlled data management environment (such as a data warehouse)\n\nETL pipelines are used to **transform raw data into analysis-ready data**, i.e., making data from heterogeneous sources available to data analysis at a central location, following a specific schema, so they can easily run queries using SQL, feed the data into generic statistical packages, etc.\n\n**ETL tools** allow to pull or receive data from a source system,  perform modifications through a sequence of processing steps, and push the transformed data into a target destination.  \n\n**ETL scheduling** allows to run an ETL process at specific intervals to load raw data into a target system.\n\nOperational databases and other sources of data need to be cleaned, standardized and integrated.  A measure of success for the intermediate goal of preparing a set of analysis-ready data inputs is the ability to combine them in exploratory data visualizations, including map visualizations, \n\n\n"
    },
    {
        "level": 3,
        "id": "d3ru9pGK",
        "title": "Building data engineering skills in NSOs",
        "text": "The ability of NSOs to efficiently integrate multiple data inputs into valuable statistics is increasingly dependent on their data infrastructure and \"data engineering\" skills.  \n\nDifferent types of data repositories and architectures require vastly different technical skills.  Statisticians need to develop a broad rang of foundational data engineering skills to  mainstream new data sources, methods and technologies into regular statistical production programmes.\n\nStatisticians are rarely given analysis-ready data that can be directly used in statistical estimation and compilation.  One of the main challenges in official statistics is to design, build and maintain data integration, processing, and dissemination pipelines. \n\nPractical data innovation projects in official statistics require the ability to extract, organize and manipulate raw, unstructured source data, and to transform it into \"clean\" datasets that can be processed and analyzed using by standard software programmes and methodologies.\n\nMoreover, it is important to continuously develop and retain these skills over time, as new data inputs needs to be fed into the statistical production process on a regular basis. \n\n"
    },
    {
        "level": 3,
        "id": "MfTvaMIL",
        "title": "Risks and challenges of adopting cloud applications",
        "text": "Moving systems currently hosted on-premises to cloud services is a very complex task that requires:\n\n- Vendor selection and procurement\n- Data cleaning and migration\n- Addressing user management and authentication issues\n- Data security and network connectivity through remote logins and VPNs\n\n"
    },
    {
        "level": 3,
        "id": "ilCtHTAx",
        "title": "Training for data innovation projects",
        "text": "Data innovation teams need to build a wide range of **data analysis and management** skills, including:\n- data engineering\n- data science\n- IT infrastructure engineering\n- statistical and econometric modeling\n- GIS analysis\n\nIt also requires building **\"soft\" skills**, such as project management, fund-raising, and communication.\n\n\n\n"
    },
    {
        "level": 3,
        "id": "UHxZBMU2",
        "title": "Data Modelling",
        "text": "**Data Modeling** is \"a design process where one carefully defines table schemas and data relations to capture business metrics and dimensions\". (Chang, 2018b)\n\nData modelling is about optimizing data structures for the purpose at hand. \n\n- Data modelling patterns used in business intelligence or for analytic purposes often involve sacrificing data normalization (i.e., accepting more data redundancy and more complex ETL pipelines to maintain) in order to facilitate data queries from tables where metrics and dimensions are already pre-joined.\n\n"
    },
    {
        "level": 2,
        "id": "ulLdtnHy",
        "title": "Cloud computing",
        "text": "Cloud computing refers to the delivery of hosted IT services over the internet, usually on a pay-as-you-go basis.  It includes:\n\n- Software as a service (SaaS) \n- Infrastructure as a service (IaaS)\n- Platform as a service (PaaS)\n\nCloud service providers are responsible for all their hardware and software maintenance, and are backed by a large network of servers and staff to ensure the reliability of their services. \n\nDue to its reliance on hardware-independent virtualization technology, cloud computing enables organizations to quickly back up data, applications, and even operating systems to a remote data center, and to deploy them to multiple users distributed in many different locations.\n\nCloud services usually involve lower upfront costs and shorter time commitments, lowering the barriers of entry for organizations that seek to modernize their IT infrastructure, explore the adoption of new tools, or scale up their ability to handle larger volumes of data. Thus, managed cloud services can provide a good solution for new data innovation projects, which start small but need to be able to rapidly grow. \n\n"
    },
    {
        "level": 3,
        "id": "3ONs7aFt",
        "title": "Software as a Service (SaaS)",
        "text": "Model in which software applications are hosted by a third-party and made available to users over the Internet, usually through a web browser.  Users do not have to install or configure anything, and the underlying cloud hardware is maintained by the service provider. \n\nMost SaaS providers offer flexible, on-demand pricing arrangements as well as tools for user management and data migratoin.\n\n- Examples of SaaS applications:\n  - Email, videoconferencing, and other basic communication tools\n  - File sharing and team collaboration. \n  - Human resource management\n  - Management of relationships with data providers\n  - Management or relationships with data users\n  - Specialized applications (e.g., statistical analysis software, GIS applications)\n  - E-Learning delivery\n\n\n"
    },
    {
        "level": 3,
        "id": "qEgpEYM3",
        "title": "Challenges related to the rapid implementation of telecommuting arrangements",
        "text": "- Procure, configure, and distribute computer equipment and software needed for remote collaboration among all staff members, including add-ons to enable secure remote data access and system administration capabilities (e.g., standardized end-point security software on all employee devises).  If staff has already begun telecommuting, it is necessary to work out how to distribute equipment and software to the locations where they are based.\n- Ensure voice connectivity and video-conferencing capabilities (e.g., forwarding calls to staff members' home or mobile phones; enabling soft-phone, voice-over-IP or video conference solutions). It is key to allow teams to regularly and easily talk to, and visually interact with, each other  \n- Ensure remote users have the necessary bandwidth. This may entail finding a way to upgrade the user's phone and/or internet access for a period of time.\n- Make sure there are processes in place to cover most common IT helpdesk support requests form staff members working remotely\n- Enable cloud-based backup services in all remote devices.  \n- Design, implement and implement new workflows\n\n"
    },
    {
        "level": 2,
        "id": "vKQFuLMb",
        "title": "The cloud",
        "text": "\"The cloud\" is the networks, services, systems and databases available to clients on the Internet to share, store, access and manipulate data.\n\nEnterprise systems are increasingly switching their focus from on-premise to cloud-based data repositories and processing capabilities. \n\n"
    },
    {
        "level": 3,
        "id": "yYla7ZKp",
        "title": "Big Data",
        "text": "Big data consists of information assets characterized by:\n\n- High volume\n- High velocity\n- High variety\n\nProjects that involve a massive amounts of data that cannot be handled by local machines require to host applications and databases on the cloud. \n\n"
    },
    {
        "level": 1,
        "id": "NPfQeP5J",
        "title": "Data integration",
        "text": "Data integration is the act of making data from disparate source systems align and conform to common standards, combining and presenting them in a unified view to users, so they are readily available for further value creation. Getting the raw data from various sources ready for modelling and analysis is usually 80 percent of the work.\n\nA key deliverable from a data integration effort is a set of source-to-target mappings (STTM).\n\nData integration is a critical task which should be tackled early in a data analysis project, way before development work begins.\n\nData integration relies on different patterns, including data replication, data virtualization, and ETL, which may or may not involve the physical transfer of data between storage systems. \n\nSuccessful data integration requires a thorough analysis of the overall system architecture, data stores, and business requirements. \n\nGood integration design is repeatable.  It also must include mechanisms for handling exceptions and monitoring the results of data integration tasks (e.g., providing users with regular success and error counts)\n\n"
    },
    {
        "level": 2,
        "id": "QSfNE2W5",
        "title": "Data integration team",
        "text": "Data integration tasks are a key component of every data innovation project.  One person cannot be responsible for an entire integration effort, including requirements, design, and deployment.  \n\nA data integration team should include the following **roles**:\n\n- Project sponsor: Provides direction and resources\n- Stakeholder: Individual or group with a vested interest in the success of the project\n- Subject matter expert: Individuals knowledgeable of the fundamental technical and/or business details of the day-to-day operations and requirements (their time is limited but precious)\n- Product owner: Main point of contact for all decisions related to the project\n- Project manager: Responsible for overseeing buget, schedule, deliverables and handover to customer\n- Data integration architect: Plans, designs and oversees the construction of data pipelines and tools that enable the movement of data from one point to another.  Works closely with ETL developers as their technical lead.\n- ETL developers: Team responsible of building ETL solutions that satisfy the technical requirements set by the data integration architect and ensure they are free of bugs, errors and defects.\n\nSince the data integration task spans across systems with multiple purposes, it is imperative to bring in experts from different domain areas, who understand day-to-day processes, are closely familiar with the design, purpose and unique processes of source systems, and know where \"data landmines\" are.\n\nThe data integration team needs to include people with technical knowledge of different database management systems, cloud platforms, data formats and serializations (e.g., XML, JSON), and communication protocols (e.g. REST). \n\nIn addition, a successful data integration team requires strong communication and the ability to discuss, negotiate and compromise. \n\nEngaging a data integration team to define a common data architecture in the early phases of a data innovation project will help prevent delays due to interoperability issues later on. \n\n\n"
    },
    {
        "level": 3,
        "id": "uKiS7iJH",
        "title": "Role of data integration architect",
        "text": "The integration architect works closely with ETL developers and stakeholder teams gathering and clarifying business and functional requirements, and conducts research to understand business rules and technical specifications of source and target systems. \n\nThe data integration architect needs to have good understanding of how all the parts of the NSS ecosystem work together, where and how can all relevant data sources be accessed by statisticians and data scientists, and what are the data storage and processing environments they need in order to do their job.  She or he also should have a clear idea of how will the outputs produced by statisticians and data scientists be fed into regular data dissemination channels. \n\nThe data integration architect will also help define the nature, frequency and schedule of data integration tasks (i.e., one-time data migration; nightly/weekly/monthly integration; on-demand batch integration, real-time/stream integration...)\n\nShe or he plays a key role in facilitating communication and common understanding among strategic and operational members of the broader data integration team.  After the discovery and testing phase, hand off work to developers. \n\n\n"
    },
    {
        "level": 3,
        "id": "sKuMe1ca",
        "title": "Standardizing ETL tools and solutions",
        "text": "The adoption of different, often proprietary ETL tools for data integration in different departments of the same organization often leads to an increase in data silos.\n\nWhen \"one department of an organization, perhaps focusing on its own need, [adopts] a completely different ETL tool than another department[, the result is] an *increase* in data silos, rather than a decrease.\"\n\nOrganizations often work with multiple ETL solutions, each with its own framework, ... \"relegating the entire architecture into a big mess\"\n\n"
    },
    {
        "level": 3,
        "id": "Tcm6TD6G",
        "title": "Quality and the integration and standardization of statistical production systems",
        "text": "\n**Requirement 11.6** of the \"National Quality Assurance Frameworks Manual for Official Statistics\" calls statistical agencies to define, promote and implement integrated and standardized production systems, including through  the promotion, sharing and implementation of standardized solutions that increase effectiveness and efficacy.  It also calls for them to adopt a statistical business architecture based on international standards and tools, such as GSBPM, GAMSO, CSPA, and SDMX. \n\n\n"
    },
    {
        "level": 2,
        "id": "68VjKPZb",
        "title": "Architectural design for a data innovation project",
        "text": "At the beginning of a data innovation project, it is necessary design the architecture of the data pipelines and data storage and management environments that will allow the intake of raw data from multiple sources and push them through various processing steps, culminating with the delivery of analytic outputs to the final users.  \n\nThis architectural design involves making decisions regarding technologies, platforms, and data management and analysis tools. This includes selecting the appropriate mix of on-premises and cloud infrastructure (depending on the type of data inputs and the different analysis and estimation methods that will be used in the project) as well as planning and design to incorporate the right type of data integration patterns (e.g., ETL, data virtualization...), storage strategies, and infrastructure/performance optimizations (e.g., streaming, in-memory, hybrid storage).\n\nThe engineering of data pipelines will need to cover both \"development\" and \"production\" environments, as well as a strategy for testing development operations before pushing them into production. \n\nMoreover, the data architecture should provide for data storage and data processing environments and tools that allow the members of cross-functional teams to collaborate in the development, testing, implementation and refinement of various data estimation methods.\n\nSome key requirements for the data architecture include:\n- Easy to maintain by the organization's own staff\n- Easy to replicate by people outside the project team\n\n\n"
    },
    {
        "level": 3,
        "id": "3BmAWMzw",
        "title": "Changing data warehouse dimensions over time",
        "text": "In order to model changing dimensions in functional data warehouses without mutating data, one could use a collection of \"dimension snapshots\", whit each snapshot containing the full dimensions available at a specific point of time.\n\n \n\n\n"
    },
    {
        "level": 3,
        "id": "9QXT3pVK",
        "title": "Reference architecture",
        "text": "\"A reference architecture is a document or set of documents that provides recommended structures and integrations of IT products and services to form a solution. The reference architecture embodies accepted industry best practices, typically suggesting the optimal delivery method for specific technologies.\"\n\n\"Reference architectures help project managers, software developers, enterprise architects, and IT managers collaborate and communicate effectively about an implementation project. A reference architecture anticipates\u2014and answers\u2014the most common questions that arise. As a result, they help teams avoid errors and delays that may occur without the use of a tested set of best practices and solution approaches.\"\n\n"
    },
    {
        "level": 3,
        "id": "LNILH6jK",
        "title": "Common Statistical Production Architecture (CSPA)",
        "text": "CSPA is a \"reference architecture for the statistical industry\", i.e., a set of **recommended patterns and approaches for the integration of business processes, applications, data, and IT infrastructure** for the creation and delivery of statistical products and services. \n\n- It covers all statistical production processes\n- It does not prescribe a particular technology environment\n\nCSPA defines a framework for common services for statistical production, providing savings opportunities for collaborative development, sharing and re-use across different statistical processes and across different organizations. \n\nA common statistical production architecture enables project managers, statisticians, IT specialists and other stakeholders **to collaborate and communicate effectively** about specific statistical productions projects and programmes.\n\nBy identifying patterns for the integration of technology and applications that have been successfully used across different statistical projects and organizations, CSPA helps teams **anticipate problems**,  **avoid errors and delays**, and **reduce costs**.\n\nA common enterprise architecture approach within and across statistical organizations allows them to respond to emerging risks and challenges, and to leverage new opportunities, by facilitating the reuse and sharing of solutions and services and enhancing standardization and interoperability of statistical production systems and processes. \n\n\n"
    },
    {
        "level": 3,
        "id": "YOp0ve5T",
        "title": "Changing data warehouse logic over time",
        "text": "Changes in data warehouse logic over time should be either \n- expressed with data (in the form of \"parameter tables\") using effective dates\n- captured in source control, so they can applied conditionally, allowing to build the full state of the data warehouse throughout all time periods, or\n\nBeauchemin (2018) illustrates this with the example of introducing a change in the way taxes are calculated in year t.  If a users \"back-fills\" data for year t-1, the change in tax calculation method should not be applied. \n\n"
    },
    {
        "level": 3,
        "id": "U1nPvhZy",
        "title": "Data virtualization",
        "text": "Data virtualization provides a single view of one or more data sources, allowing applications real-time access to to retrieve and manipulate data without imposing a single data model or having to specify how it is formatted or where it is physically located at the source.\n\nThe main objective is to provide quick and timely insights from multiple sources without having to embark on a major data project with extensive ETL and data storage\n\n- Reduce risks of data errors, \n- Avoid moving data around that may never be used\n- Bridge data across data warehouses, data marts, and data lakes without creating a new integrated physical data platform\n\nData virtualization uses various abstraction and transformation techniques to resolve differences in source and consumer formats and semantics. \n\nExisting data infrastructure can continue performing their core functions while the data virtualization layer just leverages the data from those sources. \n\n"
    },
    {
        "level": 3,
        "id": "Aslc8kRG",
        "title": "Data warehouse",
        "text": "Data warehouses are \"logically centralized data repositories where data from operational databases and other sources are integrated, cleaned up, standardized [and stored over the long run] to support business intelligence\".  They constitute \"central locations that data analysts ... can go to, to access all their data\"\n\nData warehouses implement data organization, access, and aggregation methods to support multidimensional views of the integrated data.  They are optimized for dealing with analytical queries (as opposed to transactional queries). In addition, they are designed for ease of understanding, so analysts can connect their analytic and visualization tools to them without having to invest much time understanding the underlying data structures. \n\nThe term Data Warehouse was coined by William Inmon in 1990, which he defined as \"a subject-oriented, integrated, time-variant and non-volatile collection of data in support of management's decision making process.\"   \n- *Subject Oriented:* Provides information about a particular subject.\n- *Integrated:* Merges data from multiple sources into a coherent whole.\n- *Time-variant:* Each data point refers to a specific time period.\n- *Non-volatile:* Data can be added but data is never removed from the warehouse. \n\n(Source: \"What is a Data Warehouse?\" W.H. Inmon, Prism, Volume 1, Number 1, 1995).\n\nData warehouses can run into problems with large volumnes of data -- they may need to truncate older data and keep only summary tables.  An query performance can be an issue.  Using a data warehouse database as the main interface for data is not optimal for some machine learning tools, since the data must be unloaded from the database before it can be operated on. \n\n"
    },
    {
        "level": 3,
        "id": "7UvCpDNd",
        "title": "Data pipeline requirements",
        "text": "Some important data pipeline requirements include:\n\n- **Low event latency**: Ability to query the data as soon as it has been collected\n- **Interactive querying**: Support both batch queries and smaller interactive queries allowing data scientists to explore the tables and schemas\n- **Scalability**: Handle increasing data volumes as the project scales\n- **Versioning**: Ability to make changes to the pipeline without bringing it down and losing data\n- **Monitoring**: Generate alerts when data stops coming into, or flowing through, the pipeline.\n- **Testing**: Ability to perform tests on the pipeline without interrupting the work of others\n- **Clear distinction between development and production environments**: The project should not interfere with daily business operations\n\n"
    },
    {
        "level": 3,
        "id": "yerBxsCk",
        "title": "Hybrid data warehouse / data lake architecture",
        "text": "Setting up both a data warehouse and a data lake allows to work with both structured/persistent and unstructured/ephemeral data in the analytics pipeline.\n\nThe general pattern is to store semi-structured data in a distributed database (the data lake) and run ETL processes to extract the most relevant data to an analytics database (the data warehouse).  But although most analysts interact with the data through the data warehouse, some may work in the data lake environment directly. \n\n"
    },
    {
        "level": 3,
        "id": "WV9JV2CP",
        "title": "Data storage layer in a data science project",
        "text": "Every data innovation project needs a clear strategy for saving and accessing data.  One of the first steps in a data science project is to design and build the data storage layer, consisting of one or several data warehouses and data lakes that allow to capture, organize and to access structured and un-structured data from multiple sources.\n\nThe data storage layer needs to support the analytic tools and technologies used by the data science team. \n\nThe choice of a data storage solution will determine, for instance, whether data scientists will need to unload the data before working on it, or whether they can perform in-database analytics.\n\nA data storage strategy needs to be based on:\n\n- Types of data that will be used in the project\n- Financial, technical and human resources available for the project\n- Existing data management processes and data storage infrastructure\n- Project priorities (i.e., speed, referential integrity, data security, backups...)\n\n"
    },
    {
        "level": 3,
        "id": "zDox7KV4",
        "title": "Data migration patterns",
        "text": "In a typical data migration / integration architecture, one or more source systems deliver a document or message to a middleware component (ETL tool) that then pushes to one or more target systems. \n\n**\"Kill and fill\"**: First delete existing data in the target system, and then  replace it with new data \n\n"
    },
    {
        "level": 2,
        "id": "OX8Uv2GF",
        "title": "Components of a data integration process",
        "text": "**Target system** receive records from multiple **source systems**. \n\n**Data profiling** is used to discover and document the structure, content, and relationships among source data.  \n\n**Data matching** is used to identify records in different source systems that refer to the same entity\n\n**Source-to-target mapping** provides the metadata and transformation rules needed to convert data from each source system into the structure and content required by the target system.  (building correspondences between different sources, including correspondences between sources with different geographic and temporal granularity)\n\n**Business rules for ETL processes** are criteria and conditions for transforming data from source to target, including exception handling \n\n**Testing source data** allows to discover inconsistencies with respect to ETL business rules\n\n**Version control system** records changes to ETL pipelines and data over time, allowing to recall specific versions at any given moment. \n\n"
    },
    {
        "level": 3,
        "id": "oBtYWEfG",
        "title": "Data profiling",
        "text": "Process of reviewing source data to discover its structure, content and relationships. It involves:\n\n- Calculating descriptive statistics (e.g., min, max, count, sum...).\n- Collecting data types, length and recurring patterns.\n- Tagging data with keywords, descriptions or categories.\n- Assessing data quality.\n- Discovering metadata and assessing its accuracy.\n- Identifying key candidates, foreign-key candidates, functional dependencies, embedded value dependencies, \n- Performing inter-table analysis.\n- Document connection/access details\n  - If the data is available as text file, is it comma delimited, tab-delimited, or something else?\n  - If the data is available from a database, what are the connection details?\n  - If the data is available through a web service, what are the API endpoints and in what is the serialization format of the response message?\n  - What are the authentication mechanisms granting access to the data?\n\n\n**Data profiling good practices**\n\n1. Conduct data profiling at project start to discover if data is suitable for analysis\u2014and make a \u201cgo / no go\u201d decision.\n2. Identify and correct data quality issues in source data, even before starting to move it into target database.\n3. Identify data quality issues that can be corrected by Extract-Transform-Load (ETL), while data is moved from source to target. \n4. Identify any additional manual processing that might be needed.\n5. Identify unanticipated business rules, hierarchical structures and foreign key / private key relationships, use them to fine-tune the ETL process.\n\n**Basic data profiling techniques**\n\n- Distinct count and percent\u2014identifies natural keys, distinct values in each column that can help process inserts and updates. \n- Percent of zero / blank / null values\u2014identifies missing or unknown data. \n- Minimum / maximum / average string length\u2014helps select appropriate data types and sizes in target database. \n\n**Advanced data profiling techniques**\n\n- Key integrity\u2014ensure keys are always present and identify orphan keys\n- Cardinality\u2014check for one-to-one, one-to-many, many-to-many relationships between related data sets. \n- Pattern and frequency distributions\u2014check if data fields are formatted correctly (e.g., emails, postal codes...)\n- Record matching\u2014Identify records that refer to the same entity by analyzing across fields that provide partial identification\n\n"
    },
    {
        "level": 3,
        "id": "IF5MS5t2",
        "title": "Data matching",
        "text": "Process of finding records in one or more datasets that refer to the same entity.  This process can be deterministic (by matching unique identifiers) or probabilistic (based on some measure of similarity between field values that provide partial identification). \n\n"
    },
    {
        "level": 3,
        "id": "liKE24lR",
        "title": "Regularity/Frequency of data migration tasks",
        "text": "- **One-time migration:**\n  An existing data source is moved once into a target destination, often after performing several transformations and tests through code or scripting.\n- **Nightly integration**:\n  Data from external system is loaded on a regular basis (e.g., every night)  into the target system.  This approach is common in data warehouses. \n- **Near real time integration:**\n    Listener services allow for integration to be triggered the moment data is input into the source system \n- **Hypbrid approach**\n\nNightly and real-time integration require the automation of the ETL task, which assumes that the content and structure of the source system is predictable and that exceptions can be adequately handled. \n\n"
    },
    {
        "level": 3,
        "id": "4HybrQML",
        "title": "Data integration: Single Point of Truth",
        "text": "Single Point of Truth (SPOT) or Master Data Management (MDM) system allows to reconcile data from multiple systems into a single data hub, while allowing users to trace back these reconciled data assets to their original sources. \n\nSuch a system provides a stable access point for analysis work aimed to generate value and insights from a variety of multiple data assets. \n\n"
    },
    {
        "level": 3,
        "id": "DdmZ5wq7",
        "title": "Data integration challenges",
        "text": "\"Some enterprise landscapes are filled with disparate data sources including multiple data warehouses, data marts, and/or data lakes, even though a Data Warehouse, if implemented correctly, should be unique and a single source of truth.\" \n\nThe simple task of moving data from A to B \"can turn quite messy very fast and without warning\". \n\n- Lack of standardization of ETL solutions and tools\n- Mismatch between data from source system, existing ETL pipelines, and target system schema.\n- Insufficient, incomplete, or poorly communicated metadata\n- Data quality issues\n\nNeed to research, document and test inbound data\n\n"
    },
    {
        "level": 2,
        "id": "Aslc8kRG",
        "title": "Data warehouse",
        "text": "Data warehouses are \"logically centralized data repositories where data from operational databases and other sources are integrated, cleaned up, standardized [and stored over the long run] to support business intelligence\".  They constitute \"central locations that data analysts ... can go to, to access all their data\"\n\nData warehouses implement data organization, access, and aggregation methods to support multidimensional views of the integrated data.  They are optimized for dealing with analytical queries (as opposed to transactional queries). In addition, they are designed for ease of understanding, so analysts can connect their analytic and visualization tools to them without having to invest much time understanding the underlying data structures. \n\nThe term Data Warehouse was coined by William Inmon in 1990, which he defined as \"a subject-oriented, integrated, time-variant and non-volatile collection of data in support of management's decision making process.\"   \n- *Subject Oriented:* Provides information about a particular subject.\n- *Integrated:* Merges data from multiple sources into a coherent whole.\n- *Time-variant:* Each data point refers to a specific time period.\n- *Non-volatile:* Data can be added but data is never removed from the warehouse. \n\n(Source: \"What is a Data Warehouse?\" W.H. Inmon, Prism, Volume 1, Number 1, 1995).\n\nData warehouses can run into problems with large volumnes of data -- they may need to truncate older data and keep only summary tables.  An query performance can be an issue.  Using a data warehouse database as the main interface for data is not optimal for some machine learning tools, since the data must be unloaded from the database before it can be operated on. \n\n"
    },
    {
        "level": 3,
        "id": "6OomPFuc",
        "title": "Star schema",
        "text": "Data warehouses generally implement a simple **star schema**, which consists of normalized fact and dimension tables that can be easily used to build denormalized tables for analytic purposes.\n\nA star schema design helps balance between ETL maintainability and ease of analytics.\n\n*Fact tables* - Contain the business metrics of interest\n*Dimension tables* - Contain slowly changing attributes (often organized in a hierarchical structure) that can be joined with the fact tables\n\n"
    },
    {
        "level": 3,
        "id": "hKNO8E7M",
        "title": "Database management systems (DBMS)",
        "text": "Database management systems support the creation, use and maintenance of databases.  They provide efficient data storage and retrieval, as well as tools for data acquisition, maintenance, formatting and dissemination.\n\n"
    },
    {
        "level": 3,
        "id": "ePGUzfl7",
        "title": "Non-relational (No-SQL) databases",
        "text": "Non-relational (No-SQL) databases are \"scheme-agnostic\". They support the storage and manipulation of large volumes of unstructured and semi-structured data.  It is not necessary to specify in advance the types of data that will be stored in a No-SQL database, as it can accommodate changes in data types and data schemas. \n\nNo-SQL databases are designed to distribute data across different nodes; as a consequence, in many cases data consistency is not guaranteed. \n\nSome types of No-SQL databases include:\n\n1. *Graph databases*: They represent data as a network of related nodes, and are particularly suited to analyze relationships between heterogeneous data points.\n\n2. *Document stores*: They store data in XML and JSON format, using the document name as key and the contents of the document as value.   These documents can contain many different value types and can be nested.  They are particularly useful to manage semi-structured data across distributed systems.   Examples include *MongoDB* and *CouchBase*.\n\n3. *Wide-column stores*: This type of database store data in column families or tables.  They are designed to handle very large volumes of distributed data.   Examples include *Cassandra*, *Scylla*, and *HBase*.\n\n4. *Key-value stores*: They store only key-value pairs and provide basic functionality for retrieving the value associated with a known key.  They work best with a simple database schema and when speed is important.  Examples include: *Redis*, *DynamoDB*, *Cosmos*.\n\n"
    },
    {
        "level": 3,
        "id": "CvXRU020",
        "title": "Logical warehouse of data inputs",
        "text": "Most data innovation projects rely on a large variety of source data generated or compiled by many different governmental and non-governmental organizations.  These sources include many large sets of structured, semi-structured and unstructured data, ranging from earth observation data, call detail records, and sensor data, to microdata from administrative records and data from sample surveys or census programmes.\n\nAs data sources grow, performing data analytics with multiple databases can become inefficient and costly. Thus, as part of a data innovation project, it is necessary to establish \"logical warehouse\" of data inputs, a single point of entry providing access to **analysis-ready, geo-referenced data inputs** from multiple sources, organized according to a simple and commonly agreed taxonomy, such as the **fundamental geospatial data themes** \n\nA lot of work may be required to \"condition\" the different data inputs in order to make them ready for use and analysis, including the adoption of data **interoperability standards and best practices** across different data sources and systems.  \n\nUnderlying this logical warehouse of source data can be a mixture of enterprise data warehouses and data lakes, which work together to provide access to \"immutable' data inputs and allow to trace transformations at a specific step in the pipeline,  thus enabling to test and reproduce estimation results.  \n\n"
    },
    {
        "level": 2,
        "id": "Tcm6TD6G",
        "title": "Quality and the integration and standardization of statistical production systems",
        "text": "\n**Requirement 11.6** of the \"National Quality Assurance Frameworks Manual for Official Statistics\" calls statistical agencies to define, promote and implement integrated and standardized production systems, including through  the promotion, sharing and implementation of standardized solutions that increase effectiveness and efficacy.  It also calls for them to adopt a statistical business architecture based on international standards and tools, such as GSBPM, GAMSO, CSPA, and SDMX. \n\n\n"
    },
    {
        "level": 3,
        "id": "p9p1qFpa",
        "title": "Total Quality Management",
        "text": "Total Quality Management is the systematic and ongoing monitoring and identification of quality issues across an entire organization, as well as the planning and implementation of corrective actions, aimed to maintain and continuously improve over time the quality of the all the products and services delivered to its customers. \n\n"
    },
    {
        "level": 3,
        "id": "np2OKCzl",
        "title": "Quality of statistical information",
        "text": "Quality of statistical information refers to the degree in which statistical products and services satisfy the needs and requirements of their intended users.  \n\nThe measurement of quality of statistical outputs is made operational through the specification of a set of indicators along various dimensions, which are often referred to as a \"Data Quality Assessment Framework\".    \n\nThis dimensions and indicators of such a framework usually include:\n\n- **Relevance**: Extent to which the information content of the data delivered is pertinent to help users attain their own goals more effectively.\n\n  - *Indicators of relevance may include:*\n    - References in media\n    - Number of website hits\n    - Proportion of records pertaining to statistical units outside the target population\n    - Proportion of units in the target population that are not represented in the dataset\n\n- **Accuracy**: The closeness of a statistical data point or data set to the true value or set of values that it is intended to measure.  \n\n  - *Indicators of accuracy may include:*\n    - \n    - Proportion of correct predictions among the total number of cases examined\n    - Average size of (ex-post) prediction errors among the total number of cases examined\n\n- **Reliability**: The degree in which the same overall data generation process or method can be expected to yield the same results in repeated trials.  In situation where multiple revisions of an estimate are published over time, reliability can be thought of as the closeness of the initial estimate to the subsequent estimated values. \n\n  - *Indicators of reliability may include:*\n    - Sample size relative to population size\n    - Standard deviations \n    - Standard error estimated using a resampling method (e.g., bootstrap or jackknife estimation)\n    - Number and size of subsequent revisions\n\n- **Coherence and comparability:**  Degree to which two data points (or two datasets) can be assumed to be the result of the same overall data generation process, thus allowing to meaningfully combine them for analytic purposes.  In other words, for two data points (or data sets) to be deemed coherent and comparable, the assumptions made about the respective processes that generated them should not stand in contradiction to each other in any significant way.  For statistical data, coherence and comparability require in particular the use of common standards with regard to the concepts, definitions, classifications, and methods employed in their collection, processing, and dissemination.\n\n  - *Indicators of coherence and comparability may include:*\n    - Relative size of discrepancies in spatio-temporal trends exhibitied between two or more datasets that provide the same or similar information on the same subject matter\n    - Whether or not to datasets use the same concepts, definitions, and classifications (e.g., same definition of statistical units; common sampling frameworks; same definition of geographic areas and reference time periods; same temporal frequency and geographic granularity; same definition of age groups; etc.) \n\n- **Timeliness**: Extent to which the statistical information pertaining to the measurement of a past phenomenon or event is delivered in time for users to be take meaningful action or make consequential decisions.\n\n  - *Indicators of timeliness may include:*\n    - Gap between publication date and reference period\n\n- **Punctuality:** Degree to which statistical information is delivered according to an initially announced publication schedule.\n\n  - *Indicators of reliability may include:*\n    - Gap between actual and planned publication dates\n\n- **Completeness**: Degree to which the data set includes all the relevant information expected by the user.\n  - *Indicators of completeness may include:*\n    - Number of records that are missing key variables\n    - Total percentage of empty cells\n    - Proportion of statistical units in the target population that are not represented in the data set\n    - Proportion of statistical units in the sample design for which information is available / Response rates\n\n- **Accessibility:** Ease with which users can find and obtain the statistical data in a ready-to-use format and in line with open data standards.\n\n- **Interoperability**:  Ease with which a data set can be ingested and handled by multiple, independent, standard-compliant data processing and analysis systems. \n  - *Indicators of data security and confidentiality may include:*\n    - Use of standard classifications, concepts and definitions\n    - Use of open, machine-readable data formats and serializations\n\n\n- **Clarity:** Extent to which the statistics are presented to users in an understandable manner and in a form that facilitates its proper interpretation and use. \n  - *Indicators of data security and confidentiality may include:*\n    - Availability of complete and easy-to understand reference metadata (including glossary and methodological notes)\n    - Use of meaningful, human-readable variable names\n\n\n- ** Security and confidentiality:** Effectiveness with which the integrity of the data is preserved and the risks of disclosing confidential information about individuals, households and businesses are mitigated.\n\n- **Compliance with applicable legal requirements**:\n\n\nUser satisfaction surveys are one means to collect data on quality indicators.\n\n**Relationship between dimensions of quality of statistical information:**\n\n- **Accuracy vs reliability**:  When there is a systematic bias in the data production process, then increasing the sample size will generally increase reliability but will not improve accuracy.\n\n\n"
    },
    {
        "level": 3,
        "id": "dva8i69T",
        "title": "Standard classifications",
        "text": "Standard classifications enable the integration of multiple data sets, as well as their consistent analysis and interpretations.  \n\nThey provide the taxonomical basis for managing and describing statistical data, and are fundamental components of key frameworks for the compilation of official statistics (e.g., System of National Accounts).\n\nStatistical classifications need to be easy to understand and to use, and be revised periodically to remain relevant in a changing world.\n\nThey also need to be accessible from a central repository, and leverage new metadata modelling approaches and linked open data technologies. \n\n\n\n"
    },
    {
        "level": 3,
        "id": "NImn69Rs",
        "title": "Quality and integration of administrative and other data sources",
        "text": "**Requirement 13.4** of the \"National Quality Assurance Frameworks Manual for Official Statistics\" calls statistical agencies to promote the sharing, linking, and use of administrative and other data sources to minimize respondent burden, including by:\n\n- Creating and sharing documentation about the quality and other characteristics of those data\n- Developing and implementing technical tools for sharing and linking them, while protecting data confidentiality\n- Promoting them as an alternative to survey-based data\n\n\n"
    },
    {
        "level": 2,
        "id": "sKuMe1ca",
        "title": "Standardizing ETL tools and solutions",
        "text": "The adoption of different, often proprietary ETL tools for data integration in different departments of the same organization often leads to an increase in data silos.\n\nWhen \"one department of an organization, perhaps focusing on its own need, [adopts] a completely different ETL tool than another department[, the result is] an *increase* in data silos, rather than a decrease.\"\n\nOrganizations often work with multiple ETL solutions, each with its own framework, ... \"relegating the entire architecture into a big mess\"\n\n"
    },
    {
        "level": 3,
        "id": "mcXbI3UY",
        "title": "Automation of routine statistical processes",
        "text": "Modern information and communication technologies can be used to improve performance of statistical processes through automation. \n\nFor example, routine clerical operations and statistical procedures (including ETL operations and and validation of data inputs and outputs) should be standardized and automated as much as possible.  \n\n"
    },
    {
        "level": 2,
        "id": "uKiS7iJH",
        "title": "Role of data integration architect",
        "text": "The integration architect works closely with ETL developers and stakeholder teams gathering and clarifying business and functional requirements, and conducts research to understand business rules and technical specifications of source and target systems. \n\nThe data integration architect needs to have good understanding of how all the parts of the NSS ecosystem work together, where and how can all relevant data sources be accessed by statisticians and data scientists, and what are the data storage and processing environments they need in order to do their job.  She or he also should have a clear idea of how will the outputs produced by statisticians and data scientists be fed into regular data dissemination channels. \n\nThe data integration architect will also help define the nature, frequency and schedule of data integration tasks (i.e., one-time data migration; nightly/weekly/monthly integration; on-demand batch integration, real-time/stream integration...)\n\nShe or he plays a key role in facilitating communication and common understanding among strategic and operational members of the broader data integration team.  After the discovery and testing phase, hand off work to developers. \n\n\n"
    },
    {
        "level": 3,
        "id": "wHkdn3us",
        "title": "Enterprise architecture skills",
        "text": "Roles and responsibilities of a enterprise architect include:\n\n- Evaluate the existing operating environment across the enterprise to identify redundancies, gaps and inefficiencies\n- Working with stakeholders to facilitate the discovery and documentation of the customer\u2019s business scenarios in order to understand real needs, as opposed to wants, and to translate them into business requirements\n- Conceptualize technical solutions to complex problems and meet business requirements, developing well-formulated models of the components of these solutions.\n- Verify stability, security, portability, scalability and interoperability, of the proposed enterprise architecture\n- Evaluate and guide the selection of technologies to implement the proposed enterprise architecture.\n- Collaborate with stakeholder to gain organizational commitments for all process, systems and application plans\n- Communicate project information through presentations, technical reports, or white papers \n\nDifferent roles of an enterprise architect and a solution designer/ builder:\n\n- The enterprise architect must remain at a a high level of abstraction, concentrating on the few critical components and their interfaces. \n- The solution designer/builder translates architecture into deliverable components.\n\n"
    },
    {
        "level": 3,
        "id": "jwCdelDE",
        "title": "Facilitating collaboration amongst data providers and users",
        "text": "Data integration activities require collaboration across providers and uses of data, both within and across organizations.  Each representative brings unique concerns and opinions, and different knowledge of the overall data ecosystem.  \"Everyone needs to be on the same page when it comes to build a data integration system\"\n\n- Ensure that data are well documented, consistent and of known quality\n- Harmonize the spatial and temporal scales over which data is collected\n- Agree on common metadata requirements\n- Identify tradeoffs and reach compromises regarding data formats, standards and conventions.\n\n"
    },
    {
        "level": 3,
        "id": "62wtBu4C",
        "title": "Data integration for sustainable development",
        "text": "The integrated use of data and information from multiple sources (both traditional ones, such as census and survey programmes, and non-traditional ones, such as administrative records, satellite imagery, etc.) is crucial to enable all stakeholders to make smarter, evidence-based decisions towards achieving sustainable development.  \n\nThis includes the ability to effectively and efficiently bring together data assets which often have been developed in isolation from each other and only with a narrow set of operational or very short-term needs in mind, for their joint processing, analysis and dissemination throughout the data value chain.  \n\nMoreover, it includes the development of cross-sectoral reporting tools for tactical and strategic decision making and advocacy.\n\n\n\n"
    },
    {
        "level": 3,
        "id": "irmKQb3k",
        "title": "Roles in data innovation project team",
        "text": "- **Data scientist**: Understands data science modelling techniques\n- **Data engineer**: Understands dataflow architecture\n- **Data platform administrator**: \n- **Database administrator**: Understands individual DBMSs\n- **Data administrator**: Plans and sets policies related to data\n\n"
    },
    {
        "level": 2,
        "id": "liKE24lR",
        "title": "Regularity/Frequency of data migration tasks",
        "text": "- **One-time migration:**\n  An existing data source is moved once into a target destination, often after performing several transformations and tests through code or scripting.\n- **Nightly integration**:\n  Data from external system is loaded on a regular basis (e.g., every night)  into the target system.  This approach is common in data warehouses. \n- **Near real time integration:**\n    Listener services allow for integration to be triggered the moment data is input into the source system \n- **Hypbrid approach**\n\nNightly and real-time integration require the automation of the ETL task, which assumes that the content and structure of the source system is predictable and that exceptions can be adequately handled. \n\n"
    },
    {
        "level": 3,
        "id": "TvEdCbEX",
        "title": "Data Lakes",
        "text": "A data lake is designed to capture and store multiple sets of raw data at scale for a low cost.  It stores many different types of data in the same repository, and may also allow to perform transformations on the data. \n\nData lakes are designed to handle large volumes of streaming data and to make many different types of data readily available for analysis,  without having to move the data to a separate environment.   They tend to be more ephemeral (data is housed for the short-term) and generate outputs \n\nData lakes are setup without having to first define the data structure, following a **schema on read** principle. This means that  the structure of the data is only defined at the time it is used.   \n\nGiven the large variety of data that can be poured in them, **data lakes require a sound governance framework**, including clear data quality and metadata management protocols so they don't become \"data swamps\". \n\n"
    },
    {
        "level": 3,
        "id": "1cHc8MiO",
        "title": "Reproducibility of processes",
        "text": "The ability to reproduce processes is crucial to generate trust in their outputs.\n\nReproducibility requires immutable data and versioned logic (adoption of functional programming).\n\nReproducibility allows to schedule repeatable processes to auto-execute on specific intervals\n\n"
    },
    {
        "level": 2,
        "id": "zDox7KV4",
        "title": "Data migration patterns",
        "text": "In a typical data migration / integration architecture, one or more source systems deliver a document or message to a middleware component (ETL tool) that then pushes to one or more target systems. \n\n**\"Kill and fill\"**: First delete existing data in the target system, and then  replace it with new data \n\n"
    },
    {
        "level": 2,
        "id": "4HybrQML",
        "title": "Data integration: Single Point of Truth",
        "text": "Single Point of Truth (SPOT) or Master Data Management (MDM) system allows to reconcile data from multiple systems into a single data hub, while allowing users to trace back these reconciled data assets to their original sources. \n\nSuch a system provides a stable access point for analysis work aimed to generate value and insights from a variety of multiple data assets. \n\n"
    },
    {
        "level": 2,
        "id": "NImn69Rs",
        "title": "Quality and integration of administrative and other data sources",
        "text": "**Requirement 13.4** of the \"National Quality Assurance Frameworks Manual for Official Statistics\" calls statistical agencies to promote the sharing, linking, and use of administrative and other data sources to minimize respondent burden, including by:\n\n- Creating and sharing documentation about the quality and other characteristics of those data\n- Developing and implementing technical tools for sharing and linking them, while protecting data confidentiality\n- Promoting them as an alternative to survey-based data\n\n\n"
    },
    {
        "level": 3,
        "id": "e7UgXxck",
        "title": "Data quality assessment roadmap",
        "text": "- Set a timetable for developing and implementing a data quality assessment strategy\n- Describe the data quality principles and policies to be used across the organization. \n- Secure commitment to data quality by senior management and key stakeholders\n- Establish governance framework for data quality, including roles and responsibilities around data quality management\n- Describe main statistical production processes within the organization (e.g., using the phases of the Generic Statistical Business Process Model, or GSBPM, as a reference)\n- Identify and describe the structure, definitions, attributes and inter-relationships of data inputs and outputs of each stage of the main statistical production processes (e.g., using the Generic Statistical Information Model, or GISM).\n- Develop a Quality Assessment Framework for the organization, formulating for the each input and output of every stage of the main statistical production processes:\n    - Relevant quality characteristics\n    - Appropriate quality indicators\n    - Methods and sources of data for compiling quality indicators \n- Provide training and develop guidelines for implementing data quality assessment framework across the organization\n- Compile and report quality indicators on a continuous basis\n- Analyze information from quality indicators\n- Identify necessary actions to improve quality of statistical processes and outputs (e.g., using the Generic Activity Model for Statistical Organizations, or GAMSO)\n\n"
    },
    {
        "level": 2,
        "id": "1cHc8MiO",
        "title": "Reproducibility of processes",
        "text": "The ability to reproduce processes is crucial to generate trust in their outputs.\n\nReproducibility requires immutable data and versioned logic (adoption of functional programming).\n\nReproducibility allows to schedule repeatable processes to auto-execute on specific intervals\n\n"
    },
    {
        "level": 3,
        "id": "v0j3xMCw",
        "title": "Functional programming",
        "text": "A programming paradigm \"that treats computation as the evaluation of mathematical functions\", where the output of each computation depends only on the arguments passed to it (thus isolating logic changes from data/state changes)\n\nThe goal is to break the application logic into smaller functions, where each function is focused on a single task. Functions are then **'composed'** into larger functions.\n\nAdvantages of functional programming:\n\n- Improve modularity: Individual functions can be written and tested in isolation \\(without having to understand external context\\)\n- Enhance reproducibility of results\n- Making code easier to understand\n- Making outputs easier to predict\n\nFunctional programming is part of declarative programming.\n\nSee also (https://en.wikipedia.org/wiki/Functional_programming)\n\n"
    },
    {
        "level": 3,
        "id": "vLPsC8l0",
        "title": "Pure tasks",
        "text": "- **Pure tasks** produce the same result every time they are run.\n- **Overwrite approach**: \"Re-executing a pure task with the same input parameters should overwrite any previous output that could have been left out from a previous run of the same task.\"\n- Tasks can become \"purified\" by breaking them down into smaller tasks, each of which targets a single output. \n\n"
    },
    {
        "level": 3,
        "id": "9h1AmchL",
        "title": "Establishing trust in results of data innovation projects",
        "text": "It is crucial to establish trust in the results of data innovation projects, particularly from key users such as policy and decision makers.\n\nThe quality of estimation results \"is only as good as the quality of the input data and the methodologies employed.\"  Therefore, the reliability of the sources and methods involved in data innovation projects is central to their widespread acceptance and use. \n\nThis highlights the importance of **transparent and participatory validation**:\n\n- Validate quality of data inputs\n- Validate methodology\n- Ensure that data production process are reproducible by independent reviewers\n- Check for internal and external consistency of results\n- Compare pre-existing users' perceptions with project results\n\n\n"
    },
    {
        "level": 2,
        "id": "62wtBu4C",
        "title": "Data integration for sustainable development",
        "text": "The integrated use of data and information from multiple sources (both traditional ones, such as census and survey programmes, and non-traditional ones, such as administrative records, satellite imagery, etc.) is crucial to enable all stakeholders to make smarter, evidence-based decisions towards achieving sustainable development.  \n\nThis includes the ability to effectively and efficiently bring together data assets which often have been developed in isolation from each other and only with a narrow set of operational or very short-term needs in mind, for their joint processing, analysis and dissemination throughout the data value chain.  \n\nMoreover, it includes the development of cross-sectoral reporting tools for tactical and strategic decision making and advocacy.\n\n\n\n"
    },
    {
        "level": 3,
        "id": "5k2KowcP",
        "title": "Business intelligence vs. transaction data processing",
        "text": "- Transaction processing typically deals with a few records at a time.  It is process-oriented, and relies on current data at the individual-record level.  \n- Business intelligence processing may deal with thousands or millions of records at a time. It is subject/topic-oriented, and relies on historical data at both the individual-record and aggregate/summarized levels. \n\n"
    },
    {
        "level": 3,
        "id": "6SXNQUaG",
        "title": "Inter-linkages across SDGs",
        "text": "The goals and targets of the 2030 Agenda constitute an \"integrated and indivisible\" whole. All the SDGs are interconnected, and each one of them is crucial for the well-being of individuals and societies.\n\nCross-sectoral coordination requires different actors to understand the complex interdependencies across different goals and targets that result form a multitude of trade-offs and synergies among the economic, social and environmental dimensions of sustainable development.\n\nIt is crucial to understand how does making progress made on a particular target reinforce or thwart progress on other targets. Holistic sustainable development solutions are those that maximize synergies and minimizing mutually off-setting effects between different SDGs.\n\n\n\n\n"
    },
    {
        "level": 3,
        "id": "jt99xW61",
        "title": "Objectives of Data4Now country projects",
        "text": "The Data4Now initiative aims to provide timely and disaggregated information needed by policy and decision makers to better design development strategies and deliver public policies to achieve the 2030 Development Agenda.  \n\nIt mobilizes resources and partnerships to support countries in the use of innovative technologies, data, and methods to obtain insights on key aspects of sustainable development, such as poverty, food security, education, health, disaster-risk resilience, etc.\n\nIts addresses all aspects of the data lifecycle, from data integration and data engineering to the use of data science methods and data communication and visualization tools.\n\nCollaboration in multi-functional teams covering different areas of expertise allows participating countries to leverage synergies and interlinkages between different domain areas.  For instance: \"How are drought patterns and commodity prices correlated with crop yields, poverty estimates, and population movements?\"\n\nThe initiative promotes the systematic use of sound data quality frameworks and data ethics principles in data innovation projects. \n\n"
    },
    {
        "level": 2,
        "id": "DdmZ5wq7",
        "title": "Data integration challenges",
        "text": "\"Some enterprise landscapes are filled with disparate data sources including multiple data warehouses, data marts, and/or data lakes, even though a Data Warehouse, if implemented correctly, should be unique and a single source of truth.\" \n\nThe simple task of moving data from A to B \"can turn quite messy very fast and without warning\". \n\n- Lack of standardization of ETL solutions and tools\n- Mismatch between data from source system, existing ETL pipelines, and target system schema.\n- Insufficient, incomplete, or poorly communicated metadata\n- Data quality issues\n\nNeed to research, document and test inbound data\n\n"
    },
    {
        "level": 3,
        "id": "GUadXwe7",
        "title": "Statistical organizations' threats and challenges",
        "text": "Threats\n\n- Rigid processes and methods\n  - Process and methodology changes are time consuming and expensive \n  - Inability of statistical frameworks, standards and classifications to remain relevant to modern information needs\n- Inflexible aging technology environments and legacy systems\n- Quickly changing information needs\n- Pressure to reduce provider burden \n- Inability to attract and retain critically skilled staff in a competitive market\n  - The skill-sets that underpin statistical organizations are becoming increasingly valuable in the wider market, making it difficult for statistical organizations to compete to attract and retain these skills.\n\nChallenges:\n\n- Make richer use of existing information sources\n- Increase use of administrative data and alternative data sources\n- Enhance data interoperability across systems\n\n"
    },
    {
        "level": 3,
        "id": "yHy01yYb",
        "title": "Assessing quality of source data",
        "text": "The choice of data sources for the production of statistics should be based on multiple criteria, including:\n\n- Cost effectiveness / Sustainability of the data source\n- Extent to which the source satifies the information needs of users (relevance)\n- Verify quality of individual data sources\n  - Appropriate concepts and definitions viz-a-viz the question at hand\n  - Level of disaggregation\n    - Geographic granularity\n    - Temporal frequency\n    - Disaggregation by sex, age, product, activity, etc.\n  - Availability / Completeness of data\n    - Missing values\n    - Absence of values for key variables\n    - % of empty cells\n  - Timeliness\n  - Coverage / Sample bias / representativity: Verify that the underlying population of the data generating process is consistent with the statistical output requirements \n    - % or records in the data that do not belong to the target population\n    - % of units in the target population that are not represented in the dataset\n    - Population subgroups that are over- or under-represented in the dataset\n  - Interoperability:\n    - Machine readability; standard serialization / formatting; use of standard classifications\n  - Internal consistency / Outlier detection\n  - Completeness and clarity of reference metadata\n  - Units of measurement are appropriate, well defined, identified\n- Verify comparability / consistency across data sources\n  - Consistent use of definitions of geographic areas\n  - Alignment of reference periods\n  - Consistent use of variable definitions\n  - Consistent use of standard classifications / levels of disaggregation\n- Data integrity / data security\n  - Duplicate records\n  - Processes in place to mitigate risk of disclosure of confidential information\n  - Security and integrity of data and their transmission\n\n\n"
    },
    {
        "level": 1,
        "id": "Dl66v10U",
        "title": "Scrum",
        "text": "Scrum is a **framework** consisting of values, principles and practices for organizing and managing work. Scrum is an agile approach for delivering high-quality products and services that maximize customer value in a timely and economical manner.  \n\nIt focuses on delivering smaller, more frequent releases that \"give customers what they really want, not just features they might have specified on the first day when they knew the least about their needs.\"\n\n\"Scrum ... assumes that the process necessary to create the product is complex and therefore would defy a complete upfront definition.\"\n\n\n"
    },
    {
        "level": 2,
        "id": "94L4EBHO",
        "title": "Agile project management",
        "text": "The deliverables (user stories) of a project are specified collaboratively over **short, time-blocked iterations** called sprints.  \n\nMembers of **self-organized, cross-functional teams** work together on common tasks, instead of working in isolation. Agility emphasizes frequent communication and empowerment of team members, as well as flexibility to change course at any stage of the project in order to meet the evolving stakeholders' needs. \n\nAgile project management **avoids big up-front architecture design decisions**, seeking instead to combine a small measure of up-front design with a healthy dose of emergent, just-in-time design.  Teams are encouraged to quickly explore new ideas and approaches and to learn fast whether a potential solution is viable or not. \n\nAn iterative process allows customers to discover and refine their own requirements as they gain information and knowledge in successive sprints. At the start of each sprint, the team decides which high-priority deliverables to work on so as to ensure constant forward momentum.  Each sprint includes activities for designing, building, testing, reviewing and launching specific product features, and the focus is on **delivering working features at the end of each iteration** that contribute to the final product. \n\n\n"
    },
    {
        "level": 3,
        "id": "MkZau9Uc",
        "title": "Waterfall project management",
        "text": "The waterfall model assumes the project can be divided in a linear sequence of phases (requirements analysis, design, implementation, testing, deployment, maintenance).  \n\nIt works well in situations where the requirements and task to be performed in order to meet those requirements are clearly defined in advance. \n\n"
    },
    {
        "level": 3,
        "id": "HhsYzH6I",
        "title": "Project management: establishing a rhythm",
        "text": "\"Establishing a rhythm is how teams [members] can maintain awareness of each other's workloads, upcoming decisions, and decision outcomes\"\n\nSchedule specific types of meetings at specific intervals:\n\n- **Weekly action meeting** \n  - *Objective:* Unblock the work and set the team up to get their work done for the next seven days.\n  - *Agenda:*\n    1. Check in - Everyone answers: \"What has your attention right now?\"\n    2. Checklist review - Everyone answers \"Yes/No\" to each checklist item\n    3. Metrics review - The team reviews the metrics to see whether we are on the right path\n   4. Project updates - \"What has changed in the project since last week?\"\n   5. Build agenda - Everyone adds topics to the agenda by calling out a placeholder word or phrase.  The team focuses on topics that will unblock the work in the coming week\n   6. Process agenda - Work through the agenda focusing on unblocking work for everyone\n   7. Checkout - Everyone answers: \"What did you notice?\"\n- **Monthly retrospect meeting** \n  - *Objective:* To look back on the past month and discuss necessary changes in the way the team works together.\n\n"
    },
    {
        "level": 3,
        "id": "MVQ6AEt4",
        "title": "Project management: Agile vs Waterfall",
        "text": "In practice, a mix of waterfall and agile methodologies is needed. \n\n"
    },
    {
        "level": 3,
        "id": "QZ4e6WSS",
        "title": "Organization of data innovation",
        "text": "Success in data innovation depends to a large extent on the adequate organization of workflows for the practical implementation of new sources, technologies and methodologies. \n\nIt requires **breaking down the amorphous task of \"data innovation\" into  separable tasks**, which can be completed within reasonable time, and which are clearly connected to the delivery of specific, tangible outputs, and finally to the achievement of well-defined outcomes.\n\n"
    },
    {
        "level": 3,
        "id": "nBYBmeNa",
        "title": "Innovation: Plans vs structured workflows",
        "text": "- **Innovation requires flexibility**. Detailed plans often impose too much structure for open-ended research or innovation projects that require flexibility.  \n- **Innovation cannot be predetermined**: Initial ideas are necessarily vague and change when we put them into practice.\n- Accidental encounters make up the majority of what we learn\n- The challenge is to have **overarching workflows** that allow for new ideas and insights to be generated, tested, adapted and mainstreamed\n\n"
    },
    {
        "level": 3,
        "id": "PoFWMOPo",
        "title": "Agile approach",
        "text": "Agile is an empirical approach for value delivery through team collaboration, based on a series of values, principles, practices and tools.\n\n- Teams are cross-functional and self-organizing\n- Upfront planning is limited to what is absolutely necessary to get the work started\n- The starting point is a product backlog, and the team always works  in short, time-boxed iterations / increments, focusing first on the most important or highest-priority items\n- At the end of each iteration:\n  - The team delivers a potentially shippable product\n  - The team reviews the completed features with the stakeholders to get their feedback\n- Based on feedback, product owner and team can change priorities on what to work next and decide how to do the work\n\n\n"
    },
    {
        "level": 3,
        "id": "9GzzV4Gn",
        "title": "Clarity Paradox: Success as a catalyst for failure",
        "text": "Clarity of purpose >> Success >> More options and opportunities >> Diffused efforts >> Less clarity of purpose\n\nEarly success often leads to \"the undisciplined pursuit of more.\" Thus, \"one simple antidote is the disciplined pursuit of less... purposefully, deliberately, and strategically eliminating the nonessentials... constantly reducing, focusing and simplifying.\"\n\nTo avoid the clarity paradox, we need to keep a constant check on the Sisyphean task of clarifying purpose.  \n\nAt the personal level, it means continuously finding the intersection between the set of activities we are deeply passionate about, those in which we are genuinely talented, and those that meet a significant need in the world. \n\nWe need to continuously 'figure out which ideas from the past are important and pursue those\" while throwing out the rest.  \n\nThis means also a keeping a constant check on WIP (work in progress): \"eliminating an old activity before (adding) a new one\" so \"you don't add an activity that is less valuable than something you are already doing\".\n\n"
    },
    {
        "level": 3,
        "id": "gteEMrtG",
        "title": "Transitioning to Agile project management",
        "text": "**Identify key supporters**\n\nGet the right people on board, including individual contributors form different functional areas, their managers, and a senior executive who can sponsor an agile pilot project.  These should become a team of supporters who see the value in the agile transition and are willing to try it. \n\nDiscuss with them the benefits of embracing an agile mindset. and how the agile principles and values can enhance delivery and predictability in the organization.  \n\n**Implement agile training and couching**\n\nEnsure there are training and couching opportunities for everyone in the organization to learn about the values, principles, and practices of the agile mindset.   \n\n**Focus on organizational culture**\n\nHelp people become comfortable with sharing and collaborating, explaining that when people learn more about what others do, and have an opportunity to collaborate with others outside their fields of expertise, everyone grows their own skills.  \n\n**Select and pilot project**\n\nIn order to transition from traditional to Agile project management, it's a good idea to select a medium-sized pilot project.  This will allow to test the agile fundamentals from start to finish and gather feedback on areas where the organization will need to change current practices. \n\nThe project should run for 4 to 12 weeks, involve all roles and cover all phases of development.  Moreover, it should not be a high-risk project, \n\n**Bottom-up vs. Top-down transformation**\n\nBottom-up transformation is usually faster, but may lead to issues id there is lack of senior management support. Top-down transformation may be challenging when teams do not know much about the agile mindset.\n\n**Building the business case and secure funding **\n\nOne of the main challenging aspects of an agile transformation effort is building the business case and securing funding for a project with flexible scope, which often goes against traditional way of doing things. \n\n"
    },
    {
        "level": 3,
        "id": "cPIa75Kg",
        "title": "How Spotify customized the use of Scrum",
        "text": "Spotify decided to make some of the standard scrum practices (e.g., sprint planning meeting, task breakdown, estimation, \u2026) optional.  They decided that \u201cAgile matters more than Scrum\u201d and that \u201cagile principles matter more than specific practices\u201d.  Some of the changes they introduced include:\n\n- Spotify renamed the role of \u201cscrum master\u201d to \u201cagile coach\u201d, emphasizing the need of servant-leaders rather than process masters, and instead of \u201cscrum teams\u201d they refer to their cross-functional, self-organizing teams as \u201cautonomous squads\u201d.  Each squad decides what to build, how to build it, and how to work together, within the limits of the squad\u2019s long-term mission, the product strategy, and short-term goals that are negotiated every quarter.  \n- Squads are loosely coupled but tightly aligned teams.  \u201cAlignment enables autonomy\u201d. The leader\u2019s job is to communicate what problem needs to be solved and why. The squad\u2019s job is to collaborate with each other to find the best solution. \n- Squads are grouped into \u201ctribes\u201d. Each person is simultaneously a member of a squad (product dimension), and a chapter (a competency area) that cuts across multiple squads in a tribe. \n- There are also \u201cguilds\u201d \u2013 or light-weight, company-wide communities of interest where people share knowledge on a specific area (leadership, web development, \u2026) through mailing lists, bi-annual conferences, and other informal communication methods. \n- Cross pollination rather than standardization: Instead of prescribing the use of particular tools, When enough squads use the same tool, that becomes a de-facto standard. This allows for a healthy balance between consistency and flexibility.\n- Internal open-source model. Anyone can edit any code. However, there is a culture of peer code review, which improves quality and promotes knowledge sharing.  \n- Culture of trust and mutual respect. Give credit to each other for great work, and seldom take credit for oneself. Agile at scale requires trust at scale, and that means no politics, and no fear. \n- Focus on speed: Small and frequent releases; investment in test automation and continuous delivery infrastructure. \n- Three types of squads:\n  - Feature squads: Focus on building features in one specific area (e.g., \u2018search\u2019), to be released across all platforms\n  - Client app squads: Focus on making all feature releases on one specific platform easy (e.g., desktop, iOS, Android)\n  - Infrastructure squads: Focus on providing tools and routines that make other squads more effective (e.g., continuous delivery, testing\u2026) \n\n\n"
    },
    {
        "level": 3,
        "id": "duUZSb3j",
        "title": "Collaborative data innovation projects",
        "text": "Collaboration across multiple organizations towards producing common outputs and achieving a shared outcomes allows data innovation projects to draw a wide range of data assets, skills and resources. \n\nData innovation projects require the collaboration of multiple government agencies and partners, with multi-stakeholder, multi-disciplinary teams working together to:\n\n- address methodological and institutional challenges\n- process and analyze different data inputs\n- share best practices\n- promote experimentation and co-creation of information products\n\n\n\n"
    },
    {
        "level": 3,
        "id": "wiYBU1Oc",
        "title": "Responsibilities of the project coordinator in a cross-functional team",
        "text": "- Create and track against a project plan\n- Drive team members towards the completion of milestones and project deliverables\n- Budget project resources and expenditures\n- Manage risks\n- Find ways to prevent time-sharing overload of team members between the project and their regular functional duties.\n\n"
    },
    {
        "level": 3,
        "id": "b2eiNqSx",
        "title": "Scrum values",
        "text": "- **Honesty**: Adherence to the facts, refusing to deceive others or oneself in any way. \n\n>*\"Agile teams only agree to take on tasks they believe they can complete, so they are careful not to overcommit\" and \"are also honest when they need help\".*\n\n- **Openness**: Willingness to engage with others and to hear and consider different opinions. \n\n>*\"When team members aren\u2019t sure how work is going, they ask\" and they \"consistently seek out new ideas and opportunities to learn.\"*\n\n- **Courage**: Self-confidence and moral strength to relentlessly expose any type of organizational dysfunction, waste and to learn from failure. \n\n>*\"Scrum teams must feel safe enough to say no, to ask for help, and to try new things. Agile teams must be brave enough to question the status quo when it hampers their ability to succeed.\"*\n\n- **Respect**: Recognition and appreciation of the intrinsic worth of the opinions and contributions of every team member and every stakeholder. \n\n>*Scrum team members know that \"everyone has a distinct contribution to make toward completing the work of the sprint. They respect each other\u2019s ideas, give each other permission to have a bad day once in a while, and recognize each other\u2019s accomplishments.\"*\n\n- **Focus**:  Continuous effort to avoid low-value distractions and direct everyone's attention and energy to what matters most.  \n\n>*Scrum teams finish whatever they start and \"are relentless about limiting the amount of work in process\". *\n\n- **Trust**: Reliance on the good faith, truthfulness, knowledge and skills of each other. \n\n>*\"Scrum and agile teams trust each other to follow through on what they say they are going to do.\"*\n\n- **Empowerment**: Ability of self-organizing teams to ask and answer their own questions and to define for themselves how to do the work necessary to meet project goals.\n\n- **Collaboration**: Capacity of team members to effectively cooperate and assist each other in achieving a common goal. \n\n>*\"Scrum teams work together as a unit.\"*\n\n\n\n"
    },
    {
        "level": 3,
        "id": "TBRkKxJ7",
        "title": "Paralysis by analysis",
        "text": "\"When the fear of either making an error, or foregoing a superior solution, outweighs the realistic expectation or potential value of success in a decision made in a timely manner\"\n\n\"When overanalyzing or overthinking a situation prevents progress or decision-making\"\n\n\"The more numerous our options, the more difficult it becomes to choose a single one... and so we end up choosing none at all.\"(P. Bergman, 2010)\n\nParalysis by analysis is common when:\n- Dealing with complex systems and high uncertainty.\n- There is an overload of information from numerous sources \n- There is too little knowledge leads to lack of confidence on the part of team members\n- Vast knowledge and expertise increases the number of options and considerations that appear at every decision point. \n- Excessive focus is placed on perfection and completeness of the analysis phase.\n- The goals and timeframe of the analysis phase are not clearly defined and the expectations surrounding the deliverables are fuzzy.\n- Design and implementation issues are introduced into the analysis phase.\n\nWikipedia: \"In software development, analysis paralysis typically manifests itself through the Waterfall model with exceedingly long phases of project planning, requirements gathering, program design and data modeling, which can create little or no extra value by those steps and risk many revisions.\"\n\n\"Agile software development methodologies explicitly seek to prevent analysis paralysis, by promoting an iterative work cycle that emphasizes working products over product specifications,\"\n\n\n"
    },
    {
        "level": 2,
        "id": "PoFWMOPo",
        "title": "Agile approach",
        "text": "Agile is an empirical approach for value delivery through team collaboration, based on a series of values, principles, practices and tools.\n\n- Teams are cross-functional and self-organizing\n- Upfront planning is limited to what is absolutely necessary to get the work started\n- The starting point is a product backlog, and the team always works  in short, time-boxed iterations / increments, focusing first on the most important or highest-priority items\n- At the end of each iteration:\n  - The team delivers a potentially shippable product\n  - The team reviews the completed features with the stakeholders to get their feedback\n- Based on feedback, product owner and team can change priorities on what to work next and decide how to do the work\n\n\n"
    },
    {
        "level": 3,
        "id": "LzaHiJbM",
        "title": "Incremental development",
        "text": "Break down deliverables into small increments. Each increment covers the full development cycle (analysis/design/build/test/integrate) for a specific feature, allowing to obtain feedback, to validate assumptions and to adapt.  \"In scrum we don't work on a phase at a time; we work on a feature at a time.\"\n\n\n"
    },
    {
        "level": 3,
        "id": "4BQNmycG",
        "title": "Adaptability vs. up-front planning",
        "text": "Project teams need to balance the need for up-front planning (predictive work) against the need for adaptability.  \n\nUp-front planning should be helpful without being excessive.   Lack of any planning leads to a state of constant, chaotic change.  But too much predictive planning creates sunk costs that limit adaptability to new circumstances or new information.  (It is cheaper to make changes in plans sooner than later). \n\nRequirements, designs, test cases, etc should be produced \"just in time\", keeping all options open until the last responsible moment (when the cost of not making a decision exceeds the cost of making a decision). The idea is to avoid rushing to make decisions that are difficult to reverse, and instead gathering as much information as economically feasible while avoiding costly delays in delivering value to customers.  \n\nIn other words, the goal is to keep the cost-of-change curve flat for as long as possible by limiting the volume of started-but-not-yet-finished work.  Important decisions should not be delayed for ever, but if possible should be broken down into smaller decisions, some of which can wait until more information is available to validate any underlying assumptions.\n\n\n\n\n\n"
    },
    {
        "level": 3,
        "id": "ERfoyuMI",
        "title": "Being fast vs. being hurried",
        "text": "The outcome is enhanced with speed.  However, hurrying with the intent of getting things done substantially reduces effectiveness.  High-performance teams move swiftly, nimbly and deliberately while carving out enough time for inspect-and-adapt activities along the way. \n\nThe objective is to deliver small increments quickly, in order to obtain feedback fast and to get value into the hands of the customer sooner.  \n\nThe point of velocity in agility is the ability to learn and react quickly. \n\n"
    },
    {
        "level": 3,
        "id": "wXeI3ZuL",
        "title": "Reducing requirements uncertainty thorugh probing and exploration",
        "text": "At the beginning of any project, many requirements are specified without sufficient knowledge about the desired features of the final product, the means to achieve them, or even knowledge of who the final customer is (low-quality requirements). \n\nIt's impossible to get all the requirements right up front; and even if at the beginning of a project the requirements were perfectly specified, changes in the environment during execution can make them obsolete before value is delivered to the customer.\n\nAgile teams try to specify just enough requirements up front so they can start building working solutions that allow them to obtain early feedback from stakeholders to fill-in the details, validate assumptions and learn about any unknown unknowns. Moreover, in the face of too much uncertainty, agile teams may choose to \"learn by doing\", building a prototype, creating a proof-of-concept model, performing a study or conducting an experiment. \n\n"
    },
    {
        "level": 3,
        "id": "sYzTxdf1",
        "title": "Short feedback loops",
        "text": "Fast feedback closes the learning loop quickly and allows to truncate bad development paths before they can cause serious economic damage.  Errors compound when we delay feedback, resulting in exponentially larger failures. The longer we validate assumptions, the greater the number of risky dependencies built into the product.  On the other hand, fast feedback allows to quickly uncover and exploit time-sensitive opportunities. \n\n"
    },
    {
        "level": 3,
        "id": "uRu9vpcC",
        "title": "Scrum practices",
        "text": "Scrum practices include:\n\n1. Roles:\n    - **Product owner**:\n      - Executes product leadership, deciding what features and functionalities will be developed and in what order\n      - Maintains and communicates a clear vision of what the scrum team is trying to achieve\n      - Makes sure that the most valuable work is always performed first\n      - Is available to answer questions of the team as soon as they are posted\n    - **Scrum master**:\n      - Helps everyone understand and embrace scrum values, principles and practices\n      - Protects the team from outside interference\n      - Works to remove impediments that inhibit team productivity (when the team cannot reasonably resolve them)\n      - Acts as a coach and leader during change-management process\n      - Helps the team resolve issues and make improvements in their scrum\n      - Guides the team in creating and following its own process\n    - **Development team**:\n      - Is responsible for designing, building, and testing the desired product\n      - Self-organizes to determine the best way to deliver what the product owner has asked for\n\n2. Activities\n   - Product backlog grooming\n   - **Sprint planning**:\n      - At the beginning of each sprint, the product owner, the scrum master, and the development team must jointly determine the subset of product backlog items they believe can completed in the next sprint. They also agree on a sprint goal and commit to deliver a specific set of features, creating a sprint backlog. \"Sprint planning results in both a forecast and a commitment.\"  This commitment helps build trust between the product owner and the development team, supports short-term planning and decision making in the organization, and enables teams to make decisions based on what other teams have committed to.\n   - **Sprint execution**:\n      - The development team, coached by the scrum master, performs all the task-level work specified in the sprint backlog.  Through daily scrum meetings, the team members help mange the flow of work. \n   - **Sprint review**:\n      - Stakeholders and scrum team inspect the product being built in a scheduled meeting, where they inspect the product features just completed and discuss to guide and adapt forthcoming development efforts. It is also an opportunity to get a deeper appreciation of the business side needs and expectations.\n   - **Sprint retrospective**:\n      - After the sprint review, and before the next sprint planning meeting, the scrum team inspects the scrum process being used to create the product. As a result of the retrospective, the team identifies and commits to a practical number of process improvement actions for the next sprint.\n\n3. Artifacts\n   - **Product backlog**\n   - **Sprint backlog**: Features of the product backlog that should be delivered by the end of the current sprint, broken down into more granular tasks, each of which has a detailed description and an effort-hour estimate.  \n\n4. Rules\n  - As a general rule, no changes in scope or personnel are permitted during the sprint\n  - Sprints always have a fixed start and end date, and are generally of the same duration (in order to establish a cadence)\n\n\n"
    },
    {
        "level": 3,
        "id": "LQ7sc3RD",
        "title": "Definition of done",
        "text": "Baseline criteria to establish with confidence that the work completed is of good quality and potentially shippable\n\n- For example: \"A complete slice of functionality that is designed, built, integrated, tested and documented\"\n\n- In early, exploratory stages of a project, it could mean \"a slice of functionality that is sufficiently usable to generate actionable feedback\"\n\n"
    },
    {
        "level": 2,
        "id": "cPIa75Kg",
        "title": "How Spotify customized the use of Scrum",
        "text": "Spotify decided to make some of the standard scrum practices (e.g., sprint planning meeting, task breakdown, estimation, \u2026) optional.  They decided that \u201cAgile matters more than Scrum\u201d and that \u201cagile principles matter more than specific practices\u201d.  Some of the changes they introduced include:\n\n- Spotify renamed the role of \u201cscrum master\u201d to \u201cagile coach\u201d, emphasizing the need of servant-leaders rather than process masters, and instead of \u201cscrum teams\u201d they refer to their cross-functional, self-organizing teams as \u201cautonomous squads\u201d.  Each squad decides what to build, how to build it, and how to work together, within the limits of the squad\u2019s long-term mission, the product strategy, and short-term goals that are negotiated every quarter.  \n- Squads are loosely coupled but tightly aligned teams.  \u201cAlignment enables autonomy\u201d. The leader\u2019s job is to communicate what problem needs to be solved and why. The squad\u2019s job is to collaborate with each other to find the best solution. \n- Squads are grouped into \u201ctribes\u201d. Each person is simultaneously a member of a squad (product dimension), and a chapter (a competency area) that cuts across multiple squads in a tribe. \n- There are also \u201cguilds\u201d \u2013 or light-weight, company-wide communities of interest where people share knowledge on a specific area (leadership, web development, \u2026) through mailing lists, bi-annual conferences, and other informal communication methods. \n- Cross pollination rather than standardization: Instead of prescribing the use of particular tools, When enough squads use the same tool, that becomes a de-facto standard. This allows for a healthy balance between consistency and flexibility.\n- Internal open-source model. Anyone can edit any code. However, there is a culture of peer code review, which improves quality and promotes knowledge sharing.  \n- Culture of trust and mutual respect. Give credit to each other for great work, and seldom take credit for oneself. Agile at scale requires trust at scale, and that means no politics, and no fear. \n- Focus on speed: Small and frequent releases; investment in test automation and continuous delivery infrastructure. \n- Three types of squads:\n  - Feature squads: Focus on building features in one specific area (e.g., \u2018search\u2019), to be released across all platforms\n  - Client app squads: Focus on making all feature releases on one specific platform easy (e.g., desktop, iOS, Android)\n  - Infrastructure squads: Focus on providing tools and routines that make other squads more effective (e.g., continuous delivery, testing\u2026) \n\n\n"
    },
    {
        "level": 2,
        "id": "sqiecPWI",
        "title": "Daily scrum / daily standup",
        "text": "A daily, timeboxed (15-minutes-or-less) meeting of the development team members, facilitated by the scrum master, to discuss:\n\n- What did I accomplish since the last daily scrum?\n- What do I plan to work on?\n- What obstacles or impediments are preventing me from making progress?\n\nThis is a synchronization, inspection and adaptive planning activity; not a traditional status meeting, nor a problem-solving meeting.\n\n"
    },
    {
        "level": 2,
        "id": "b2eiNqSx",
        "title": "Scrum values",
        "text": "- **Honesty**: Adherence to the facts, refusing to deceive others or oneself in any way. \n\n>*\"Agile teams only agree to take on tasks they believe they can complete, so they are careful not to overcommit\" and \"are also honest when they need help\".*\n\n- **Openness**: Willingness to engage with others and to hear and consider different opinions. \n\n>*\"When team members aren\u2019t sure how work is going, they ask\" and they \"consistently seek out new ideas and opportunities to learn.\"*\n\n- **Courage**: Self-confidence and moral strength to relentlessly expose any type of organizational dysfunction, waste and to learn from failure. \n\n>*\"Scrum teams must feel safe enough to say no, to ask for help, and to try new things. Agile teams must be brave enough to question the status quo when it hampers their ability to succeed.\"*\n\n- **Respect**: Recognition and appreciation of the intrinsic worth of the opinions and contributions of every team member and every stakeholder. \n\n>*Scrum team members know that \"everyone has a distinct contribution to make toward completing the work of the sprint. They respect each other\u2019s ideas, give each other permission to have a bad day once in a while, and recognize each other\u2019s accomplishments.\"*\n\n- **Focus**:  Continuous effort to avoid low-value distractions and direct everyone's attention and energy to what matters most.  \n\n>*Scrum teams finish whatever they start and \"are relentless about limiting the amount of work in process\". *\n\n- **Trust**: Reliance on the good faith, truthfulness, knowledge and skills of each other. \n\n>*\"Scrum and agile teams trust each other to follow through on what they say they are going to do.\"*\n\n- **Empowerment**: Ability of self-organizing teams to ask and answer their own questions and to define for themselves how to do the work necessary to meet project goals.\n\n- **Collaboration**: Capacity of team members to effectively cooperate and assist each other in achieving a common goal. \n\n>*\"Scrum teams work together as a unit.\"*\n\n\n\n"
    },
    {
        "level": 3,
        "id": "1MfDXpXo",
        "title": "What makes a good team",
        "text": "Working in a teams where members jockey for the leadership position or criticize one another's ideas can often be a source of stress.  \n\nConflicts over who is in charge and who gets to represent the group may lead to teammates trying to show authority by speaking louder or talking over each other. \n\nWhat distinguishes \"good\" from \"dysfunctional\" teams is how teammates treat one another.   \"In the best teams, members listen to one another and show sensitivity to feelings and needs\"\n\n"
    },
    {
        "level": 3,
        "id": "W7T2p8gF",
        "title": "Embracing failure when it happens",
        "text": "People do not take risks for fear of failure. But taking risks is intrinsic to growth and innovation.  We should therefore become familiar with, and embrace, the feelings associated with taking risks, failing, admitting mistakes, and recovering from them, so fear of those feelings is not any more a source of paralysis. \n\n\"The faster you fail, the faster you can recover and learn, leading to successful products. (...) Do your best, learn from your mistakes, and be humble.\"\n\n"
    },
    {
        "level": 3,
        "id": "bpltunOR",
        "title": "Transparency in scrum",
        "text": "All relevant information must be available to the people involved in creating the product.  Transparency makes inspection possible, which is needed for adaptation.  Transparency also establishes trust. \n\n"
    },
    {
        "level": 3,
        "id": "dELQJ204",
        "title": "Psychological safety is key to building an effective team",
        "text": "Two behaviors generally observed in effective teams:\n\n1. **Equal distribution of conversational turn-taking\"**: Members speak in roughly the same proportion; everyone gets a chance to talk, instead of having one person or a small group speak all the time. \n\n2. **High social sensitivity\"**: Members are \"skilled at intuiting how others feel based on their tone of voice, their expressions and other nonverbal cues.\"\n\nBoth are aspects of \"**psychological safety**\"-- when team members share the belief that it is safe to speak up without fearing that the team will \"embarrass, reject or punish\" them.  \"A team climate characterized by **interpersonal trust and mutual respect** in which people are comfortable being themselves.\" (Prof. Amy  Edmondson, 1999).  \n\nHuman bonds are as important at work as anywhere else.  Most people spend the majority of their time working; if we are not able to be open and honest at work, we are not really living a fulfilling life. Psychological safety allows to have emotional conversations at work. \"No one wants to put on a 'work face' when they get to the office. (..) No one wants to leave part of their personality and inner life at home. (...) We must be able to talk about what is messy or sad, to have hard conversations with colleagues who are driving us crazy.\"\n\n\"Google's data indicated that psychological saftey, more than anything else, was critical to making a team work.\"\n\n\n\n"
    },
    {
        "level": 3,
        "id": "SRFvQdH5",
        "title": "Importance of organization's mission, vision and values in times of crisis",
        "text": "In times of crisis, the most valuable assets of an organization are its people and its reputation.  An organization cannot start communicating its mission, vision and values during a crisis.  Staff will know what to do only if they have already internalized the organization's guiding principles.  \n\nA strong culture enables an organization to maintain focus in a time of crisis.\n\n\"Our principles will never fail us as lon as we do not fail to live up to them\" (Henry Paulson, Goldman Sachs CEO)\n\n"
    },
    {
        "level": 3,
        "id": "57Y4WA5S",
        "title": "Common Statistical Business Architecture Principles",
        "text": "- **Holistic approach**: Ensure data, methods, technology and skills are consistent, re-usable and interoperable across multiple functions and business units in the organization.\n\n- **End result focus**: Make the envisioned output the reference starting point for the design of new statistical processes.\n\n- **Enterprise-level value**: Design and implement new or improved statistical business processes aiming to maximize value at enterprise level. Leverage all enterprise capabilities so that the end result is well integrated, measurable and operationally effective. \n\n- **Customer value**: Design statistical services that deliver value to end customers. \n\n- **Sustainability**: Make investments and plans aiming to maximize the long-term continuity, adaptability and growth of core statistical functions.\n\n- **Re-use before designing new**: Re-use all existing capabilities and products before designing new ones.\n\n- **Design new for re-use**: Design new capabilities and products to be re-used, making sure they can be easily standardized, assembled and adapted to accommodate changing demands.\n\n- **Maximization of use of existing data assets**: Leverage all currently existing data before collecting additional data; monitor and reduce respondents' burden over time.\n\n- **Standards**: Adopt open, industry-recognized and international standards where available.\n\n- **Metadata-driven processes**: Design business processes to ensure that their composition, operation and management are metadata-driven and automated as much as possible.\n\n- **Discoverability and accessibility**: Ensure that statistical services are easily discoverable and accessible to enable sharing and re-use.\n\n- ** \"Just-right\" granularity**: Define statistical services at the level of granularity that is most relevant to the business needs.\n\n- **Trust and security**: Conduct all business in a manner that builds trust and confidence in the statistical organization's decision making and practices and its ability to preserve the integrity, quality, security, and confidentiality of the data assets entrusted to it.\n\n- **Service-level agreements**: Clearly define performance expectations for every statistical service, laying out metrics and remedies should agreed-on service levels not be achieved.\n\n- **National and international partnerships**: Collaborate nationally and internationally to leverage and influence statistical and technological developments which support the sharing of statistical services.\n\n\n\n"
    },
    {
        "level": 2,
        "id": "uRu9vpcC",
        "title": "Scrum practices",
        "text": "Scrum practices include:\n\n1. Roles:\n    - **Product owner**:\n      - Executes product leadership, deciding what features and functionalities will be developed and in what order\n      - Maintains and communicates a clear vision of what the scrum team is trying to achieve\n      - Makes sure that the most valuable work is always performed first\n      - Is available to answer questions of the team as soon as they are posted\n    - **Scrum master**:\n      - Helps everyone understand and embrace scrum values, principles and practices\n      - Protects the team from outside interference\n      - Works to remove impediments that inhibit team productivity (when the team cannot reasonably resolve them)\n      - Acts as a coach and leader during change-management process\n      - Helps the team resolve issues and make improvements in their scrum\n      - Guides the team in creating and following its own process\n    - **Development team**:\n      - Is responsible for designing, building, and testing the desired product\n      - Self-organizes to determine the best way to deliver what the product owner has asked for\n\n2. Activities\n   - Product backlog grooming\n   - **Sprint planning**:\n      - At the beginning of each sprint, the product owner, the scrum master, and the development team must jointly determine the subset of product backlog items they believe can completed in the next sprint. They also agree on a sprint goal and commit to deliver a specific set of features, creating a sprint backlog. \"Sprint planning results in both a forecast and a commitment.\"  This commitment helps build trust between the product owner and the development team, supports short-term planning and decision making in the organization, and enables teams to make decisions based on what other teams have committed to.\n   - **Sprint execution**:\n      - The development team, coached by the scrum master, performs all the task-level work specified in the sprint backlog.  Through daily scrum meetings, the team members help mange the flow of work. \n   - **Sprint review**:\n      - Stakeholders and scrum team inspect the product being built in a scheduled meeting, where they inspect the product features just completed and discuss to guide and adapt forthcoming development efforts. It is also an opportunity to get a deeper appreciation of the business side needs and expectations.\n   - **Sprint retrospective**:\n      - After the sprint review, and before the next sprint planning meeting, the scrum team inspects the scrum process being used to create the product. As a result of the retrospective, the team identifies and commits to a practical number of process improvement actions for the next sprint.\n\n3. Artifacts\n   - **Product backlog**\n   - **Sprint backlog**: Features of the product backlog that should be delivered by the end of the current sprint, broken down into more granular tasks, each of which has a detailed description and an effort-hour estimate.  \n\n4. Rules\n  - As a general rule, no changes in scope or personnel are permitted during the sprint\n  - Sprints always have a fixed start and end date, and are generally of the same duration (in order to establish a cadence)\n\n\n"
    },
    {
        "level": 3,
        "id": "r6RqDLiO",
        "title": "Sprint",
        "text": "Time-boxed (weekly to monthly) iterations that create something of value to the customer. \n\nSprints have always fixed start and end dates, and are generally of the same duration.  \n\nNo goal-altering changes are permitted during a sprint\n\n"
    },
    {
        "level": 3,
        "id": "lu7UXT1g",
        "title": "Inspection and adaptation",
        "text": "It is important to inspect and adapt not only \"what\" we are building (product), but also \"how\" we are building it (process).  \n\n"
    },
    {
        "level": 3,
        "id": "DCfr2zpK",
        "title": "Estimating size of features for product backlog",
        "text": "Product owners need to know an item's cost to properly determine its priority.  \n\nScrum teams usually employ a relative size measure (e.g., story points or ideal days) to estimate the size of a product feature. \n\n"
    },
    {
        "level": 3,
        "id": "TlYLFIPL",
        "title": "Product Owner role",
        "text": "The product owner is responsible to manage the backlog of items that form the requirements and the shared understanding of the product's problem and solution.  \n\nThe product owner represents the customer, but does not only \"hand off\" requirements.  Instead, the product owner sets the direction for the development efforts, defines priorities, and shares responsibility with the development team for the success of the project. \n\n\n\n"
    },
    {
        "level": 1,
        "id": "Nwuk7TBb",
        "title": "Kanban",
        "text": "Kanban is an approach focused on measuring and visualizing the flow of work through the system in order to identify opportunities for continuous, gradual improvement.  It emphasizes the elimination of over-burden and the reduction of variability in workflows.  \n\nIn contrast to scrum, kanban is well suited to manage \"interrupt-driven\" work environments. The idea is to monitor the \"work in process\" (WIP) at each step, ensuring that teams are not doing more work than they have the capacity to do.  \n\n"
    },
    {
        "level": 2,
        "id": "6bvsiUUz",
        "title": "Importance of an overarching workflow",
        "text": "- It is crucial to maintain a \"holistic perspective\" so everything that needs to be taken care of is in one place and can be processed in a standardized way\n- Having a simple, overarching and streamlined workflow in place helps to stay in control by focusing on the important things and being able to pick up tasks quickly where they are left off.\n\n\n"
    },
    {
        "level": 3,
        "id": "nDxqu3EV",
        "title": "Lessons from the shipping container",
        "text": "Ahrens (2017) explains how the initial attempts to introduce the use of the shipping container --a very simple solution--failed as long as ship owners failed to change their infrastructure and routines and to recognize that what mattered was the entire transport chain, from packaging of goods at the point of production to their delivery at the final destination.\n\n> It wasn\u2019t just another way of shipping goods. It was a whole new way of doing business. \n>(Ahrens, 2017, p. 40) \n\nSimilarly, simple innovations in statistical production can only be mainstreamed if they are accompanied by necessary changes and adaptations along the whole data value chain.\n\nFor example... (?)\n\n"
    },
    {
        "level": 3,
        "id": "pS33XHDA",
        "title": "Knowledge management systems",
        "text": "- We need to compensate for the limitations of our brains by relying on external structures ('scafolding') to capture ideas and supports our thinking process.\n- Knowledge management systems help keep track of ever-increasing volume of information and relieve brain capacity to focus on what is important\n\n\n"
    },
    {
        "level": 3,
        "id": "MJyDsMeW",
        "title": "Interrupt-driven environments",
        "text": "In an \"interrupt-driven\" environment, teams are not able to reliably plan work beyond one week or so--it's not possible to know what the work will be that far into the future.  \n\nNew, critical requirements come in on a continuous basis, forcing the team to abandon early plans and to re-prioritize.  At any given moment there is a high probability or receiving a high-priority request that will force to re-assign resources that had already been committed to other tasks.\n\nWhen the content and order of the product backlog changes very frequently (even hourly or every few minutes), it is impossible to plan for iterations longer than a couple of days.\n\nIn this case, scrum may not be the best alternative.  Other agile tools, such as Kanban, may be more appropriate. \n\n\n"
    },
    {
        "level": 3,
        "id": "eac4gaf6",
        "title": "Why workflows become complicated",
        "text": "- Workflows become clogged over time as we try to apply a variety of new approaches and techniques, each promising to make something easier or better, but which combined have the opposite effects.\n- When new techniques are used without regard to the overarching workflow, \"nothing really fits together\", every little step suddenly becomes its own project, and it becomes very difficult to get things done.\n\n\n"
    },
    {
        "level": 3,
        "id": "XkfA6AFU",
        "title": "Exergonic vs endergonic workflows",
        "text": "Ahrens (2017) explains that workflows can be characterized as either \"exergonic\" (requiring constant addition of energy to keep them going) or \"endergonic\" (once triggered, they continue by themselves and even release energy).\n\nGood (endergonic) workflows turn into **virtuous cycles** where the experience of becoming better at what we do motivates us to take on the next task (Ahrens, 2017, p.53). \n\nSuch workflows need to include a **learning system** based on actionable **feedback loops**.\n\n"
    },
    {
        "level": 2,
        "id": "eac4gaf6",
        "title": "Why workflows become complicated",
        "text": "- Workflows become clogged over time as we try to apply a variety of new approaches and techniques, each promising to make something easier or better, but which combined have the opposite effects.\n- When new techniques are used without regard to the overarching workflow, \"nothing really fits together\", every little step suddenly becomes its own project, and it becomes very difficult to get things done.\n\n\n"
    },
    {
        "level": 3,
        "id": "JExDAHlM",
        "title": "Simplicity is paramount",
        "text": "- Big transformations start with simple ideas.  What matters is how well these simple ideas fit in the overall workflows of a system or organization.\n- To avoid undesired side effects, it is important to focus on small units of work.\n\n\n"
    },
    {
        "level": 3,
        "id": "oqMxwKBR",
        "title": "Cynefin Framework",
        "text": "A framework that classify operations into 5 main types of environments:\n\n- **Chaotic**:\n  - *Strategy*: **Act -> Sense -> Respond**\n  - *What is required*: Leadership and immediate action to quickly identify a practical solution, instead of trying to find the best solution.  \n  - *Objective*: To stabilize the situation, re-establishing order and migrating to the \"complex\" domain as soon as possible\n  - *Tools*: Kanban\n\n- **Complex**:\n  - *Strategy*: **Probe -> Sense -> Respond**\n  - *What is required*: Creative / innovative approaches; a fail-safe environment for experimentation; intense interaction and communication\n  - *Objective*: Discovering patterns and documenting emerging practices\n  - *Tools*: Scrum, Kanban\n\n- **Complicated**:\n  - * Strategy*: **Sense -> Analyze -> Respond**\n  - *What is required*: Investigating several options, applying good practices, using metrics and relying on experts\n  - *Objective*: Gain insight and control; discover cause-effect relations\n  - *Tools*: Six sigma\n\n- **Simple**:\n  - *Strategy*: **Sense -> Categorize -> Respond**\n  - *What is required*: Assessing and categorizing the situation based on known facts; applying best practices\n  - *Objective*: Implementing the optimal solution\n  - *Tools*: Off-the-shelf solutions\n\n- **Disorder**: Don't know in what environment we find ourselves\n\n"
    },
    {
        "level": 2,
        "id": "MJyDsMeW",
        "title": "Interrupt-driven environments",
        "text": "In an \"interrupt-driven\" environment, teams are not able to reliably plan work beyond one week or so--it's not possible to know what the work will be that far into the future.  \n\nNew, critical requirements come in on a continuous basis, forcing the team to abandon early plans and to re-prioritize.  At any given moment there is a high probability or receiving a high-priority request that will force to re-assign resources that had already been committed to other tasks.\n\nWhen the content and order of the product backlog changes very frequently (even hourly or every few minutes), it is impossible to plan for iterations longer than a couple of days.\n\nIn this case, scrum may not be the best alternative.  Other agile tools, such as Kanban, may be more appropriate. \n\n\n"
    },
    {
        "level": 3,
        "id": "QyoZwcP9",
        "title": "Managing own's time and effort",
        "text": "\"It is no fun to find ourselves with too little time and too many deliverable, rushing headlong into panic mode while the hours ebb dep into the night.\" (Goldfedder 2020)\n\nTrying to manage time is a fools errand when it is objectively impossible to fit everything into the limited amount of time that is available in a day, no matter how efficient one is.  \n\nTime is a limited resource and should be allocated in alignment of one's own personal definition of success.\n\nRecognize and excel in what really counts, and aim for less than perfect in everything else.  \n\n- Decline invites to tactical meetings \n- Reduce your involvement in committees\n- Focus on strengths instead of trying to shore up your weaknesses\n\n"
    },
    {
        "level": 3,
        "id": "2uCsbhU6",
        "title": "Results of bad planning",
        "text": "\"Typical results of bad planning are missed deadlines, unrecognized overhead, and, more often than not, ... desperate acts of last-minute heroism that take on a weirdly optimistic tone (\"I did it and with only three ours of sleep in the last two days!\") \"\n\n\n\n"
    },
    {
        "level": 1,
        "id": "x97FBvWU",
        "title": "Managing a crisis communication programme",
        "text": "Managing a crisis communications program requires the same level of dedication and resources typically given to other functions. \n\nIn times of extreme crisis, internal communications take precedence. Key priorities include reestablishing communication with any groups of staff that are stranded or isolated, and rebuilding staff morale.\n\n"
    },
    {
        "level": 2,
        "id": "YJqwphec",
        "title": "Critical communication infrastructure",
        "text": "Heads of NSOs need to be able to communicate easily with their staff during the crisis.  Staff needs to have adequate connectivity, equipment and infrastructure, not only to be able to work from home, but also to stay in touch with colleagues and stay appraised of current developments. \n\n"
    },
    {
        "level": 3,
        "id": "BQwQetnc",
        "title": "Business continuity of statistical organizations",
        "text": "The current COVID-19 crisis is affecting critical operations of across the entire global statistical system, and national and international statistical organizations need to urgently develop and implement action plans to ensure the continuity of key statistical compilation activities and the continued availability of data to inform emergency mitigation actions by governments and all sectors of society. \n\nSenior management in statistical organizations need to define guidelines, in consultation with front-line managers and IT teams, to deal with the contingency.  This includes establishing new procedures and workflows on issues like:\n\n- **Management of virtual teams (task tracking and performance management)**: Can leaders of NSOs communicate easily with their staff? Is there a centralized location from which critical information is accessible to staff?\n- **Secure remote data access and data exchange**: Can staff stay connected through internet from home?  Are critical data safely stored and securely accessible?\n- **Responding to users' most urgent needs**: Can users reach service and response teams through dedicated phone lines and email address? Are  specific instructions on how to obtain immediate assistance available through social media and the organization's website? \n\n\n\n"
    },
    {
        "level": 3,
        "id": "hGt3as0b",
        "title": "Digital collaboration tools",
        "text": "Modern online collaboration tools are increasingly important for virtual and cross-functional teams to achieve greater levels of transparency, optimize resources and deliver results.\n\nThe selection of collaboration tools should be based on business needs and the budget available to the project or organization.\n\nTypical requirements include:\n\n- Team communication:\n  - Chat / instant messaging\n  - Screen sharing\n  - Audio/video conferencing\n  - Discussion forums\n- File and data sharing\n- Project management\n  - Planning\n  - Budget management\n  - Procurement management\n  - Contact management\n  - Calendar management\n  - Note taking\n  - Monitoring and evaluation\n- Code development / versioning\n- Content creation and management\n- Workflow management / automation\n\n"
    },
    {
        "level": 3,
        "id": "xUr89DOT",
        "title": "Priority objectives of COVID-19 contingency plans for statistical programmes",
        "text": "Priority areas are:\n\n- Maintain adequate coverage of the target population\n- Ensure high questionnaire- and item-response\n- Guarantee internal consistency, comparability, and overall quality of the data collected\n- Maintain timely data collection, processing and dissemination\n- Minimize response burden on information providers\n- Use resources efficiently / minimize cost of statistical operations in the new environment\n\n"
    },
    {
        "level": 3,
        "id": "bay5lBgW",
        "title": "Need for crisis management skills",
        "text": "The current situation demands strong crisis management skills and expertise, and agility in planning, designing and implementing innovative approaches to carry out critical statistical operations. \n\n\n"
    },
    {
        "level": 2,
        "id": "bay5lBgW",
        "title": "Need for crisis management skills",
        "text": "The current situation demands strong crisis management skills and expertise, and agility in planning, designing and implementing innovative approaches to carry out critical statistical operations. \n\n\n"
    },
    {
        "level": 3,
        "id": "4kDQlxl8",
        "title": "Leading the response of NSOs to the COVID-19 pandemic",
        "text": "The worldwide spread of COVID-19 is having vast, long-term consequences for people, organizations, economies and society at large, which call for a new kind of leadership.  \n\nLeaders from governments and organizations at all levels must not only make decisions to keep people safe and ensure continuity of operations in the middle of the crisis. They must also keep an eye on the long-term recovery and look for opportunities to innovate, create growth and \"build back better\".\n\nEven after the health crisis is over, things will not \"return to normal\", and national and international statistical organizations will have to adapt to a new reality, characterized by new demands from users and lasting changes in the configuration of the entire data value chain. \n\nIn responding to the crisis, heads of national statistical offices cannot default to known approaches, as the unprecedented nature and scale of the challenge surpasses anyone's past experience.  NSO staff at all levels need to quickly adapt and experiment with \"next practices\" in their day-to-day statistical operations, acknowledging that many of the traditional \"best practices\" are no longer relevant. \n\nMoreover, any success in the initial response by NSOs should not be followed by complacency or the illusion of a return to normalcy. Instead, statistical organizations must continue focusing on innovation and sustain their efforts to understand and adaptation to the new reality in the global data ecosystem. \n\nIn this context, heads of statistical organizations must identify the principles and practices that must be preserved in the compilation of official statistics, as well as those that must be abandoned or modified in order to move forward.\n\n\n"
    },
    {
        "level": 3,
        "id": "84OVqhJD",
        "title": "Leading in times of trauma",
        "text": "In times of trauma, leaders of an organization can help staff channel their desire to help and get back into their normal routine by fostering their pride in the organization and what they do, and providing context for grieving, meaning and action. \n\n1. *Context for grieving*: Create an environment where people can freely express and discuss the way they feel, and seek or provide comfort\n\n2.  *Context for meaning*:  Communicate and reinforce organizational values, reminding people about the larger purpose of their work and helping make sense of the pain\n\n3. *Context for action*: Create an environment where those who experience or witness pain can imagine a more hopeful future and find ways to alleviate their own and other's suffering.\n\n\n\n"
    },
    {
        "level": 3,
        "id": "5dUHywzA",
        "title": "Adapting infrastructure and operations of statistical organizations in times of crisis",
        "text": "National and international statistical organizations must adapt to the uncertainties of a new reality characterized by health emergencies, environmental crises, economic recession and political instability. \n\nFor instance, the central information systems of National Statistical Offices need to adapt quickly to effectively manage and monitor statistical operations in the context of the COVID-19 crisis, such as staff recruitment, training, data collection logistics, and supervision and gathering of operational intelligence. \n\n\n\n"
    },
    {
        "level": 3,
        "id": "gxrbaZc1",
        "title": "Small, bottom-up initiatives in response to COVID-19",
        "text": "Small, bottom-up initiatives can play an important role in a statistical organization's response to the disruptions created by the COVID-19 pandemic.   Instead of attempting to develop and implement grand and detailed plans, in a crisis like this it is better to learn quickly and often from the successes and failures of many small-scale experiments.\n\nAdaptability usually comes \"from the accumulation of microadaptations originating throught [the organization] in response to its many microenvironments\".\n\nIn times of crisis, information sharing, listening and learning must take precedence over hierarchy and formal authority, so as to be able to draw from the collective knowledge and skills across diverse functions and locations to generate solutions, \n\n"
    },
    {
        "level": 3,
        "id": "lCHnOEAE",
        "title": "Managing projects that are at risk of failure",
        "text": "Projects at risk of failing are actually very common in most organizations, but most of them recover when action is taken.  \n\n**Why projects become \"at risk\"**\n\n- Requirements (poorly defined, not prioritized, contradictory or not agreed upon)\n- Resources (not available, tied up in conflicts, or not properly planned for)\n- Schedules (unrealistic).\n- Planning (based on incomplete data, bad estimates, and insufficiently detailed).\n- Risks (not properly identified or measured)\n\n**Critical determinants of project recovery**\n\nThe project manager has a large impact on whether a project at risk of failing ultimately succeeds or not, since she or he decides which root causes to address and which actions to undertake.\n\nHaving a standardized project management methodology significantly reduces the likelihood of project failure in an organization. \n\n**Most frequent interventions that lead to successful recovery**\n\n- Improve communication\n- Redefine the scope or business case of the project\n- Right-size resources (add if too little, reduce is too many resources had been allocated)\n- Address technical issues trough in-depth analysis\n- Shift talent (e.g. replace project manager or bring in a consultant)\n\n**Steps to recovery**\n\n1. Review the project's history, purpose, goals, assumptions, and team composition\n2. Facilitate team communication about issues and root causes (without finger pointing)\n3. Address incentives and verify that stakeholders are still vested in the project\n4. Assess tradeoffs between time, cost and scope, subject to reputation, quality and value constraints\n5. Obtain buy in from stakeholders into the proposed solution\n6. Restart the project by communicating learnings from past mistakes, updated plan, roles and responsibilities\n\n\n"
    },
    {
        "level": 1,
        "id": "r7WsYDhx",
        "title": "Individual vs team productivity",
        "text": "How can we become \"faster, better, and more productive versions of ourselves\"?  We have today a much better understanding of what drives personal and team productivity. \n\nAnalyzing and improving individual productivity (\"employee performance optimization\") is not enough.   Organizations need to look not only at how people work, but also how they work together. \"Teams are now the fundamental unit of organization\" \n\nWhen people collaborate effectively in teams, they:\n\n- Innovate faster\n- See mistakes more quickly\n- Find better solutions to problems\n- Achieve higher job satisfaction\n- Increase profitability\n\n\n\n"
    },
    {
        "level": 2,
        "id": "rR6VySYD",
        "title": "Virtual teams",
        "text": "A virtual team is composed of members from different cultures and languages\nwho working remotely from one another.  They require intensive use of communication technology and virtual collaboration tools. \n\nThe early stages of a project are particularly challenging for virtual team, as the gathering of requirements often requires face-to-face conversations where body language and nuanced interpretation of voiced guidance are particularly important.   For instance, what would usually take a few minutes to cover in a face-to-face meeting, could take hours to explain in a series of emails or video conversations.  \n\nDelays in having questions answered may lead to a growing backlog.\n\n"
    },
    {
        "level": 3,
        "id": "7dELQecN",
        "title": "Updating telecommuting policy",
        "text": "In many organizations, existing telecommuting policies were written under the assumption that telecommuting was offered only to certain employees under special circumstances.  \n\nIn the face of the COVID-19 crisis, telecommuting will be the most common work arrangement for many organizations over the foreseeable future. \n\nThere is an urgent need to review and adapt existing telecommunication policies so organizations can nimbly adjust to the new situation. \n\n- Telecommuting as the rule and not the exception\n- Less cumbersome process for formal agreements with staff regarding use of corporate infrastructure and services\n\n"
    },
    {
        "level": 3,
        "id": "lh9J280B",
        "title": "Bandwidth requirements for telecommuting",
        "text": "Voice and video-conferencing are essential tools for effective telecommuting. However they require a minimum level of bandwidth that is not always present.\n\n\n"
    },
    {
        "level": 3,
        "id": "kfu8nyho",
        "title": "COVID-19 - Sudden spike in need for telecommuting",
        "text": "To limit the COVID-19 epidemic, organizations are requiring all or most of their staff work from home. This has created a huge challenge of having to manage \"a very large and sudden spike\" in the number of remote workers, even for organizations that already support certain number of telecommuters. National Statistical Offices are facing the prospect of a protracted telecommuting crisis.\n\n> \"Until now, telecommuting has been voluntary or even a reward of sorts. Not it's mandatory...\"  (Rist, 2011)\n\nSuch challenges include:\n- Meeting increased demand for IT help-desk support\n- Enabling staff to assume additional responsibilities regarding device and data security\n- Migrating on-premises workloads to cloud services\n- Providing remote access and remote management solutions for applications that need to remain served form on-premises servers\n- Adapting to remote performance tracking and virtual team collaboration\n\n"
    },
    {
        "level": 3,
        "id": "yHIpNa5l",
        "title": "Online collaboration tools",
        "text": "As remote working becomes the rule rather than the exception, modern online collaboration tools are increasingly important for teams to achieve greater levels of transparency, optimize resources and deliver results.\n\nThe selection of collaboration tools should be based on the organization's business needs and budget.\n\nMost online collaboration tools support more than one means of communication:\n- Chat / instant messaging\n- Email\n- Screen sharing\n- Audio/video conferencing\n\n\n\n"
    },
    {
        "level": 2,
        "id": "JXu4aYg9",
        "title": "Impact of COVID-19 lockdown on knowledge workers' productivity",
        "text": "Due to the lockdown, day-to-day schedules of knowledge workers have changed and teams have developed new ways of working. \n\nThe results of a recent survey (Birkinshaw, Cohen and Stach, 2020) found that the COVID-19 lockdown has helped knowledge workers devote more time to interacting with customers and external partners, while reducing time spent on large meetings.  The survey also shows that, during lockdown, knowledge workers are being more intrinsically motivated and taking more personal ownership of their work.\n\nOn the negative side, working remotely makes it more difficult to kick off new projects, resolve internal conflicts, and focus on long-term staff development. \"While time spent on self-education went up...\", e.g., through webinars and online courses, this type of learning is does not encourage the same level of active experimentation and personal reflection as face-to-face interactions. \n\n"
    },
    {
        "level": 3,
        "id": "88RTR3eB",
        "title": "Business continuity teams",
        "text": "The development and implementation of a business continuity plan requires the establishment of a cross-functional team, composed of:\n\n- Senior managemers\n- Front-line managers\n- Information Technology department\n- Legal department\n\n"
    },
    {
        "level": 3,
        "id": "sO5SxY53",
        "title": "Knowledge workers",
        "text": "\"Knowledge workers apply subjective judgment to tasks, they decide what to do when, and they can withhold effort ... often without anyone noticing\"\n\nKnowledge workers should be evaluated on their outputs, instead of their inputs.\n\n"
    },
    {
        "level": 1,
        "id": "8B67V92I",
        "title": "Africa's Programme on Accelerated Improvement of CRVS",
        "text": "Africa's Programme on Accelerated Improvement of Civil Registration and Vital Statistics was created under the directive of African Ministers Responsible for Civil Registration in 2010.  \n\nIts secretariat is based at UN ECA\n\n"
    },
    {
        "level": 2,
        "id": "Yx0ZUlq2",
        "title": "Decade for repositioninig CRVS in Africa",
        "text": "2017-2026 has been designated as the \"decade for repositioning CRVS in Africa\" by the Executive Councl of the African Union in Kigali. \n\n"
    },
    {
        "level": 3,
        "id": "1i1bh7AA",
        "title": "CRVS systems as a development imperative",
        "text": "There has been significant progress in recognizing CRVS systems as a development imperative\n\n"
    },
    {
        "level": 3,
        "id": "66kUhExT",
        "title": "Importance of CRVS Systems",
        "text": "CRVS Systems are crucially important to:\n- Build a modern public administration\n- Uphold human rights\n- Support national development initiatives\n- Improve service delivery to all people\n\n"
    },
    {
        "level": 2,
        "id": "1i1bh7AA",
        "title": "CRVS systems as a development imperative",
        "text": "There has been significant progress in recognizing CRVS systems as a development imperative\n\n"
    },
    {
        "level": 3,
        "id": "Gjve1WgL",
        "title": "CRVS systems and the 2030 Agenda",
        "text": "The 2030 Agenda for Sustainable Development's pleadge to leave no one behind means that no one should remain invisible.  Target 16.9 reads: \"By 2030, provide legal identity for all, including birth registration\".   \n\nHowever, many developing countries still do not have a comprehensive and complete CRVS system aligned with international standards.\n\n"
    },
    {
        "level": 2,
        "id": "66kUhExT",
        "title": "Importance of CRVS Systems",
        "text": "CRVS Systems are crucially important to:\n- Build a modern public administration\n- Uphold human rights\n- Support national development initiatives\n- Improve service delivery to all people\n\n"
    },
    {
        "level": 3,
        "id": "AMu8kLNJ",
        "title": "How does a good CRVS system look like?",
        "text": "- Universal\n- Continuing / permanent\n- Compulsory\n- Confidential\n- Every vital event (but primarily birth and death) is registered upon occurrence\n- Vital statistics are produced and used to guide policy\n\n\n"
    },
    {
        "level": 1,
        "id": "6WFYziDD",
        "title": "Comparability across data sources",
        "text": "The comparability of variables measured across different data sources is a key data quality issue.  \n\nChanges over time that may adversely impact comparability of variables across different data sets include:\n\n- Changes in the definition and coverage of reference geographic areas (e.g., creation or re-drawing of boundaries of new administrative units)\n- Changes in statistical classifications\n- Changes in sampling methodology\n- Changes in definition of reference time periods (e.g., calendar vs. fiscal year)\n- Changes in survey questions or measurement instruments\n\n"
    },
    {
        "level": 2,
        "id": "tltgpLsL",
        "title": "Levels of disaggregation of administrative units",
        "text": "The hierarchy of political divisions of a country's territory is comprised by administrative units, each of which is delineated by specific geographic boundaries.\n\n- The country is the highest-level administrative unit, and it is referred to as the \"**Level 0**\" administrative unit.  \n- The first level of subdivision of administrative units within a country is referred to as '**Level 1**\".\n- Administrative units at further levels of geographic disaggregation are denoted as \"**Level 2**\", \"**Level 3**\", etc.\n\n\n\n"
    },
    {
        "level": 3,
        "id": "etUj8mX6",
        "title": "Map visualizations",
        "text": "Map visualizations allow to:\n- Identify outliers in the spatial distribution of individual variables\n- Identify and highlight spatial correlations across multiple datasets.  \n- Identify and highlight spatial patterns of inequality at the subnational level (e.g., across districts, municipalities, and communities)\n\n"
    },
    {
        "level": 3,
        "id": "qpy6M6vh",
        "title": "Geo-spatial disaggregation",
        "text": "The geographic scale at which data are disaggregated affects the type of information that can be derived from it.  \n\nData collected at more granular level of geographic detail often provide more useful information about patterns of geographic variation and correlation, and can be more flexibly aggregated into broader geographic areas suitable for different types of analysis.\n\nExamples of geographic scales:\n\n- Administrative unit (level 0, level 1, level 2...)\n- Populated place\n- Point location\n- Grid location\n- Area/Line features\n\nDefining a minimum level of geospatial disaggregation facilitates data integration and analysis.  Such minimum level of geospatial disaggregation should be established considering factors such as:\n\n- **Analytic objectives** (e.g., informing policies or decisions at national or local levels)\n- **Country context** (e.g., how large are the different levels of administrative units)\n- Expected **variability** within and across different units at each level of disaggregation (e.g., too much variability within units / too little variability between units may require more granular disaggregation)\n- Type of **data collection process** (e.g., earth observation, on-site measurements, or administrative/business records). \n- Potential **privacy** implications (e.g., whether combining data at a specific level of geographic disaggregation with other datasets could lead to re-identification of personal or individual-level information)\n- **Resources** (e.g., expertise and technology required to collect, process, and analyze data at a specific level of geographic desaggregation).\n\n"
    },
    {
        "level": 3,
        "id": "cTjLfkWN",
        "title": "How to improve interoperability across data sources",
        "text": "To make various data sources ready-to-use, it is necessary to transform them to ensure they are interoperable. This includes:\n\n- Use of canonical data models\n- Geo-reference all data inputs using common boundaries and use consistent location-identification codes \n- Use common vocabularies, classifications and code lists\n- Develop of standardized API documentation\n\n\n"
    },
    {
        "level": 3,
        "id": "pXxz1MHR",
        "title": "Geo-referencing",
        "text": "Geo-referencing makes it relatively easy to bring together, overlay and analyze information from multiple sources based on different units of analysis. \n\n"
    },
    {
        "level": 3,
        "id": "ttc01lbK",
        "title": "Defining data disaggregation priorities",
        "text": "The definition of national data priorities include the definition of priority dimensions for which disaggregated data needs to be available:\n\n- Key population groups (e.g., women, urban/rural population, indigenous groups, youth, elderly population, people living with disabilities, ...)\n- Required level of granularity in geographic disaggregation\n\n"
    },
    {
        "level": 2,
        "id": "cTjLfkWN",
        "title": "How to improve interoperability across data sources",
        "text": "To make various data sources ready-to-use, it is necessary to transform them to ensure they are interoperable. This includes:\n\n- Use of canonical data models\n- Geo-reference all data inputs using common boundaries and use consistent location-identification codes \n- Use common vocabularies, classifications and code lists\n- Develop of standardized API documentation\n\n\n"
    },
    {
        "level": 3,
        "id": "OyHGHNbK",
        "title": "Open access and interoperability",
        "text": "Providing open access to a dataset or a document is more than just allowing users to obtain a physical or electronic copy of it for their perusal; it requires that their contents be described and structured in a standardized manner, broken down into meaningful elements that can be read and correctly interpreted by different people and by multiple software applications.\n\n"
    },
    {
        "level": 3,
        "id": "OkCaKf6I",
        "title": "SDMX is the shipping container of official statistics",
        "text": "Paraphrasing https://trello.com/c/RnpNCnXf, \"\\[SDMX\\] is the shipping container of the \\[official statistics\\] world.  Instead of having different \\[mechanisms for the exchange of\\] different \\[datasets\\], everything goes into the same \\[multi-dimensional schema\\] and is standardized into the same format. (...) Everything is streamlined towards one thing only: \\[statistical data\\] that can be \\[easily exchanged with, and utilized by, users\\].\n\n"
    },
    {
        "level": 3,
        "id": "MUnZi5qo",
        "title": "Unique identifiers best practices",
        "text": "Identifiers are part of the basic data infrastructure of a country or organization. \nThey help structure and link data together.  They are *boundary objects*, i.e., information objects used in different ways by different communities.  Therefore, they enable cross-disciplinary work and collaboration. \n\nTo be useful, they need to be well documented. \n\nUnique identifiers should be:\n\n- unique\n- valid over the entire lifetime of entities\n- issued from a central authority\n- stored in all relevant systems / databases\n- **never** re-issued\n- assigned to all entities\n\n"
    },
    {
        "level": 3,
        "id": "Lr60zbkO",
        "title": "Understanding multiple sources of data",
        "text": "No single individual (or organization) is ever familiar with all the attributes and caveats of all the data inputs that are required in a data innovation project.   Therefore, it is crucial to involve from the beginning all relevant experts who have helped produced various data inputs.\n\n"
    },
    {
        "level": 2,
        "id": "Lr60zbkO",
        "title": "Understanding multiple sources of data",
        "text": "No single individual (or organization) is ever familiar with all the attributes and caveats of all the data inputs that are required in a data innovation project.   Therefore, it is crucial to involve from the beginning all relevant experts who have helped produced various data inputs.\n\n"
    },
    {
        "level": 3,
        "id": "bR2Woaf4",
        "title": "Types of source data required for nowcasting",
        "text": "Nowcasting is about producing \"near real-time\" estimates.  This means being able to project in over time the values of variables measured in the past.\n\nThis in turn requires to leverage any \"panel components\" in input data form household surveys, administrative records, etc., which track the same individuals, households or statistical units with repeated measurement of the same variable(s) at different moments in time.\n\n"
    },
    {
        "level": 3,
        "id": "nqqQlCkb",
        "title": "Poverty maps production: data sources",
        "text": "The production of small-area poverty estimates usually relies on the following two major data sources of household welfare:\n\n1. Detailed household surveys which collect a measure of welfare (typically consumption per capita)\n2. A national census or large national survey that includes a significant share of the country's population\n\nOther data sources that may be used to approximate individual welfare (in approximately real time):\n\n- Individual consumption of mobile phone services\n- Measures of mobility (e.g., derived from mobile phone records)\n- Social network metrics (e.g., derived from social media or mobile phone records)\n- Financial transactions (e.g., derived from mobile phone or credit card records)\n\n"
    },
    {
        "level": 1,
        "id": "2GTwGKsm",
        "title": "ETL Frameworks and tools",
        "text": "Developing and implementing ETL (extract-tranform-load) processes is \"time-consuming, brittle, and often unrewarding\" (Beauchemin, 2018)\n\nThe data flows of real-life statistical production processes can be very complex, and ETL frameworks and tools help address common problems in building ETL pipelines:\n\n- Documentation - Succinctly describe the data flow\n- Automation/Scheduling\n- Monitoring - Track the progress of long running processes and alert when errors arise\n- Backfilling - Ability to re-process historical data\n\nETL Frameworks are often implemented in Python (e.g., Airflow and Luigi).  Examples include:\n\n  - **Google's Cloud Composer**: Python-based workflow orchestration service to connect data, processing, and services between on-premises and the public cloud. Built on Apache Airflow. Pipelines are configured as directed acyclic graphs (DAGs). Includes library of connectors and multiple graphical representations of workflows.  Allows to set up a continuous integration/continuous deployment (CI/CD) pipeline for processing data on Google Cloud.\n\n  - **Google's Cloud Dataflow**: Unified, serverless stream and batch data processing service.  Allows for flexible resource scheduling (FlexRS).\n\nThere are plenty of \"drag and drop\" data pipeline and workflow automation tools that do not require knowledge of the underlying code.:\n\n- **Informatica** - Offers a portfolio of data integration products as well as tools for master data management, data quality, data cataloging, and API management. \n\n- **SQL Server Integration Services (SSIS)** - Microsoft tool for data integration tied to SQL Server.\n\n- **Stitch** - cloud-based ETL platform \n\n- **Google's Cloud Data Fusion**: Cloud-native, scalable data integration service with a visual point-and-click interface, enabling code-free deployment of ETL/ELT data pipelines.  Library of 150+ preconfigured connectors and transformations, plus ability to create custom connections and transformations that can be validated, shared, and reused across teams.\n\n"
    },
    {
        "level": 2,
        "id": "2fx14QVS",
        "title": "Representing ETL jobs as Directed Acyclic Graphs (DAG)",
        "text": "It is often useful to visualize complex ETL data flows using a directed acyclic graph, where each node corresponds to a task (which only needs to be performed once), and arrows represent dependencies between tasks. \n\n"
    },
    {
        "level": 3,
        "id": "zNXU0RHP",
        "title": "Directed graphs (digraphs)",
        "text": "Directed graphs have at least one directed edge \"parent of\". \n\n"
    },
    {
        "level": 3,
        "id": "kvpUkOPt",
        "title": "Graph",
        "text": "In graph theory, a graph is a set of discrete objects (nodes) possibly joined to each other in binary or reflexive relationships (edges).   It is a relational form of organizing and representing discrete data, where nodes and edges can be decorated with additional properties known as \"attributes\".\n\nA binary relationship represents a relationship between two nodes.  A reflexive relationship represents a relationship of a node with itself.\n\nEdges can be thought of as representing pairs of connected nodes (dyads). \n\nGraphs can be used to describe networks (systems of interconnected objects).  \n\n"
    },
    {
        "level": 3,
        "id": "t4mv3eQD",
        "title": "Network analysis: Undirected graphs",
        "text": "Edges can be traversed in either direction (an edge from A to B is the same as an edge from B to A).\n\nCan have self-loops (reflexive relationship)\n\n\n"
    },
    {
        "level": 1,
        "id": "4lWg4cUa",
        "title": "Business Architecture: Functions, Processes and Services",
        "text": "A **business function** is something that an enterprise does, or needs to do, to achieve its objectives. \n\nA **business process** is a sequence of steps required to perform one or more business functions.  Each step may be a sub-processes consisting of various steps. \n\nA **business service** provides a means for accessing a business function by performing one or more business processes.  It has a defined interface that delivers a specific output given a particular set of inputs.    \n\n\n\n"
    },
    {
        "level": 2,
        "id": "spNOWLtX",
        "title": "Service-oriented architecture (SOA)",
        "text": "A service is a business activity that generates a specific outcome. It is self-contained and can be re-used by a number of business processes within and across different organizations. \n\nServices can be classified according to their level of granularity:\n\n- An atomic service encapsulates a fine-grained piece of functionality\n- An aggregate service contains larger, more complex or composite functionalities\n\nWhile aggregate services can be optimized to attain greater efficiency, fine-grained services allow greater flexibility for sharing and re-use. \n\n"
    },
    {
        "level": 3,
        "id": "sxbK7yRl",
        "title": "Web service",
        "text": "A web service allows a pre-formatted data message to be called \"on demand\" from a source by a client, using open standards such as HTML, XML, JSON, REST and SOAP. \n\n"
    },
    {
        "level": 2,
        "id": "HzaOGY5M",
        "title": "Enterprise architecture",
        "text": "An enterprise architecture is a framework (i.e., a set of principles and practices) for the integration of business processes, data, software applications and hardware infrastructure to achieve desired business results. \n\nIt is composed of:\n\n- **Business architecture:** Elements of the enterprise's business strategy, including business functions, processes and services (\"What the enterprise does, why it is done, and how\")\n- **Data architecture:** Definition of the organization's data structures, data flows, and data storage and management principles and practices\n- **Application architecture:** Principles and practices for selecting, designing and deploying software applications in accordance with business goals and processes\n- **Technology architecture:** Description of all hardware, and IT infrastructure necessary to develop and deploy business applications.\n\nAn enterprise architecture \"helps remove silos, improves collaboration [within] an organization and ensures that the technology is aligned to the business needs.\" \n\nHaving a well-defined enterprise architecture enables an organization to transform its legacy processes and applications and to adapt to emerging technology trends.  \n\nOther benefits of an enterprise architecture:\n\n- Better collaboration between IT and business units\n- Ability to prioritize IT investments\n- Effective evaluation and procurement of IT solutions\n- Common understanding of IT infrastructure and systems across all business units in the organization\n\nAn enterprise architecture addresses the entire organization in a holistic manner, instead of focusing on the needs and problems of individual business units.  It is akin to a \"master urban plan\" that provides the framework for a well-functioning city, beyond the \"blueprints\" specifying the design of a single building.  Following this analogy, one could say that the lack of an enterprise architecture leads to the proliferation of incoherent and inefficient solutions across the enterprise, similar to the effects of \"unconstrained urban sprawl\". \n\n\n\n"
    },
    {
        "level": 3,
        "id": "ysFKrz35",
        "title": "Organization map",
        "text": "An organization map identifies the business units and stakeholders of an enterprise, describing their capabilities and roles in the information and value streams within the context of the whole enterprise architecture.\n\nIt shows:\n\n1. Main organizational units, suppliers, partners, customers and other stakeholder groups that participate in the business model of the enterprise\n2. The network of formal and informal cooperative and collaborative relationships and interactions between each of those entities that are critical to the business, including:\n  - Value flows\n  - Information flows\n\nIn addition, an organization map shows:\n\n- Which organization units have which business capabilities (in a many-to-many mapping)\n- Relationships between organization units and geographic location\n\nAn organization map describes an enterprise as a social network, showing how its organizational units, suppliers, partners, customers and other stakeholder groups actually interact and operate across formal boundaries and siloes. \n\nIt includes individual organizational units as well as collaborative teams operating across two or more organizational units. \n\nPurpose of organizational mapping:\n\n1. Improve understanding of how different business units may be impacted by changes in business conditions and the implementation of new systems, applications, data models and infrastructure\n2. Support a holistic approach to strategic planning and investment analysis, showing how an initiative sponsored by one business unit may affect others. \n3. Maximize asset reuse across multiple business units, identifying common requirements and solutions.\n4. Identify which business unites need to communicate and collaborate in order to realize the goals of specific initiatives\n\nThe creation of an organization map is a key step in the development of a business architecture aimed to \"ensure that the real problem is solved\" thorugh changes in business processes, roles and responsibilities and investments in data, applications and IT infrastructure.\n\n"
    },
    {
        "level": 3,
        "id": "gFelA64o",
        "title": "Central IT and methodological services",
        "text": "Centralized IT and methodological units allow to pool resources and investments\n\n"
    },
    {
        "level": 3,
        "id": "kjBXVcZc",
        "title": "\"Accidental architecture\"",
        "text": "Situation in which technical solutions are \"built for a very specific purpose with little regard for ability to share information with other adjacent applications in the statistical cycle and with limited ability to handle simlar but slightly different processes and tasks.\"\n\n\"Historically, statistical organizations have developed their own business processes and IT-systems for producing statistical products. Although the products and the processes conceptually are very similar, the individual solutions are not\" (https://statswiki.unece.org/display/CSPA/I.++CSPA+2.0+The+Problem+Statement)\n\n"
    },
    {
        "level": 1,
        "id": "JO0BPDLM",
        "title": "Anti-patterns",
        "text": "An anti-pattern is the opposite of a design pattern: \"a proven flawed technique that will most likely cause some trouble and cost time and money. (...) A priori, it seems like a good idea, but in the end it will most likely cause more harm than good.\"\n\nSome examples of anti-patterns include:\n\n- **God class**: A class that handles too many things, and breaks the application every time someone touches it. \n\n"
    },
    {
        "level": 2,
        "id": "VsoEyAHs",
        "title": "Design Patterns",
        "text": "A design pattern is proven technique that can be sued to solve a specific problem.  \n\nDesign patterns help craft solutions at scale.\n\n"
    },
    {
        "level": 3,
        "id": "VURXQNM8",
        "title": "Test-driven development",
        "text": "Test-driven development (TTD) is a method of developing software in which tests are written before the actual code.  \n\nTTD can be implemented using the `Red-Green-Refactor` technique:\n\n1. Write a failing test (**red**)\n2. Write just enough code to make the test past (**green**)\n3. **Refactor** the code to improve the design while ensuring that all the tests are still passing\n\n\n"
    },
    {
        "level": 3,
        "id": "UdjxEMkP",
        "title": "Interface segregation principle",
        "text": "It is better to have multiple, smaller client-specific interfaces instead of  one large, general-purpose interface. \n\nCohesion - The elements of an interface should align towards a common goal.\n\nSmaller interfaces are easier to re-use, and help expose only the features that are needed for a specific purpose. They are also easier to compose into bigger pieces.   \n\n"
    },
    {
        "level": 3,
        "id": "GoNTrbVa",
        "title": "Writing high-quality code",
        "text": "Characteristics of high-quality code:\n\n- **Flexibility**\n- **Reusability**\n- **Maintainability**\n- **Readability**: Less code is simpler to visualize, leading to quicker undrestanding.  Use descriptive names for variables and methods. \n- **Simplicity**: Aim to write code that solves the issue at hand, instead of over-engineering it\n- **Testability**\n\n## Code smells:\n\nA code smell is an indicator of a possible problem. For example:\n\n- **Too many comments**: This is often an indication that the code could be split into smaller chunks with proper names, leading to better readability. \n\n- **Long methods**: More than 10 to 15 lines of code suggests a method (1) contains complex logic intertwined in multiple if statements, (2) does too many things, and/or (3) contains duplications of code.\n\n- **Names that are not self-explanatory**: When the name of an element does not immediately suggest what it is about, it is likely that the element could be split into smaller multiple pieces. \n\n"
    },
    {
        "level": 2,
        "id": "GoNTrbVa",
        "title": "Writing high-quality code",
        "text": "Characteristics of high-quality code:\n\n- **Flexibility**\n- **Reusability**\n- **Maintainability**\n- **Readability**: Less code is simpler to visualize, leading to quicker undrestanding.  Use descriptive names for variables and methods. \n- **Simplicity**: Aim to write code that solves the issue at hand, instead of over-engineering it\n- **Testability**\n\n## Code smells:\n\nA code smell is an indicator of a possible problem. For example:\n\n- **Too many comments**: This is often an indication that the code could be split into smaller chunks with proper names, leading to better readability. \n\n- **Long methods**: More than 10 to 15 lines of code suggests a method (1) contains complex logic intertwined in multiple if statements, (2) does too many things, and/or (3) contains duplications of code.\n\n- **Names that are not self-explanatory**: When the name of an element does not immediately suggest what it is about, it is likely that the element could be split into smaller multiple pieces. \n\n"
    },
    {
        "level": 3,
        "id": "HCHhyZym",
        "title": "Testing code",
        "text": "Testing is an integral part of the development process.  If developers have any difficulties consuming their own code when creating tests, then most likely everyone else will also have difficulties using the code.  \n\n** Testing good practices**:\n\n- Avoid writing useless tests\n- Keep the test suite healthy by adding missing test cases and removing obsolete or useless tests\n- Think in terms of use case coverage, instead of how many lines of code are covered by the tests\n- Automate tests as much as possible (running manual tests becomes tedious, time-consuming and error-prone as applications grow).  \n\n**Types of tests**\n\n1. **Unit tests**: Tests that focus on individual/atomic methods.  They should not rely on any infrastructure (such as a database); should be fast and test a specific code path only.  \n\n2. **Integration tests**: Tests that focus on the interaction among components, e.g., what happens when a component queries a database, or when two components interact with each other.  Integration tests often involve some infrastructure. \n\n3. **Functional tests**: Tests that focus on testing the whole application, from a user's perspective. \n\n"
    },
    {
        "level": 1,
        "id": "H2cZYvUV",
        "title": "Command and control",
        "text": "A command-and-control organizational culture is based on a clear hierarchy  where people at the top make decisions and direct how work is to be executed by those under their supervision.  In this setting, everyone knows who they work for and who works for them, and predictability of projects with clear scope, budget and schedules is highly valued.  \n\n"
    },
    {
        "level": 2,
        "id": "e7WKiFlE",
        "title": "Collaboration and decentralization",
        "text": "Collaboration is about decentralized decision making - so people closest to the work make the decisions.\n\n"
    },
    {
        "level": 1,
        "id": "W7R4OkLD",
        "title": "Technical debt",
        "text": "Technical debt consists of \"all the corners that are cut during the development of a feature or system.\" While it is unavoidable to incur in technical debt in every project, it is important to manage it and to keep it within limits;  otherwise, it will not be possible to pay it back. \n\nSome ways to manage and limit technical debt include: \n\n- Refactoring often\n- Learning to cooperate with all stakeholders involved in the project\n- Using proper governance and management techniques\n\n"
    },
    {
        "level": 2,
        "id": "MSGyGW7x",
        "title": "Refactoring",
        "text": "Refactoring is a process of improving existing code without changing its behavior.  It helps clean the code base of a project and get rid of some technical debt.\n\nRefactoring time must be factored into the planning of work (e.g., into product/sprint backlogs if using an Agile approach to project management).\n\n"
    },
    {
        "level": 3,
        "id": "c1CmqLad",
        "title": "Cooperation is essential to manage technical debt",
        "text": "Form time to time, it may be necessary to cut short the usage of best practices in order to meet short-term constraints, such as deadlines, budgets, etc.  To ensure that the resulting technical debt is properly managed, everyone involved must learn to cooperate and to understand each other's reality, and agree on how to allocate time and resources to re-factor existing solutions as soon as possible.  \n\n"
    },
    {
        "level": 3,
        "id": "zgBmscjq",
        "title": "Backward-compatibility principle",
        "text": "Everything that worked in a particular way in the past should still work at least in that particular way after changes are made to a system. \n\n"
    },
    {
        "level": 3,
        "id": "hAem1VBd",
        "title": "Embracing change",
        "text": "Applications are born to change.  It is important to remain flexible\n\n"
    },
    {
        "level": 2,
        "id": "c1CmqLad",
        "title": "Cooperation is essential to manage technical debt",
        "text": "Form time to time, it may be necessary to cut short the usage of best practices in order to meet short-term constraints, such as deadlines, budgets, etc.  To ensure that the resulting technical debt is properly managed, everyone involved must learn to cooperate and to understand each other's reality, and agree on how to allocate time and resources to re-factor existing solutions as soon as possible.  \n\n"
    },
    {
        "level": 1,
        "id": "R4yOjNYV",
        "title": "Knowledge assets",
        "text": "\"Statistical organizations deal with knowledge assets that aren't visible in the same way as physical or financial assets.\"\n\n"
    },
    {
        "level": 2,
        "id": "f7nY4e4m",
        "title": "Outsourcing knowledge work",
        "text": "To understand what sorts of knowledge work can be outsourced, ask:\n\n- Can the task be easily specified and measured?\n- Is delay between value creation and consumption possible?\n- Can the task be done remotely?\n\n"
    },
    {
        "level": 3,
        "id": "y1Zuq28z",
        "title": "Need for knowledge sharing across organizations",
        "text": "No organization alone has all the knowledge it needs to attain its objectives.  This creates an incentive for reaching across organizational boundaries in search of knowledge. \"The sharing of knowledge across organizations, particularly in the public sector, can lead to innovation, facilitate better problem definition, (...) reduce redundancies, build long-term collaborative capacity, and (...) increase accountability.\"\n\n"
    },
    {
        "level": 1,
        "id": "Eg5ejSSM",
        "title": "Lean concepts: Quality at the source (Jidoka)",
        "text": "\"Lean teaches us that quality inputs lead to quality outputs.  Focus on inputs allows for actionable metrics and direction for continuous improvement. (...)  \n\nA by-product of lean's overall drive for continuous improvement is teamwork, as \"the borders around responsibility and blame fade quickly when each person strives for improvement\".\n\n\n\n"
    },
    {
        "level": 2,
        "id": "aeKIUGRa",
        "title": "Quality",
        "text": "Quality is the degree to which the characteristics of a product or service meet the needs of its users.  \n\nSince different users have different, multiple needs that must be balanced against each other, a practical analysis of quality must break this definition down into multiple dimensions, so as to address different, but inter-related and often conflicting, types of user needs.\n\n"
    },
    {
        "level": 3,
        "id": "nJYCl4LV",
        "title": "Quality testing is a continuous task",
        "text": "Quality is not something that should be delegated to a \"test team\" at the end of the process.  \n\nQuality should be owned by the whole cross-functional team, and be continuously built in and verified in every iteration of the product. \n\n"
    },
    {
        "level": 1,
        "id": "JJsmi4lC",
        "title": "Fighting misinformation and conspiracy theories",
        "text": "\"It's undeniable that we face ongoing battles against misinformation, on subjects from COVID-19 to climate change, from vaccines to votes.\" What can be done \"to convince people to rethink their positions on fake news and propaganda'?\n\nDiscussing and exposing the common mechanisms behind the spread of misinformation, such as (1) use of a false authority figure, (2) appeal to an individual's anger and prejudices, (3) claim of urgency.  Promote \"active open-minded thinking\". \n\n\"People who are educated to recognize how misinformation spreads are less likely to be duped by it, or to spread it themselves\". \n\n\n\n"
    },
    {
        "level": 2,
        "id": "2pva38Uf",
        "title": "Conspiracy theories",
        "text": "A conspiracy theory is an unsubtantiated belief in the existence of a secret plot between a specific group of actors to manipulate a situation or engage in wrongdoing. It is a belief framework that postulates the existence of an external agent or group who is responsible for some unacceptable state of affairs.\n\nThe conspiratorial mindset provides:\n\n- **Epistemic benefits**: A framework for understanding the world and bringing order to otherwise random events\n- **Existential benefit**: A distraction from facing one's own fears and assuming one's own responsibilities.\n- **Social benefit**: The possibility of joining a community of similarly disaffected thinkers who can validate one another's anxieties and shared worldview. \n \n\"Conspiracy theories are memetic--they mutate easily and take on new forms-- which makes them a perfect fit for [viral spread in] social media platforms.\"\n\nConspiratorial ideas are now habitually deployed / weaponized by political leaders in order to create political tension. \n\n\"Legitimization of conspiracies ... has fundamentally altered the way many of us receive and accept information, so that now many people, without any evidence, view scientific method and fact-based journalism as suspicious, and see once-trusted leaders as nefarious plotters.  The damage to the public trust has been sever and won't be easily healed.\"\n\n"
    },
    {
        "level": 3,
        "id": "ZveYwM4W",
        "title": "Group-think or tribal epistemology",
        "text": "Group think, or \"tribal epistemology\", is a way to describe situations where people evaluate new information based on whether their \"tribe\" or community endorses it, instead of through the lens of \"common standards of evidence\". \n\nHere, the \"tribe\" is a community of similarly disaffected thinkers who can validate one another's anxieties and shared worldview.\n\n\"'Truth,' then, is whatever the tribal rhetoric says it is.\"\n\n"
    },
    {
        "level": 1,
        "id": "QMGTrrZV",
        "title": "Benefits from joining a community of practice",
        "text": "Individuals and organizations who share valuable ideas through a community of practice can build their reputation and trustworthiness.   \n\nThe community of practice provides:\n1. Access to multiple channels of informal cross-group communication \n2. Ability to connect with peers and potential partners from other organizations and occupational communities on a broad range of issues. \n3. The opportunity to obtain relevant \"know-how\" from a variety of perspectives\n\nIndividuals who do the same type of work in different jurisdictions can support each other when facing similar challenges, while individuals who have different roles can gain understanding of different viewpoints and find opportunities for cooperation.\n\nIn other words, the community of practice reduces the cost of knowledge sharing. \n\nUltimately, an effective community of practice leads participants to \"generate improvements in their own organizations, which can then be brought back to the community of practice, creating a cycle of learning and knowledge exchange for participants.\"\n\n\n"
    },
    {
        "level": 2,
        "id": "BLH4F9z3",
        "title": "Knowledge sharing in the public sector",
        "text": "- \"Knowledge sharing in the public sector tends to focus on connections across organizations that must coordinate to achieve a particular goal\"\n- But communities of practice can facilitate knowledge sharing across different public sector organizations that do similar things in different jurisdictions and \"simply need to know what others know\" even when they do not necessarily work towards a common goal or common service delivery.  \n- Such knowledge sharing can lead to improved performance. \n\n"
    },
    {
        "level": 3,
        "id": "ey9luVUC",
        "title": "Purpose of \"collaborative\" groups in official statistics",
        "text": "Over the years, several \"collaborative\" groups and \"working groups\" composed by practitioners from national and international statistical organizations have been established with the objective of helping official statisticians address the challenges they face by:\n  - Learning what peers do in similar situations\n  - Seeking and sharing knowledge\n\n\"Belonging to an international community reduces individual risks for new developments through additional scrutiny and testing [and allows to] harness capacity from across the community to insulate individual statistical organizations from budgetary shock.\"  (By Th\u00e9r\u00e8se Lalor, [Introducing CSPA!](https://statswiki.unece.org/pages/viewpage.action?pageId=112132835))\n\n"
    },
    {
        "level": 3,
        "id": "z9Y6pN8M",
        "title": "Communities of practice",
        "text": "Social relationships play an important role in people's ability to share and to use knowledge. \n\nCommunities of practice are:\n- \"Groups of people informally bound together by shared expertise and a passion for joint enterprise\" (Wenger and snyder, 2000, p. 139)\n- \"Building blocks of social learning systems\" (Wenger, 2000, p. 229)\n\nThey provide a low-risk way to access and share both explicit know-how (e.g., training in specific skills) and tacit know-how (e.g., values, norms, and behavioral expectations), as well as \"a safe space in which members can openly admit lack of knowledge\", \"empathize with each other's problems\" and \"show pride in their work.\"\n\nCommunities of practice usually cut across organizational boundaries, as people often identify with occupational or professional groups outside their own organization. For example, statisticians, data engineers and data scientists often describe their professional identity in terms of what they do, as opposed to where they work. Moreover, when people interact with others outside their organization, new knowledge emerges and becomes enriched as established facts, theories and hypothesis are examined, applied and tested in different contexts. \n\n"
    },
    {
        "level": 2,
        "id": "wX8xRCMT",
        "title": "Meetings of a community of practice",
        "text": "The primary activity of a community of practice usually consists of holding face-to-face or virtual meetings. \n\nPrior to each meeting, coordinators collect data from participants on the main topic for discussion (e.g., via an electronic survey), and analyze it along other publicly available data.\n\nEach meeting starts with a presentation of the preliminary data analysis by the coordinators, followed by a list of suggested questions for discussion. These questions specifically encourage participants to make informal comparisons among each other, with the aim to generate new insights and learning (e.g., \"who uses X approach?\", \"how many staff/resources are available to do Y?\", \"who thinks X is a good practice?\"...)\n\nMeetings allow participants to:\n\n- Discuss differences in their own approaches to address specific challenges\n- Share anecdotes\n- Pose additional questions and pass information to the group\n- Get new ideas and brainstorm\n- Identify and \"bubble up\" good practices\n- Construct an unofficial list of who knows what\n\nIdeally, the meeting is only the initial contact point that generates off-line follow-up discussions.  Ideally, participants \"seek each other out prior to, during, and after the meeting to connect on issues thy have in common or find out more details about what they are doing in response to a particular challenge.\"\n\n"
    },
    {
        "level": 3,
        "id": "2lr4gGNA",
        "title": "Meeting management",
        "text": "Meetings can go off tracks due to unanticipated displays of passion from a team member.\n\n\n"
    },
    {
        "level": 2,
        "id": "z9Y6pN8M",
        "title": "Communities of practice",
        "text": "Social relationships play an important role in people's ability to share and to use knowledge. \n\nCommunities of practice are:\n- \"Groups of people informally bound together by shared expertise and a passion for joint enterprise\" (Wenger and snyder, 2000, p. 139)\n- \"Building blocks of social learning systems\" (Wenger, 2000, p. 229)\n\nThey provide a low-risk way to access and share both explicit know-how (e.g., training in specific skills) and tacit know-how (e.g., values, norms, and behavioral expectations), as well as \"a safe space in which members can openly admit lack of knowledge\", \"empathize with each other's problems\" and \"show pride in their work.\"\n\nCommunities of practice usually cut across organizational boundaries, as people often identify with occupational or professional groups outside their own organization. For example, statisticians, data engineers and data scientists often describe their professional identity in terms of what they do, as opposed to where they work. Moreover, when people interact with others outside their organization, new knowledge emerges and becomes enriched as established facts, theories and hypothesis are examined, applied and tested in different contexts. \n\n"
    },
    {
        "level": 3,
        "id": "Tlb9tDLU",
        "title": "Sharing of experiences, good practices and lessons learned",
        "text": "Success in the use of new technologies and sources of data will depend on a wide variety of contextual and operational factors, as well as on the evolving severity and nature of the impact of the COVID-19 epidemic in each country. \n\nManagers of national and global statistical programmes affected by the crisis require a platform for rapidly sharing experiences and lessons learned as they navigate a new environment characterized by many uncertainties and risks. \n\nThis sharing should focus on the essential aspects of planning, management and implementation of re-organization and adaptation strategies. \n\n"
    },
    {
        "level": 3,
        "id": "4jDVh28g",
        "title": "Characteristics of effective communities of practice",
        "text": "What structures, behaviors, and processes facilitate knowledge sharing in a community of practice? What factors influence an organization's decision to join and actively participate?\n\nTo be effective, a community of practice needs to generate value to participants seeking information on how to address specific challenges they face in their own organizations. \n\nSuccessful communities of practice usually:\n \n- **Are coordinated by a respected outside party**: Rather than being ad hoc, meetings are planned and scheduled in advance, and participants know ahead of time the topical focus for discussion.\n- **Have a small, but sufficient funding stream**: Funding allows for minimal expenses, such as hiring a part time administrator and a part-time analyst, and the production of materials and handouts necessary for the meetings. \n- **Have an agenda driven by their members**: The topics and the format of the meetings are driven by a steering committee and the interests of participating communities\n- **Allow different levels of engagement**: There are no prescriptive rules specifying what will be shared by whom, and participants can decide to fully engage or to passively observer discussions, have one-on-one informal conversations, and see what other share.\n\nIt is important to understand the social networks established through a community of practice:\n\n- Are some actors more central to others?\n- How does social-network centrality influence behavior of participants?\n\n"
    },
    {
        "level": 3,
        "id": "4hQReuCs",
        "title": "Role of coordinators of a community of practice",
        "text": "Coordinators of a community of practice must fulfill the following tasks:\n\n- Plan and announce group meetings\n- Gather relevant data through research and through pre- and post-meeting surveys of members\n- Analyze data\n- Facilitate discussions\n- Ensure timeliness and consistency of the group's activities\n- Manage time\n\n\n\n"
    },
    {
        "level": 2,
        "id": "4hQReuCs",
        "title": "Role of coordinators of a community of practice",
        "text": "Coordinators of a community of practice must fulfill the following tasks:\n\n- Plan and announce group meetings\n- Gather relevant data through research and through pre- and post-meeting surveys of members\n- Analyze data\n- Facilitate discussions\n- Ensure timeliness and consistency of the group's activities\n- Manage time\n\n\n\n"
    },
    {
        "level": 3,
        "id": "e1cZF4Rq",
        "title": "Coordination of statistical capacity development in the UN System",
        "text": "The Statistics Division of DESA is the Secretariat of the United Nations Statistical Commission, and as such, its main counterparts at the national level are National Statistical Offices.  \n\nIn addition to this direct channel of communication with NSOs, the Statistics Division regularly engages with the Committee of Coordination of Statistical Activities, the Development Coordination Office, and the UN Resident Coordinator Offices to ensure that its statistical capacity development activities are aligned with those of other UN Agencies at the national, regional and global levels. \n\n"
    },
    {
        "level": 1,
        "id": "4xQthRYr",
        "title": "Simple vs Complex networks",
        "text": "- Simple networks have regular nodes and edges. \n- Complex networks have a non-trivial structure that often results from decentralized processes with no central control.\n\n"
    },
    {
        "level": 2,
        "id": "MV7Xi5dH",
        "title": "Network analysis",
        "text": "Exploring quantitative relationships in networks with non-trivial structure.\n\nThe study of complex networks did not start until the late 1800s/ early 1900s due to lack of proper mathematical apparatus (graph theory) and adequate computational tools.\n\n"
    },
    {
        "level": 3,
        "id": "p6ocJHTv",
        "title": "Network analysis: Degree",
        "text": "The degree of a node in a network is the number of immediate neighbors (adjacent nodes).  It is a non-negative integer number. \n\n"
    },
    {
        "level": 3,
        "id": "5uEpgy6B",
        "title": "NetworkX",
        "text": "Python library for complex network analysis, including construction, exploration, analysis, measurement an visualization of networks.\n\nhttps://networkx.org/documentation/stable/index.html\n\n\n"
    },
    {
        "level": 3,
        "id": "bit4HVlF",
        "title": "Network model of knowledge management",
        "text": "A network model facilitates the management, creation, updating and diffusion of knowledge through digital cross-referencing.\n\n"
    },
    {
        "level": 3,
        "id": "BaBoinzs",
        "title": "Network analysis: Modularity",
        "text": "Modularity measures the strength of division of a network into clusters (modules). A network that is partitioned into non-overlapping modules is said to have high modularity if the connections between the nodes within modules are dense, while  the  connections between nodes in across modules are sparse.\n\nIt is defined as the fraction of the edges of a network that fall within the given modules, minus the expected fraction if edges were distributed at random.\n\n"
    },
    {
        "level": 3,
        "id": "sA6Ay1UI",
        "title": "Network analysis: Snowball sampling",
        "text": "Method for discovering all the nodes and edges of interest, starting from a \"seed\" node, using a breadth-first algorithm.\n\n1. Start with a seed\n2. Then the seed's neighbors\n3. Then the neighbors' neighbors\n4.  ...\n\nThe algorithm must remember which nodes have been already processed and which ones have been discovered but not yet processed (done/to do), keeping track of the distance from the node being currently processed to the seed. \n\n\n\n"
    },
    {
        "level": 3,
        "id": "ifY62dRd",
        "title": "Network formation: preferential attachment",
        "text": "Mechanism that leads to complex network formation, whereby nodes with more edges get even more edges, forming large hubs in the core, surrounded by poorly connected nodes in the periphery. \n\n"
    },
    {
        "level": 2,
        "id": "ifY62dRd",
        "title": "Network formation: preferential attachment",
        "text": "Mechanism that leads to complex network formation, whereby nodes with more edges get even more edges, forming large hubs in the core, surrounded by poorly connected nodes in the periphery. \n\n"
    },
    {
        "level": 3,
        "id": "Mv4Ukj5Q",
        "title": "Network formation: transitive closure",
        "text": "Connect two nodes together if they are already connected to a common neighbor\n\n"
    },
    {
        "level": 1,
        "id": "h1xZIuYx",
        "title": "Investing in people",
        "text": "\"To effectively reenergize their workforces, organizations need to shift their emphasis from getting more out of people to investing more in them, so they are motivated--and able--to bring more of themselves to work every day.\n\nEmployees reward organizations that treat them humanely, fairly and with dignity.  In contrast, loyalty to the organization erodes not only among employees that are treated inhumanely, unfairly or without respect, but also among their colleagues who witness such behavior.  This can lead to serious talent retention problems. \n\n"
    },
    {
        "level": 2,
        "id": "fOMXYZho",
        "title": "Conflict management",
        "text": "- Depersonalize conflict, focusing on the disagreement issue rather than on the interested parties. \n- Look beyond the merits of an issue and understand the interests, feasrs, aspirations, and loyalties of the factions around it.\n\n"
    },
    {
        "level": 3,
        "id": "k89uWQ1l",
        "title": "Self-management: Change the narrative",
        "text": "- **Reverse lens**: \"What would the other person in this conflict say and in what ways might that be true?\"\n- **Long lens**: \"How will I most likely view this situation in six months?\"\n- **Wide lens**: \"Regardless of the outcome, how can I learn and grow from it?\"\n\n"
    },
    {
        "level": 1,
        "id": "V9r8N6je",
        "title": "Function expressions in JavaScript",
        "text": "**Function expressions** create functions as a variable.  They are not hoisted -- cannot be invoked before being created. See (https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function#Function_declaration_hoisting)\n\nIn **JavaScript**, functions are \"first-class\" members. It is possible to declare functions with the `var`, `const`, or `let` keywords.  Functions can do the same things that variables can do.  Functions can be added to objects and arrays, sent to functions as arguments, and can be returned from functions.  \n\nNote:\n\n- With the `let` keyword, we can scope a variable to any code block, protecting the value of the global variable\n- The `const` keyword declares a variable that cannot be overwritten. Once declared, its value cannot be changed.\n\n\n"
    },
    {
        "level": 2,
        "id": "WtKaAhds",
        "title": "JavaScript for React",
        "text": "- Arrays\n- Objects\n- Functions\n- Functional JavaScript\n\n"
    },
    {
        "level": 3,
        "id": "h6EOMFkP",
        "title": "Node.js and JavaScript projects",
        "text": "`Node.js` is an asynchronous, event-driven JavaScript runtime environment  designed to build scalable network applications.\n\nNode.js allows to use JavaScript in web-application development, for both server- and client-side scripts.\n\nTo make sure Node.js is intalled:\n`node -v`\n\n## npm\n\n`npm` is a package manager for the Node.js environment that makes it easier  to publish and share source code of Node.js packages, and is designed to simplify installation, updating, and uninstalling of packages.\n\n`package.json` is a file describing a JavaScript project and its dependencies.  When running `npm install` in the folder that contains the `package.json` file, npm will install all the packages listed in the project.  \n\n- `npm init -y` will initialize a new project and create the `package.json` file.\n- `npm install <package name>` will install a package\n- `npm remove <package name>` will remove a package\n\n### yarn\n\n`yarn` is an alternative to `npm`.  \n\n- `npm install -g yarn` will install yarn globally with npm:\n- `yarn` will install dependencies from `package.json`\n- `yarn add <package-name>`\n- `yarn remove <package-name>`\n\n\n\n"
    },
    {
        "level": 3,
        "id": "V8p4H53X",
        "title": "Deconstructing JavaScript Objects",
        "text": "Technique that consists of pulling fields within an object to create local variables for them.\n\n"
    },
    {
        "level": 3,
        "id": "b8UigmQs",
        "title": "React learning roadmap",
        "text": "1. JavaScript for React\n2. Building a user interface with components\n3. Adding logic with props and state\n4. Use of React Hooks - to reuse stateful logic between components\n\n"
    },
    {
        "level": 3,
        "id": "GRNZb1vP",
        "title": "React",
        "text": "React is a JavaScript library designed to build component-based, high-performing, single-page applications (front ends). It allows to write JavaScript code that looks like HTML and can update the browser DOM, using patterns that are readable, reusable and testable.  \n\nReact needs pre-processing to run in a browser (it needs a build tool like Webpack).\n\nThe major advantage of React is its ability to separate data from UI elements.\n\n"
    },
    {
        "level": 1,
        "id": "PLe99ZmC",
        "title": "React hooks",
        "text": "React hooks are a new way of add and share stateful logic across components. When data within the hook changes, they can cause a component to re-render in order to reflect the new data.\n\nHooks contain reusable code logic that is separate from the component tree.\n\nExamples of hooks include:\n\n- `useState`: \n\n  - The value sent to the `useState` function is the default variable of the state variable\n  - The first value of the return array is the state variable\n  - The second value of the return array is a function that can be used to change the state value\n\n\n"
    },
    {
        "level": 2,
        "id": "zC7DLmEh",
        "title": "React components",
        "text": "React components are parts that make up a user interface (e.g., buttons, lists, heading...).  They allow the re-use of the same structure, which can be populated many time with different sets of data.\n\nReact applications are data-driven. Components are \"vessels\" for data. The value of a React application depends on the data that flows through its components.\n\nA component is created by writing a function, which returns the re-usable part of a user interface.\n\n\n\n\n"
    },
    {
        "level": 3,
        "id": "GfeBsLyO",
        "title": "Bundling in React applications",
        "text": "Network performance is improved by having to load only one dependency in the browser: the \"bundle\"\n\nBy bundling all the dependencies into a single file, it is possible to load everything with a single HTTP request, avoiding additional latency\n\n`Webpack` is one of the leading React bundling tools.  It takes all different files (JavaScript, CSS, JSX...) and turns them into a single file for improved modularity and performance,\n\n"
    },
    {
        "level": 3,
        "id": "W0uZ0vW4",
        "title": "Function composition and chaining",
        "text": "Composition is the process of putting smaller, pure functions that perform specific tasks back together, to call them in parallel, or compose them into larger functions. \n\nThe 'dot' notation is often used to allow a function to act on the return value of the previous function. It allows to 'pipe' an input value through a sequence of functions.\n\n"
    },
    {
        "level": 3,
        "id": "mJPzl1Wc",
        "title": "State in React",
        "text": "The state of a React application is driven by data\n\n- Create stateful components\n- Send state down a component tree\n- Send user interactions back up the component tree\n\n"
    },
    {
        "level": 1,
        "id": "NwHayaCP",
        "title": "Liberalism",
        "text": "Liberalism seeks to provide a way out of the continuous oscillation between authoritarianism and individualism.  It aims to establish a social order that is not built on the basis of irrational dogma, but at the same time, allows for a minimum of stability and order necessary for the preservation and functioning of the state. \n\n"
    },
    {
        "level": 2,
        "id": "uIHekGyz",
        "title": "Individual freedom vs social cohesion",
        "text": "In human history, as in the history of philosophy, the pursuit of individual freedom has been always in conflict with the quest for social cohesion \n\n- In ancient Greece, social bonds where established, in varying degrees, through the loyalty and duties _of citizens_ towards the state. \n- After the Greeks where conquered by Rome, a more individualistic ethic emerged (e.g., the stoics identified virtue not so much in the relationship between citizens and the State, but between the soul and God).  \n- With Christianity, the idea became more widespread that an individual's duties towards God take precedence over those towards the state. \n- Liberalism provided for a clear separation between the public and the private spheres.  \n- This separation let, on one hand. to romanticism and individualism, and on the other to doctrines centered around the glorification of the state.\n\n\n"
    },
    {
        "level": 3,
        "id": "V2qxeWls",
        "title": "Why study philosophy?",
        "text": "Historian's perspective:\n\n- People's behavior is at least in part determined by their notions of good and evil. \n- \"To understand an era or a people, one has to understand their philosophy.\"\n- There is a two-way causality between human history and the history of philosophy; similarly, one's life situation determines one's own philosophy, and vice-versa.\n\nPersonal perspective:\n\n- Philosophy tries to teach us how to live without being paralyzed by the absence of certainty.\n\n"
    },
    {
        "level": 3,
        "id": "SLgwJQDz",
        "title": "Conflict between church and state in the Middle Ages",
        "text": "In the conflict between church and state that ensued between the end of the 5th century until middle of the 11th century, the former prevailed. The established philosophy of this era played a key role in this, since although the secular centers of power kept the monopoly of violence and were not bound by a notion of legality, the authority of kings and barons of Germanic descent was dependent on the loyalty and support of their feudal aristocracies, while the church had the monopoly on education and, more importantly, *was believed* to have the keys to everyone's eternal salvation or damnation.\n\n\n\n"
    },
    {
        "level": 3,
        "id": "GH8W0S4g",
        "title": "Locke",
        "text": "Locke opposed both individualism and the unmitigated submission of the individual to an absolute authority.  According to Russell, this eventually led to doctrines centered around the glorification of the state.\n\n"
    },
    {
        "level": 2,
        "id": "GH8W0S4g",
        "title": "Locke",
        "text": "Locke opposed both individualism and the unmitigated submission of the individual to an absolute authority.  According to Russell, this eventually led to doctrines centered around the glorification of the state.\n\n"
    },
    {
        "level": 3,
        "id": "rJKUGkhR",
        "title": "Origins of Philosophy as a discipline",
        "text": "- Philosophy originated as a discipline independent of theology in Greece around the **6th century BC**, with Thales. In its origins, philosophy and science were not different from each other.\n- With the rise of Christianity and the **end of the Roman empire**, philosophy merged back into theology \n- Philosophy's second \"big era\" between the **11th and 14th centuries** under the rule of the catholic church, and ended with the Reformation.\n- It's third era, from the **17th century to date**, is more influenced by science and secular points of view, although traditional religious influences are still present. \n\n"
    },
    {
        "level": 1,
        "id": "bawDrRBv",
        "title": "Renaissance and Reformation",
        "text": "The Renaissance and the Reformation destroyed the unity of Christianity around the pope's authority.\n- New knowledge about antique cultures became more widespread\n- Copernicus' astronomy gave Man and the Earth a different place in philosophy\n- The belief in eternal regularity was replaced by scientific inquiry, subjectivism and moral relativism\n- With Machiavelli, politics was understood as the naked pursuit of power.\n- The Reformation was a revolt of the Nordic kings and peoples against the pope's dominance.\n- The authority of the pope was countervailed not by one secular emperor, but by a multitude of nation states (and the strengthening of social bonds within them)\n- In the new protestant philosophy, there is no longer a single earthly intermediary between the soul and God\n- The truth does not come from an authority, but is grasped by each individual through reason\n- This led to the advent of anarchism, mysticism. \n- There is no one but many \"protestant philosophies\".\n- Slowly, the seeds are planted for a re-emergence of individualism and pluralism.\n\n"
    },
    {
        "level": 2,
        "id": "kSY5JM8j",
        "title": "Origins of modern philosophy: Descartes and subjectivism",
        "text": "Modern philosophy begins with Descartes, who attempts to derive the knowledge of the external world from the certain knowledge of the existence of one's self and one's own thoughts.\n\nThe resulting philosophical subjectivism--whose origins can be traced back to the protestant opposition to the authority of the pope--goes hand in hand with the emergence of liberalism and a more general opposition to all kinds of secular government and the gradual rise of political anarchism.\n\n"
    },
    {
        "level": 1,
        "id": "tnUhXJcI",
        "title": "The decade of action",
        "text": "In spite of progress made so far in many places, action towards achieving the SDGs by 2030 is not fast or comprehensive enough.  The **Decade of Action** launched by the UN Secreatry-General in September 2019 calls for accelerating sustainable solutions to the world\u2019s biggest challenges on three levels: \n\n- **Global action**: Secure leadership and resources\n- **Local action**: Design and implement policies, budgets, institutions and regulatory frameworks of governments, cities and local authorities\n- **People action**: Mobilize youth, civil society, the media, the private sector, unions, academia and other stakeholders\n\n"
    },
    {
        "level": 2,
        "id": "5u1sLb2p",
        "title": "Objectives of data innovation for Sustainable Development",
        "text": "- Data innovation projects should enable countries to *regularly* produce better, more timely and more disaggregated data to inform policies and decisions that contribute to achieving the 2030 Agenda for Sustainable Development\n- Measures of success: Data is available and openly accessible online to policy and decision makers in easy-to-use formats and presentations.\n  - Frequency (at least once a year)\n  - Time lag (less than 2 years)\n  - Geographic coverage (national coverage)\n  - Geographic disaggregation (at least 3rd level)\n  - Coverage of specific population groups (women, disabled, youth, elderly)\n\n"
    },
    {
        "level": 3,
        "id": "EojLU88D",
        "title": "Global SDG indicator framework",
        "text": "The 17 goals and 169 targets of the 2030 Agenda for Sustainable Development are monitored and reviewed at the global level through an indicator framework developed by the Inter-agency and Expert Group on SDG Indicators. \n\nThe Global SDG indicator framework was initially agreed upon at the 48th session of the United Nations Statistical Commission in March 2017, and adopted by the General Assembly on 6 July 2017 in its [resolution on the Work of the Statistical Commission pertaining to the 2030 Agenda for Sustainable Development (A/RES/71/313)](https://undocs.org/A/RES/71/313).  \n\nAnnual refinements to the Global SDG Indicator Framework, as well as more comprehensive reviews, are submitted by the IAEG-SDGs to the United Nations Statistical Commission for its approval.  \n\nThe 2020 comprehensive review included 36 major changes to the framework in the form of replacements, revisions, additions and deletions to the framework.  The next comprehensive review is scheduled to take place in 2025.\n\n\n"
    },
    {
        "level": 3,
        "id": "oTbmWlt2",
        "title": "Mainstreaming data innovation projects",
        "text": "It is crucial to ensure that the objectives and scope of national data innovation projects fit with the priorities of National Strategies for the Development of Statistics, with the regular work programme of NSOs and with the overall institutional setting of the National Statistical system.\n\nFor instance, there has to be a clear link between project outputs and SDG reporting platforms and VNR processes to ensure sustained demand from policy makers and other key stakeholders.\n\n"
    },
    {
        "level": 3,
        "id": "0zGb0Ntw",
        "title": "The 2030 Agenda for Sustainable Development",
        "text": "The 2030 Agenda for Sustainable Development is a call for action by all the member states of the United Nations to end poverty and other deprivations, improve health and education, empower women and girls, reduce inequalities, spur economic growth, strengthen universal peace in larger freedom, realize the human rights of all, tackle climate change, and preserve the environment for current and future generations.  \n\nIt recognizes the eradicating poverty as the greatest global challenge and as an indispensable requirement for sustainable development. \n\nThrough 17 Sustainable Development Goals and 169 targets, all member states committed to take bold and transformative steps to shift the world onto a sustainable and resilient path and to leave no one behind,\n\nThe 2030 Agenda cover 5 Areas of action:\n\n- **People:** End poverty and hunger, and ensure everyone can fulfill their potential in dignity and equality\n\n- **Planet:** Protect the planet from degradation, so it can support the needs of the present and future generations.\n\n- **Prosperity:** Ensure everyone's ability to enjoy prosperous and fulfilling lives and economic, social and technological progress in harmony with nature.\n\n- **Peace:** Foster peaceful, just and inclusive societies, free from fear and violence. \n\n- **Partnership:** Mobilize global solidarity among all people, countries, and stakeholders, focused particularly on the needs of the poorest and most vulnerable\n\n"
    },
    {
        "level": 2,
        "id": "GQbIN4UB",
        "title": "Role of multi-stakeholder partnerships",
        "text": "Multi-stakeholder partnerships are key to **mobilize support and action** around the 2030 Agenda for Sustainable Development.\n\n*SDG Targets on Multi-stakeholder partnerships:*\n\n- 17.16 Enhance the global partnership for sustainable development, complemented by multi-stakeholder partnerships that mobilize and share knowledge, expertise, technology and financial resources, to support the achievement of the sustainable development goals in all countries, in particular developing countries\n- 17.17 Encourage and promote effective public, public-private and civil society partnerships, building on the experience and resourcing strategies of partnerships\n\nThe [Partnerships for SDGs online platform](https://sustainabledevelopment.un.org/partnerships/) is a global registry of voluntary commitments and multi-stakeholder partnerships maintained by the United Nations to facilitate the global engagement of all stakeholders in the implementation of the 2030 Agenda.\n\n"
    },
    {
        "level": 2,
        "id": "0zGb0Ntw",
        "title": "The 2030 Agenda for Sustainable Development",
        "text": "The 2030 Agenda for Sustainable Development is a call for action by all the member states of the United Nations to end poverty and other deprivations, improve health and education, empower women and girls, reduce inequalities, spur economic growth, strengthen universal peace in larger freedom, realize the human rights of all, tackle climate change, and preserve the environment for current and future generations.  \n\nIt recognizes the eradicating poverty as the greatest global challenge and as an indispensable requirement for sustainable development. \n\nThrough 17 Sustainable Development Goals and 169 targets, all member states committed to take bold and transformative steps to shift the world onto a sustainable and resilient path and to leave no one behind,\n\nThe 2030 Agenda cover 5 Areas of action:\n\n- **People:** End poverty and hunger, and ensure everyone can fulfill their potential in dignity and equality\n\n- **Planet:** Protect the planet from degradation, so it can support the needs of the present and future generations.\n\n- **Prosperity:** Ensure everyone's ability to enjoy prosperous and fulfilling lives and economic, social and technological progress in harmony with nature.\n\n- **Peace:** Foster peaceful, just and inclusive societies, free from fear and violence. \n\n- **Partnership:** Mobilize global solidarity among all people, countries, and stakeholders, focused particularly on the needs of the poorest and most vulnerable\n\n"
    },
    {
        "level": 3,
        "id": "3QR1WCvN",
        "title": "Sustainable development",
        "text": "Sustainable development is \"development that meets the needs of the present without compromising the ability of future generations to meet their own needs\". \n\n\n"
    },
    {
        "level": 1,
        "id": "ClFwmtgA",
        "title": "Data visualization",
        "text": "Exploring data and analytic results in visual form helps identify and communicate conclusions in ways that are easily understood, shared and acted upon.\n\n"
    },
    {
        "level": 2,
        "id": "3c7dZcB6",
        "title": "Communicating results of data innovation projects",
        "text": "**Risk:**\n\n- Outputs from data innovation projects do not reach their intended users, and remain unused by policy and decision makers\n- Key insights and caveats of project results are not presented in formats that are easy to understand by different types of technical and non-technical users \n- Potential users are not aware of project outputs and/or their value to inform policy and decision making\n\n**Mitigation:**\n\n- Communicate clearly and frequently any limitation of the source data and of the applicability of outputs, with a view to avoiding the building of unrealistic expectations\n- Explain possible applications and demonstrate the value of resulrs at an early stage\n- Develop a results communication strategy aimed to maximize the reach of the outputs and their impact. This strategy may include, among other things:\n  - A **social media campaign** to highlight main results and outputs, and direct user to data and resources to understand the underlying methodologies. \n  - **Online dissemination of results** through the official data dissemination platforms of the NSO and participating government agencies, including: \n    - Downloadable **datasets**\n    - Online **maps and visualizations**\n    - Data stories / **narratives** that put data outputs in context and make them easier to understand for policy and decision makers\n  - Production and distribution of project **reports** in multiple formats\n  - User outreach **events**\n\n"
    },
    {
        "level": 3,
        "id": "wFd1FvbC",
        "title": "Ensuring national ownership of data innovation projects",
        "text": "**Risk**: \nExternal partners may be perceived by local teams as exerting too much ownership over the process\n\n**Mitigation**: \n- A specific entity within each country (usually the NSO) should be identified and recognized as the **owner** of every data innovation project. \n- This entity should be responsible to ensure the quality of outputs and should be willing to commit staff and invest resources in the project. \n- The main **focal point** for every country project should be within that entity, and all inquiries should be referred to that focal point first. \n- The first publication of all results should be done by the country, and publications by external partners should make reference to national publications\n\n\n\n"
    },
    {
        "level": 3,
        "id": "Eymkhg9U",
        "title": "Data Communication",
        "text": "\"The goal of effective data communication is to ensure that data is transmitted, decoded, and understood accurately, and acted upon.\"  Describing different spation-temporal trends, identifying patterns through storytelling supported with visual aids is an effective way of communicating data to users\n\n"
    },
    {
        "level": 2,
        "id": "iJY1iH4F",
        "title": "Geospatial data analysis and visualization",
        "text": "Geospatial data analysis and map visualizations can help identify trends, patterns and relationships between different data sets over geographic space.  The analysis and visualization of geospatially disaggregated data \"enhances the ability to understand and respond to place-based factors\" affecting the phenomenon under study.  It also can lead to \"new questions or ideas about relationships in the data that can be further explored with additional methods\". \n\n"
    },
    {
        "level": 3,
        "id": "U3elusdN",
        "title": "Google Earth Engine",
        "text": "Combines a large catalogue of satellite imagery and geospatial datasets with GIS analysis capabilities to detect changes, map trends and quantify differences on Earth surface\n\n"
    },
    {
        "level": 1,
        "id": "URGTzbbu",
        "title": "Measures by WHO and public health authorities to contain COVID-19",
        "text": "WHO and public health authorities across the world are taking measures to contain the COVID19 epidemic.  All sectors of society, including members of the global and national statistical systems, need to act promptly and in a coordinated manner to respond effectively to the crisis and mitigate its human and economic impacts, as well as to usher a rapid and sustainable recovery.\n\n"
    },
    {
        "level": 2,
        "id": "mRUJpSCa",
        "title": "Impact of COVID19 pandemic on national statistical offices",
        "text": "The measures to contain the spread of the epidemic in many countries include the requirement for very large parts of the population to stay at home and avoid social contact.  There is a substantial risk that the global health emergency will continue disrupting the normal operations of all sectors of society over several months. \n\nThis is disrupting statistical operations around the world, as the staff of statistical organizations are unable to go to their offices or to the field in order to perform their regular tasks.  The crisis is also making it more difficult to engage with the broader statistical community, as workshops, seminars, expert forums, inter-agency coordination meetings and other public events are cancelled. \n\nThe impact on statistical operations include:\n- Disruptions in data collection\n- Disruption of data processing and analysis workflows\n- Delay in publication of statistical outputs\n- Suspension of user engagement events and staff training activities\n\n\n\n"
    },
    {
        "level": 3,
        "id": "rps4c5Jo",
        "title": "Impact of population mobility restrictions on data collection programmes",
        "text": "In an effort to contain the spread of the COVID-19 epidemic, many governments are imposing severe restrictions on the mobility of the population, disrupting field data collection operations and threatening the ability of National Statistical Offices to deliver high-quality, timely and cost-effective statistical outputs. \n\nThis results in the urgent need to replace current field data collection operations that rely on face-to-face interviews with alternative remote data collection methodologies, such as telephone personal interviewing or paper or web-based self-reporting methods.  \n\nTo respond to this challenge, many countries need to build quickly the necessary capacity to accelerate the implementation of fully digital data collection technologies instead of traditional paper-based methods.\n\n"
    },
    {
        "level": 3,
        "id": "jj13yRW9",
        "title": "COVID19 contingency plans in statistical organizations",
        "text": "- Determine how existing statistical activities, processes and programmes are being affected by the crisis\n- Find alternative means to reach existing data sources (e.g., computer-assisted self interviews with the use of internet and smart phone, or computer assited telephone interviews), or alternative data sources when existing ones are no longer accessible (e.g., switch to use of administrative sources and webscraping).\n- Develop a plan to switch on-site and field processes to a remote/telecommuting setting:\n    - Processes that can be performed remotely by staff using existing infrastructure (e.g., repurposing CAPI software and equipment to support CATI data collection)\n    - Processes that could be performed remotely after digitizing and/or migrating them to cloud environments (e.g., maintenance of public data dissemination portals)\n    - On-site and field processes that would require establishing mechanisms for secure remote access to central databases and systems in order to be performed out of premises (e.g., processing and analysis of confidential microdata records).\n- Identify priority data needs of governments and other stakeholders to respond to the COVID19 crisis\n\n\n"
    },
    {
        "level": 3,
        "id": "nVgnq2cz",
        "title": "Impact of COVID-19 on census programmes",
        "text": "The COVID-19 crisis creates unprecedented and sudden challenges for countries conducting population and housing censuses and other major statistical operations, disrupting data collection, processing, analysis and dissemination activities carried out by National Statistical Offices, forcing them to rapidly develop and adopt alternative ways of implementing them.\n\n"
    },
    {
        "level": 2,
        "id": "jj13yRW9",
        "title": "COVID19 contingency plans in statistical organizations",
        "text": "- Determine how existing statistical activities, processes and programmes are being affected by the crisis\n- Find alternative means to reach existing data sources (e.g., computer-assisted self interviews with the use of internet and smart phone, or computer assited telephone interviews), or alternative data sources when existing ones are no longer accessible (e.g., switch to use of administrative sources and webscraping).\n- Develop a plan to switch on-site and field processes to a remote/telecommuting setting:\n    - Processes that can be performed remotely by staff using existing infrastructure (e.g., repurposing CAPI software and equipment to support CATI data collection)\n    - Processes that could be performed remotely after digitizing and/or migrating them to cloud environments (e.g., maintenance of public data dissemination portals)\n    - On-site and field processes that would require establishing mechanisms for secure remote access to central databases and systems in order to be performed out of premises (e.g., processing and analysis of confidential microdata records).\n- Identify priority data needs of governments and other stakeholders to respond to the COVID19 crisis\n\n\n"
    },
    {
        "level": 3,
        "id": "mRaNOol6",
        "title": "Complexity of census programmes",
        "text": "Population and housing census programmes are complex data collection operations comprising a series of many interrelated activities. They require contacting and collecting information on the whole population of a country within a limited period of time. \n\n"
    },
    {
        "level": 3,
        "id": "SGskQjpC",
        "title": "Contingency planning needs to be tailored to national circumstances",
        "text": "The response of each National Statistical Office to the COVID-19 crisis needs to be tailored to its own particular circumstances, including the nature and severity of the disruptions caused by the crisis, its existing infrastructure, technical capacities and resources, and other institutional, operational, economic, and socio-cultural factors. \n\n"
    },
    {
        "level": 3,
        "id": "qvOtxtiK",
        "title": "Continuity of statistical programmes: Role of new technologies",
        "text": "It is imperative to leverage innovative technologies and approaches to ensure the continuity of censuses, household surveys, and other major statistical programmes.  This includes making use, to the fullest extent possible, of mobile connectivity, cloud computing, smart mobile devises, and other technological innovations that offer alternative means to ensure that activities around the capturing, validation, processing and dissemination of census and survey data can go on in the new environment of limited mobility of staff and of the population at large. \n\n"
    },
    {
        "level": 1,
        "id": "5woerok1",
        "title": "Establishing a project steering committee",
        "text": "To ensure the success of a cross-functional team, it is useful to establish a steering committee of senior managers, whose role is to ensure that the project is adequately resourced and that the project goals are aligned with functional and high-level organizational goals. \n\nThis **steering committee** should include a representative of each functional area involved, and should be responsible of assigning individual project tasks to the member of the cross-functional team that is better positioned to undertake it.  \n\n"
    },
    {
        "level": 2,
        "id": "mmwn1oEU",
        "title": "Establishing a national project team",
        "text": "Each national data innovation project requires dedicated personal, both from the NSO and from other participating agencies, to carry out the work at the country level.  The national data innovation project team should consist of:\n\n- A national project coordinator (bridge between technical team at NSO and other stakeholders)\n- Two statisticians (closely familiar with source data and statistical analysis prcesses)\n- One econometricians (with expertise in nocasting and/or small area estimation techniques)\n- One GIS expert (with expertise in GIS analysis and map visualization tools)\n- One IT focal point (with admin rights and familiar with data security protocols)\n- Two policy experts\n\n"
    },
    {
        "level": 3,
        "id": "Vk1c3Bjw",
        "title": "Allocation of staff to data innovation projects",
        "text": "Risk: Lack of time availability of skilled technical staff\n\nMitigation: To be successful, any data innovation project must have dedicated staff, who must be freed up from other duties to participate in it (trainings, data analysis, coordination) \n\n"
    },
    {
        "level": 1,
        "id": "47pZOC1r",
        "title": "Product vs. Process Ownership",
        "text": "\"Owning a process\" requires a different perspective than the more familiar \"owning a product\"\n\n"
    },
    {
        "level": 2,
        "id": "3ETVkGDf",
        "title": "Total cost of ownership",
        "text": "The \"total cost of ownership\" is an estimate of the direct cost of developing and applying a particular system.  It includes:\n\n- Software/hardware costs\n- Operational costs (including maintenance of physical infrastructure)\n- Personnel costs (including hiring and training costs of both permanent and temporary staff)\n\n"
    },
    {
        "level": 3,
        "id": "U3iX23IM",
        "title": "National ownership of data innovation projects",
        "text": "National ownership of data innovation projects requires local staff to have the ability and confidence to make the right choices regarding sources, technology and methods in real situations.\n\nCapacity development projects try to make learning easier for NSO staff by prearranging information, sorting it into modules, categories and themes, However, they achieve the opposite if they take away the opportunity to build meaningful connections to their needs and context.  \n\n"
    },
    {
        "level": 1,
        "id": "3jLAEhY0",
        "title": "Importance of funding from national governments",
        "text": "Over the long term, building institutional capacity requires government funding.\n\nData innovation projects need to secure financial resources at the country level:\n\n- specific budget allocations\n- grants from external donors\n- shared resources from existing projects\n\n"
    },
    {
        "level": 2,
        "id": "QGyxiutK",
        "title": "Key principles of data innovation projects",
        "text": "1. Country ownership\n\nWithout country ownership, and funding, a data innovation project will not survive very long. Decision makers in government and senior NSO officials need to be convinced that the project will help drive the national strategy for the development of statistics and other national policy agendas. \n\n2. Sustainability\n\n3.  Long-term relevance\n\nAdequate planning of data and infrastructure is essential to ensure country ownership, sustainability and long-term relevance. \n\n"
    },
    {
        "level": 3,
        "id": "zGHMrgZh",
        "title": "Capacity building and sustainability",
        "text": "One of the main objectives of capacity building is to enable local staff to become technically **self-reliant** and to gain **confidence** and a strong sense of **ownership**.\n\nLocal staff should be able to understand and to explain to others not only the final outputs, but all aspects of the process, from beginning to end.\n\nCountry officers need to be able to replicate and update the outputs on a regular basis by their own means.\n\n"
    },
    {
        "level": 3,
        "id": "CNiPnq0T",
        "title": "Long-term relevance of data innovation projects",
        "text": "**Risk:**\nProject outputs and the data sources, methods and technologies used to generate them can become quickly outdated, due to rapid social, economic, environmental or technological changes.\n\n**Mitigation**:\n\n- Setup a continuous maintenance programme to keep IT infrastructure and systems up to date (e.g., software updates)\n- Update and release new results as new data inputs become available\n\n"
    },
    {
        "level": 3,
        "id": "n3KkkMdF",
        "title": "Mainstreaming projects:  sustainability",
        "text": "When data innovation projects are viewed only as ad hoc activities, it is very difficult to secure long-term resources.   To be sustainable, data innovation projects need to be aligned with over-arching strategic plans and be embedded within the organization's existing multi-year programme of activities. This includes ensuring that the project activities are funded through regular budget sources and that they are adequately staffed over the long term. \n\n\n\n"
    },
    {
        "level": 1,
        "id": "HZvkseXI",
        "title": "Delivering methodology as a blackbox",
        "text": "Some aspects of data innovation projects are highly technical.  There is a risk of \"delivering methodology as a black box\", making it impossible for local teams to maintain and update the project results in the long run.\n\n"
    },
    {
        "level": 2,
        "id": "1j2ClWxr",
        "title": "Understanding the how and the why",
        "text": "Only after we understand why and how new technologies and methods work, are we able to tweak them for our own needs.\n \n"
    },
    {
        "level": 3,
        "id": "DLdh6sOz",
        "title": "Nothing is more practical than a good theory",
        "text": "Facts need to \"hang together on a latticework of theory\" in order to provide  insights that can be used to systematically solve real-world problems.\n\n"
    },
    {
        "level": 1,
        "id": "XVq7u8bY",
        "title": "Poverty map production: basic steps",
        "text": "Estimation of small-area poverty maps usually follows methodology developed by the World Bank over more than 20 years, combining census data with household survey data. This methodology usually includes the following general steps:\n\n1. Verify quality and comparability of Household survey data and census data\n2. Verify quality of additional geospatial co-variates\n3. Estimate a model of household consumption, based on data sample from household survey (using only variables that are available in both survey and census data) and geospatial co-variates.\n4. Apply estimated parameters to census data and geospatial co-variates in order to estimate consumption per capita for all households in the census\n5. Apply appropriate poverty lines to estimate poverty rates at various levels of aggregation\n6. Build confidence intervals using standard errors\n7. Use GIS to produce visualizations of the results\n\n\n"
    },
    {
        "level": 2,
        "id": "RHCUOmoR",
        "title": "Use of EO in poverty mapping",
        "text": "Poverty mapping projects typically use geospatial datasets derived from high-resolution EO imagery, including:\n\n- Land cover\n- Land use\n- Objects (cars, buildings, ...)\n- Night-time lights\n- Road networks\n- ...\n\n"
    },
    {
        "level": 3,
        "id": "Cr014GcV",
        "title": "Fundamental Geospatial Data Themes",
        "text": "The 14 Global Fundamental Geospatial Data Themes adopted by UNGGIM at its seventh session under decision 7/104 provide a \"taxonomy\" of the geospatial data assets that are needed to enable \"the measurement, monitoring and management of sustainable development in a consistent way over time and to facilitate evidence-based decision making and policy-making.\" \n\nThey can be used to facilitate global geospatial information management and the implementation of the integrated geospatial information framework.\n\n1. Global geodetic reference frame* \n2. Addresses\n3. Buildings and settlements\n4. Elevation and depth\n5. Functional areas\n6. Geographic names\n7. Geology and soils\n8. Land cover and land use\n9. Land parcels\n10. Orthoimagery\n11. Physical infrastructure\n12. Population distribution (including population characteristics)\n13. Transport networks\n14. Water\n15. Economy*\n\n(*) Additional theme not included in the 14 GFGDT\n\n\n"
    },
    {
        "level": 3,
        "id": "vWWASKo8",
        "title": "Additional geo-referenced information used in poverty estimation",
        "text": "- Points of service delivery (e.g., schools, health centers, boreholes...) and their attributes (e.g., number of beds in hospitals; number of health personnel)\n- Networks of infrastructure (e.g., roads, electricity, water...) and their attributes (e.g., condition/quality of roads)\n- Natural features (e.g., elevation, agroclimatic characteristics...)\n\n"
    },
    {
        "level": 2,
        "id": "5Y5WIm31",
        "title": "Poverty maps production: intermediate indicators",
        "text": "Poverty map production usually requires the computation of intermediate indicators to be used as covariates in poverty estimation methods:\n\n- Distance to nearby markets\n- Distance to nearby cities\n- Distance to roads\n- Distance to service delivery points (e.g., schools, health centers...)\n...\n\n"
    },
    {
        "level": 3,
        "id": "vlQ3wkHh",
        "title": "Gaps in poverty data",
        "text": "Traditional poverty measures are generally available at low frequency intervals (e.g., every ten to five years) and only at highly aggregated levels of granularity\n\n>*As of \\[ \\], \\[  \\] countries have not poverty data at all for the period 2005-2019, and \\[   \\] have only one data point.   For half of in the global database, the most recent data point is for the year \\[  \\] or earlier. *\n\n\n"
    },
    {
        "level": 1,
        "id": "F3sZj5cS",
        "title": "Early and continuous integration",
        "text": "Integration and testing activities are crucial to verify whether components being developed in multiple parallel tracks meet customer expectations.   Continuous integration is critical to validate assumptions about how different components interface with one another and address any issues early on.\n\n"
    },
    {
        "level": 2,
        "id": "IZQ63Vb3",
        "title": "An agile approach to measuring progress",
        "text": "The agile way of measuring progress is by delivering working, validated assets to the customer, not by whether we are proceeding according to a per-defined plan. \n\nFinishing on time and on budget according to a per-specified plan is not a measure of success; the true measure of success is the amount of customer-valuable work that is delivered (value-centric delivery)\n\n\"A project in production can be a failure.  Delivering a product does not guarantee success.\"\n\n"
    },
    {
        "level": 3,
        "id": "UdC6b3Tq",
        "title": "Work in Progress (WIP)",
        "text": "Work in Progress is work that has been started but is not yet finished.  The inventory of work in progress is a critical variable agile management.  The focus should be on reducing idle work and establishing a steady workflow.  \n\nThe cost of change is higher when there is too much work in progress. \n\nThe benefits of working in smaller batches include:\n\n- Smaller amounts of work are waiting to be processed \n- Reduced flow variability\n- Accelerated feedback\n- Less impact of errors\n- Reduced complexity\n- Increased focus and sense of responsibility\n\n"
    },
    {
        "level": 1,
        "id": "LjcB50KJ",
        "title": "Code switching",
        "text": "Code-switching: Ability to rapidly and unconsciously go back and forth between different languages. \n\n\"Different languages ... embody different worldviews and different ways of organizing the world around us.\"\n\nLanguage can subtly affect people's visual perception and sense of time.   For example, bilinguals seem to \"think differently about time, depending on the language context in which they are estimating the duration of events.\"\n\n- English, Swedish and Spanish speakers usually visualize the future as in front of us, and the past as behind us. \n- For Aymara speakers, the future is behind, the past is ahead. \n- For Mandarin speakers, the future is down, the past is up.\n\n\n"
    },
    {
        "level": 2,
        "id": "Gvu1WMA8",
        "title": "Semantic interoperability",
        "text": "Semantic interoperability is about making the meaning of the various components of a dataset, document or information product visible and accessible to users (both humans and machines), in a standardized manner, so as to enable the decentralized creation of knowledge and insights through high-value information services.\n\n"
    },
    {
        "level": 2,
        "id": "Upsh1o2P",
        "title": "Perception: Top-down vs Bottom-up",
        "text": "Our perception of the world is \"a virtual reality constructed inside our heads\" \"Perceptions ... are more about what the brain expects to encounter than what is truly there.\"\n\nThe human brain is not just a passive receiver of information, but is constantly making predictions about the world.  When sensory input does not match up with accumulated experience, the brain will often disregard the information it receives from the senses. \n\nThere are two types of perception: \n\n- **Top-down perception**: When we experience the world as the brain expects it to be\n- **Bottom-up perception**: When the brain faithfully represents sensory input it receives. \n\n\"Psychologies Gary Lupyan of the University of Wisconsin and Andy Clark of the University of Edinburgh argue that top-down perception ... is the brain's default method for engaging with the world.  [Only] when the brain isn't confident about its expectations...it depends much more on bottom-up perception.\"\n\nPerception-as-prediction influences all sorts of daily experiences\", from listening to music (music with familiar, predictable patterns is more pleasant) to our interpersonal relationships (we are constantly making predictions about what others will say or do, and only when they behave contrary to our expectations they catch our attention).\n\nLanguage creates expectations that influence our perception of the world.  \n\n"
    },
    {
        "level": 3,
        "id": "SifabErO",
        "title": "Cognitive overload",
        "text": "In cognitive psychology, cognitive load refers to the used amount of working memory resources. (Wikipedia)\n\n- Intrinsic: Associated with a specific topic\n- Extraneous: Related to how information or tasks are presented to a learner\n- Germane:  Related to the creation of a permanent store of knowledge, or a schema.\n\n"
    },
    {
        "level": 1,
        "id": "TsYS8WO6",
        "title": "Challenges of introducing new technologies in response to COVID-19",
        "text": "National Statistical Offices are being challenged to introduce telephone-based interviewing and web-based self-reporting techniques at once for many critical data collection operations--such as population and housing, agricultural, and economic censuses, as well as household, business and other types of surveys. In many cases, they need to do it without the benefit of prior experience and with very limited time to conduct detailed analysis and testing of the different alternatives.   \n\nThe introduction of these new technologies is risky and can be expensive. To make an informed decision on the type of technologies best suited to mitigate disruptions to data collection programmes, National Statistical Offices need to take into account their existing infrastructure, technical capacities and resources. \n\n"
    },
    {
        "level": 2,
        "id": "5gf8Ujfa",
        "title": "Testing of new IT systems for data collection",
        "text": "Statistical organizations are under huge pressure to accelerate the process of procuring, developing and deploying new IT systems in order to respond to the challenges of the COVID-19 crisis. \n\nHowever, in order to prevent further disruptions in critical statistical operations, the introduction of new IT systems for data collection, processing and dissemination requires that these systems be thoroughly tested with respect to:\n\n- Functionality\n- Usability\n- Integration\n- Accessibility\n- Security\n- Reliability / stress testing\n\n"
    },
    {
        "level": 3,
        "id": "oRq6vy7w",
        "title": "Re-purposing CAPI infrastructure to conduct CATI data collection",
        "text": "National Statistical Offices that have in place a data collection programme based on computer-assisted personal interviews (CAPI) may consider re-purposing the existing software and hardware infrastructure to support computer-assisted telephone interviews (CATI) instead.  This would allow to leverage the existing devices (tablets, personal digital assistants, smart phones or portable computers) as well as their specialized software, including their ability to instantly transmit data over mobile data networks.  \n\nHowever, this re-purposing is not trivial.  For instance, it requires to integrate CATI operations with existing digital mapping and operational management applications built under the assumption that enumerators/interviewers would be entering the data on the same location as the respondent. As interviewers will now be entering the information from a remote location, this creates new challenges for the automatic geo-coding of questionnaire responses and for the supervision of the interview process. \n\n"
    },
    {
        "level": 3,
        "id": "smZXjmav",
        "title": "On-site infrastructure",
        "text": "Many statistical organizations still use server infrastructure located in their own premises to support key functions, such as collecting source data from information providers, giving staff access to data management and data analysis software, and delivering statistical outputs to users.\n\nIf staff is not able to access the software tools and data needed to perform their work, statistical organizations face major disruption in essential workflows. \n\n"
    },
    {
        "level": 2,
        "id": "A23mYrDu",
        "title": "Challenges of remote interviewing and self-reporting",
        "text": "Even National Statistical Offices that have started using electronic data collection approaches, such as computer-assessed personal interviews (CAPI), still rely heavily on personal interviews, Some of the challenges in moving away from face-to-face interviews to remote interviews and self-reporting methodologies include:\n\n- Implementing new mechanisms to identify, contact, authenticate and communicate with respondents\n- Establish mechanisms to geo-locate responses obtained via remote interviewing and self-reporting.\n- Procure/develop and test IT systems to support computer-assisted telephone interviews and online self-reporting portals\n- Implement mechanisms to support respondents in completing online questionnaires\n- Implement mechanisms to support and oversee telephone-based interview workflows\n- Ensure secure remote access to IT systems and secure data exchange.\n\n"
    },
    {
        "level": 3,
        "id": "87In6UYV",
        "title": "Selecting alternative data collection approaches - time considerations",
        "text": "Time is a key factor in selecting alternative data collection approaches to ensure the continuity of statistical operations:\n\n- Estimated time necessary to procure/develop, test and deploy technical solutions \n- Estimated time that needs to be spent in training of staff in new skills and the use of new technologies\n\n"
    },
    {
        "level": 3,
        "id": "vIDWWZjJ",
        "title": "Communication strategy with respondents",
        "text": "It is crucial to design and launch as quickly as possible a contact and communication strategy towards respondents in the target population, aimed to maximize high response rates in a new web-based or telephone-based data collection setting.  This include communications soliciting households to complete online questionnaires or to be interviewed by telephone, sending reminders and follow-ups in case of non-response.\n\n"
    },
    {
        "level": 1,
        "id": "o2aK159Q",
        "title": "Slip box",
        "text": "- The slipbox is a simple external system to organize one's thought, ideas, and collected facts.\n- To be effective, it has to be embedded in one's overarching workflow (daily routine)\n- Video of Prof. Niklas Luhmann: https://www.youtube.com/watch?v=qRSCKSPMuDc&feature=youtu.be&t=37m30s\n\n"
    },
    {
        "level": 2,
        "id": "f6AE5k9M",
        "title": "Types of notes",
        "text": "1. **Fleeting notes**: Sever to capture \"raw\" ideas we come across. They are stored in one place for later processing.  They are only useful if reviewed and turned into proper notes within a day or so.\n2. **Literature notes**: Capture bibliographic details and brief description of sources.\n3. **Permanent notes**: Serve to develop ideas based on fleeting notes and literature notes.  Written in precise, clear, and brief full sentences. They can be understood even outside the context they were taken from.\n4. **Project notes**: Are only relevant to one particular project and can be discarded or archived after the project is finished\n\nTypical mistakes:\n- Treat every note as if it belongs to the \"permanent\" category\n- Collect notes only related to specific projects\n- Just collecting unprocessed fleeting notes\n\n"
    },
    {
        "level": 1,
        "id": "6SkgzEAZ",
        "title": "Iterative development",
        "text": "Teams usually get things wrong before they get them right, and initial results are often poor before they are good.  The number of iterations is not fixed from the beginning. but guided by feedback received after each iteration. \n\n"
    },
    {
        "level": 2,
        "id": "onHn6c4S",
        "title": "Iterative abstraction / re-specification",
        "text": "- Innovation requires to combine and re-combine ideas, liberating them from their original context by means of an iterative process of abstraction and re-specification. (Ahrens, 2017, p. 123).\n- Abstraction from concrete situations and re-specification allows to apply ideas from one practical context into another.\n\n"
    },
    {
        "level": 3,
        "id": "5WJqHcE5",
        "title": "Abstraction and scalability",
        "text": "Complexity increases when NSOs seek to mainstream data innovations beyond pilot projects run by a small team.  \"This makes enforcing ETL best practices, upholding data quality, and standardizing workflows increasingly challenging.\" \n\nIdentifying and automating common ETL patterns into standard workflows allows to leverage the power of abstraction in order to address scalability challenges.\n\nThis requires both governance and re-usable technologies\n- repositories (git)\n- notebooks (Jupyter)\n- containers (Docker)\n\n"
    },
    {
        "level": 3,
        "id": "d700Xe9w",
        "title": "Innovation is not a linear process",
        "text": "- The quest for innovation requires to constantly iterate between different tasks\n- The search for meaningful connections is a crucial part of any innovation processes (p. 114).\n- Any attempt to squeeze a non-linear process into a linear order only leads to problems and frustrations\n\n"
    },
    {
        "level": 1,
        "id": "FOPuLnHI",
        "title": "ETL principles",
        "text": "Principles of good ETL pipelines:\n\n- Partition data tables\n- Load data incrementally\n- Use imutable data tables - so queries return the same result when run against the same business logic and time range\n- Parameterize backfilling logic\n- Run early and frequent data checks: Write data into a staging table first, validate data quality, and only then push to final production table\n- Build alerts and monitoring system\n\n\n"
    },
    {
        "level": 2,
        "id": "Jwlw5emQ",
        "title": "A persistent and immutable staging area",
        "text": "By accumulating and persisting all source data in a stating area (keeping it forever unchanged) one can have shorter retention policy on derived tables, \"knowing that it\u2019s possible to backfill historical data at will.\"\n\n"
    },
    {
        "level": 2,
        "id": "5W8MgJOp",
        "title": "Data Partitioning",
        "text": "**Data Partitioning** - breaking up data into independent, self-contained chunks, instead of storing in a single table or file.\n\nIt is \"a practice that enables more efficient querying and data backfilling\" (Chang, 2018b)\n\n"
    },
    {
        "level": 1,
        "id": "2j4pK36q",
        "title": "Key stakeholders in data innovation projects",
        "text": "In every data innovation project, it is very important to identify from the beginning a broad range of actual and potential stakeholders who may become involved in various phases of the project. This includes:\n\n- **Data providers** (line ministries, land administration, mobile phone companies, industry regulators, business associations, local governments, civil society organizations, space agencies, tech companies...)\n- **Technology providers** (e.g., national data centers, intl. organizations, private sector)\n- **Knowledge and expertise providers** (e.g., international organizations, research institutes, universities...)\n- **Funding providers** (e.g., ministry of planning/finance, intl. donors)\n- **Data users** (e.g., line ministries, parliament, local governments, international organizations, civil society organizations, intl. donors)\n\nIt is also crucial to establish from the beginning both institutional and personal links with these stakeholders, and to involve them early on in the planning and execution of project activities.\n\n"
    },
    {
        "level": 2,
        "id": "UejoKjiC",
        "title": "User engagement in data innovation projects",
        "text": "It is important to build strong ties of collaboration between technical team and the user community. Representatives from key user groups need to be invited to be part of all project activities, from the planning stage on.  \n\n"
    },
    {
        "level": 3,
        "id": "Dyg391KX",
        "title": "Users of statistical data and their needs",
        "text": "Statistical organizations must identify their key audiences / user groups and regularly assess and prioritize their data and information needs. \n\nThe extent to which the needs of key user groups are met is the ultimate measure of every success of statistical organization, and thus needs to be monitored continuously and acted upon.\n\nThe Information and audience model of the UN Economic Commission for Africa identifies various user communities and their data requirements.   Broader audiences require data and information that are more aggregated and summarized (e.g., headline indicators and policy briefs). On the other hand, narrower/specialized audiences require detailed information and disaggregated data. \n\nTypes of audiences:\n\n- Decision makers \n  - Leaders from governmental, international, business, and civil society organizations\n  - Public officials, managers\n- General interest users\n  - The general public\n  - Students\n- Specialists\n  - Analysts\n  - Researchers\n  - Regulators\n\n"
    },
    {
        "level": 2,
        "id": "4RDMe98t",
        "title": "Technical experts",
        "text": "Data innovation projects need to identify and bring on board technical experts who are deeply knowledgeable of the data inputs, technologies, and methods being pursued, and with ample practical experience in their implementation in the field.\n\nIt is particularly important to partner early on with local experts (e.g., form UN Country Teams, World Bank, government agencies and local universities and think tanks), in order to stimulate interest in the project outcomes and learn more about country needs and priorities. \n\n\n\n"
    },
    {
        "level": 2,
        "id": "38O0h16q",
        "title": "Project champions",
        "text": "To be successful, data innovation projects need to identify champions willing to put their names behind them, who can mobilize resources and institutional support, as well as facilitate collaboration across different organizations.\n\nNational champions are especially crucial to help broker collaboration with key government agencies and partners. \n\n"
    },
    {
        "level": 1,
        "id": "3BC8Oyhi",
        "title": "Data dissemination policy",
        "text": "Data dissemination is \"the ultimate objective of a statistical system\", and statistical organizations need to develop and implement a coherent data dissemination policy that enables them to \"supply the right data to the right audience in the right format\".   Such data dissemination policy should include:\n\n- The **portfolio of statistical products** that will be targeted at different **user groups**\n- The **media** in which statistical data and information are delivered (e.g., paper-based or digital products and services). 4\n- The **distribution channels** for delivery aof those statistical products an services\n- The appropriate **timing** for release of statistical products\n- The model of communication (e.g., one-to-one vs. one-to-many)\n- The use of **data visualization and communication tools** (e.g., story telling)\n\nThe data dissemination policy needs to be regularly reviewed an updated, to respond to changes in user needs, technology, etc.  To this end, statistical organizations should **monitor the effectiveness of their data dissemination policy** through a set of well defined indicators (e.g., use statistics of online products derived from Google analytics). \n\n"
    },
    {
        "level": 2,
        "id": "EcF1VIBW",
        "title": "Public right to access official statistics",
        "text": "\nThe public at large should have access to the same information reported to regional and international agencies\n\n"
    },
    {
        "level": 2,
        "id": "DgHoUEzL",
        "title": "Statistical data reporting",
        "text": "Reporting by statistical agencies to international monitoring agencies often follows specific reporting requirements and formats.\n\nThe International Monetary Fund's General Data Dissemination System (GDDS) Special Data Dissemination system (SDDS) are standards for National, regional and international reporting.\n\n"
    },
    {
        "level": 2,
        "id": "CsAV96wd",
        "title": "Proliferation of data portals",
        "text": "A common problem is the proliferation and lack of integration of multiple data portals with similar functionalities. This leads to duplication of work, as well as to confusion among users.  \n\n"
    },
    {
        "level": 1,
        "id": "rb3VCsdF",
        "title": "Akoma Ntoso",
        "text": "A standard for techology-neutral electronic representation of parliamentary, legislative and judiciary documents in XML format.   Provides XML schemas to model the structure and semantic components of digital documents. \n\n"
    },
    {
        "level": 2,
        "id": "bzURsW8Z",
        "title": "Linked Open Data for statistical manuals and handbooks",
        "text": "*\"Embodying knowledge into a hard-cover book does not translate into useful and usable form in today's environment\"*\n\n\"Online publication\" is not about simply making available online the electronic counterparts of traditional paper publications, but about taking advantage of the opportunities provided by new Information and Communication Technologies (ICTs) and markup languages, including:\n- embedded cross-references (hypertextual access)\n- semantic search\n- cross-domain linking\n- automatic information extraction and knowledge elicitation\n- document life-cycle automation (from authoring to consumption)\n- creation of high-value information services / applications\n\nOne of the main challenges at present is the lack of uniformity in the preparation and structuring of methodological documents, including lack of shared citation and cross-referencing mechanisms.  There is a need to standardize methodological documents to support the exchange of information and collaboration. \n\n**Inter-linking statistical frameworks on the semantic web**\n\nThe objective is to ensure that the meaning and structure of the different elements of methodological documents are available in machine readable form to software applications.\n\n\n\n"
    },
    {
        "level": 3,
        "id": "Ben2OQZe",
        "title": "Document Management Lifecycle",
        "text": "Although it\u2019s common to view methodological documents as static items fulfilling a purpose at a single point in time (the time of publication). \n\nStatistical offices often treat the publication of manuals, handbooks, and other methodological documents as the primary objective, while managing the drafting process itself, or the process of accessing and using its contents, is given little thought.\n\nIncreasingly, however, methodological documents are dynamic knowledge assets, whose content needs to be continuously updated and made available through an evolving list of channels, following innovations in Information and Communication Technology. \n\nMost methodological documents share a common end-to-end lifecycle, fulfilling specific needs at each stage.\n\n1. Drafting / Creation\n\n2. Classification/categorization - determining and tagging the contents of the document so that it may be stored appropriately;\n\n3. Storage - migrating the document to a secure storage repository, from which it may be retrieved as needed;\n\n4. Use - Performing any tasks and actions that are required to ensure the document fulfills its intended purpose;\n\n5. Archiving and/or destruction - ensuring that inactive and/or expired documents are either archived or properly destroyed in order to reduce clutter and manage risks.\n\n"
    },
    {
        "level": 1,
        "id": "UyOKOyqS",
        "title": "COVID-19 and innovation",
        "text": "\"A global disaster on the scale of the pandemic spurs innovation at a much faster rate\", since resources and funding are \"made available to those with solutions that might usually be regarded with more scepticism\".\n\n\"scientific advances like mRNA vaccines are always the product of the collaborative efforts of many people with diverse skills and backgrounds\".\n\nAn urgent, common challenge of the scale of the pandemic makes everybody more willing to collaborate:\n\n\"it\u2019s not entirely surprising that the two first Covid-19 vaccines to announce phase 3 results were mRNA-based. They were first off the blocks because, as soon as the genetic code of Sars-CoV-2 was known (it was published by the Chinese in January 2020), companies that had been working on this technology were able to start producing the virus\u2019s mRNA\"\n\n\n\n"
    },
    {
        "level": 2,
        "id": "uhh1kRu3",
        "title": "Innovation starts with what you have",
        "text": "- Nobody ever starts from scratch. Innovation projects should start with what you *have*, and not with an unfounded idea about what the data, technology or methods that you are planning to acquire or develop might eventually provide.\n\n"
    },
    {
        "level": 3,
        "id": "CHr0Qnj2",
        "title": "Prioritize already available data and technology resources",
        "text": "Data innovation projects should prioritize and maximize the use of data sources and tools that are already under the control of the NSO and other members of the National Statistical System.\n\n"
    },
    {
        "level": 1,
        "id": "8TivtZUx",
        "title": "Study groups in MBA programmes",
        "text": "\"Study groups have become a rite of passage at M.B.A. programs, a way for students to practice working in teams. (...) Business schools around the country have revised their curriculums to emphasize team-focused learning.\"\n\n"
    },
    {
        "level": 2,
        "id": "QRWAHOzF",
        "title": "Working in pairs to ensure sustainable skill building",
        "text": "In the face of high staff turnover rates, organizations face the challenge of retaining over the long term the skills acquired during training activities and hands-on implementation activities. \n\nTo address this challenge, a good practice is to ensure that in every activity related to the implementation of a project feature, at least two team members take the lead and are jointly responsible for the deliverable, and that they work via pairing and review each other's work,\n\n"
    },
    {
        "level": 3,
        "id": "jwQ8qCBg",
        "title": "Loss of expertise through staff attrition",
        "text": "**Risk**: \n\nLoss of expertise through staff attrition or turnover\n\n**Mitigation**: \n\n- Maintain focus on institutional capacity rather than on individuals \n- Promote knowledge sharing and team collaboration (e.g., working in pairs and peer reviews)\n\n"
    },
    {
        "level": 1,
        "id": "t36XnVjf",
        "title": "Data security: the \"weakest link\" problem",
        "text": "'Security measures are often thwarted by the \"weakest link\" problem: If just one person responds to an attack, it may succeed.'\n\n"
    },
    {
        "level": 2,
        "id": "odZoIUPn",
        "title": "Data security: Preventing phishing",
        "text": "\"Phishing accounts for 90% of all data breaches--but an estimated 30% of fraudulent emails are opened nonetheless.\"\n\nNeed to encourage a reflective, analytic approach to cyber security, instead of simply a rules-based training that may result in automatic, careless decision making. \n\n\"Pause if an email requests action; consider the nature, timing, purpose, and appropriateness of the request; and consult a third party about any suspicions.\"\n\n\n\n\n"
    },
    {
        "level": 1,
        "id": "zBMhHUQc",
        "title": "Apache Hadoop",
        "text": "Open source framework for distributed storage and processing of large datasets across clusters of computers. \n\nIt is often used in the implementation of data lake architectures. \n\nHadoop consists of four main modules:\n\n1. *Hadoop Distributed File System (HDFS)* \u2013 A distributed file system with high fault tolerance and native support of large datasets.\n2. *Yet Another Resource Negotiator (YARN)* \u2013 Used to manage and monitor cluster nodes and resource usage, and to schedule jobs and tasks.\n3. *MapReduce* \u2013 An implementation of the MapReduce programming model for large-scale parallel computation on data. The **map task** takes input data and converts it into a dataset that can be computed in key value pairs. The output of the map task is consumed by **reduce tasks** to aggregate output and provide the desired result.\n4. *Hadoop Common* \u2013 Common Java libraries\n\n"
    },
    {
        "level": 2,
        "id": "sLFsKmFz",
        "title": "Computer cluster",
        "text": "A network of computers that are connected and work together to accomplish the same task. \n\nWikipedia: \"A computer cluster is a set of loosely or tightly connected computers that work together so that, in many respects, they can be viewed as a single system. Unlike grid computers, computer clusters have each node set to perform the same task, controlled and scheduled by software.\"\n\nComputer clusters provide for improved performance, availability and scalability.\n\n"
    },
    {
        "level": 1,
        "id": "uVrsmP2n",
        "title": "Synchronous tasks",
        "text": "Instructions are executed immediately in order.  While each operation is being executed, nothing else is happening. \n\n"
    },
    {
        "level": 2,
        "id": "N24sVqGE",
        "title": "Asynchronous tasks",
        "text": "Their execution does not block the main thread.  \n\n"
    },
    {
        "level": 3,
        "id": "YwXkKMRo",
        "title": "JavaScript promises",
        "text": "Promises help make sense of asynchronous behavior in JavaScript.  \n\nA promise is an objects that represents whether an operation is pending, has been completed, or has failed.\n\n-  Simple promises are handled with `fetch` and `.then()`\n- `Asynch` / `Await` is a special syntax to work with promises in a more comfortable fashion.  \n\n"
    },
    {
        "level": 3,
        "id": "KG4OwoAJ",
        "title": "Recursion",
        "text": "A technique that involve creating a function that recalls itself.\n\n- Often can be used instead of a loop\n- Works well with asynchronous processes (functions can recall themselves when they are ready).\n\n"
    },
    {
        "level": 1,
        "id": "RpDkqWDR",
        "title": "Babel - Tool for JavaScript compilation",
        "text": "Babel converts modern JavaScript code to mode widely compatible code before running it in the browser.  It makes possible to use the latest features of JavaScript right away.\n\nSee (https://babeljs.io/repl)\n\n\n"
    },
    {
        "level": 2,
        "id": "UhAj73Yn",
        "title": "React source code",
        "text": "Source code is the set of files that belong to a project that don't run in the browser. \n\n"
    },
    {
        "level": 1,
        "id": "UXewQVNc",
        "title": "Reading and note-taking",
        "text": "- Reading and thinking are the main task. \n- The goal is to understand and come up with new ideas. \n- The notes are just the tangible outcome.\n\n\"The ability to express understanding in one's own words is a fundamental competency\", the same as the ability \"to distinguish the important bits of a text from the less important ones.\" (Ahrens, 2017, p. 54). \n\n"
    },
    {
        "level": 2,
        "id": "ZyufNNxO",
        "title": "Smart note-taking",
        "text": "According to Ahrens (2017), smart note taking requires:\n- Reading a text with questions in mind and try to relate it to other possible approaches\n- Spotting the limitations of a particular approach\n- Seeing what is *not* mentioned\n- Interpreting particular information within the bigger frame or argument of the text\n- Thinking hard about how the main ideas of the text connect with other ideas from different contexts:  *\"Notes are only as valuable as the ... reference networks they are embedded in.\"* (Ahrens, 2017, p. 108).\n\n"
    },
    {
        "level": 3,
        "id": "FlUbxxop",
        "title": "Be flexible - don't cling to a fixed idea",
        "text": "- Don't cling to an idea if another, more promising gains momentum\n- Follow your interest and always take the path that promises the most insight\n- The more you become interested in something, the more likely it is that you will generate insights from reading, taking notes, and writing about it\n\n"
    }
]