[
    {
        "level": 1,
        "id": "oWg5LrNe",
        "title": "Responsibilities of the content analyst",
        "text": "The content analyst digs into the use analytics to measure content performance and create reports that help determine what works and what doesn't, in order to fine tune:\n- Content creation strategy (what type content should we create?)\n- Content promotion strategy (what content should we promote? What content promotion activities are most effective?)\n\n"
    },
    {
        "level": 2,
        "id": "i54AuJAT",
        "title": "Content promotion strategy",
        "text": "Once new content is published, there is need to promote content through social media and other channels.\n\nThe effectiveness of the content promotion strategy should be monitored through content analytics.  \n\n"
    },
    {
        "level": 3,
        "id": "1hLRzYSe",
        "title": "Content Production Team",
        "text": "A content production team is often an example of a cross-functional team, with members coming from different functional areas of the organization that  may include:\n- Content strategist\n- Content manager\n- Content writer(s)\n- Content editor(s)\n- Content analyst\n\n"
    },
    {
        "level": 3,
        "id": "ofQx7aBs",
        "title": "Responsibilities of the social media manager",
        "text": "- Plan the social media calendar\n- Write and schedule social media posts\n- Determine the strategy for responding to and interacting with social media users\n\n"
    },
    {
        "level": 3,
        "id": "bHaKr9ga",
        "title": "Types of content",
        "text": "There are many types of content:\n- Blogs\n- Emails\n- Videos\n- Infographics\n- Newsletters\n- Press releases\n- ...\n\n"
    },
    {
        "level": 3,
        "id": "uOWtzwd6",
        "title": "Content design principles",
        "text": "In order to produce structured content that efficiently meets the needs of known information-seeking audiences, content should be:\n\n- **Targeted**: Customized to the needs, areas of interest and questions of specific audience groups\n  - *Who is the target audience?*\n- **Useful**: People come to a site to get a job done\n  - *What do the members of the target audience actually need?*\n- **Usable**: Information should be easy to access, navigate, understand and act upon\n  - Clear and consistent formatting and presentation\n  - Support for different devices\n- **Findable**: Available in open formats (e.g., HTML), annotated with machine-readable metadata that enables search engines to figure out what the content is about\n- **Focused**: Short, on topic and self-contained, with most important information first\n- **Distinctive**: Offering a unique value proposition\n\n"
    },
    {
        "level": 3,
        "id": "4INbWGw6",
        "title": "COVID-19 response website types of users",
        "text": "Our content strategy needs to cater to multiple types of audiences. We need to know who are our target audiences.\n\n- Chief statisticians from national and international statistical organizations\n- Data experts/practitioners from national and international statistical organizations\n- Data experts/practitioners from UN Country Teams\n- Data experts/practitioners from civil society, academia and the private sector\n- Students at different education levels (high school/undergraduate/graduate)\n- Analysts / policy experts from national governments and international organizations\n- Donors providing funding for statistical capacity building\n\n"
    },
    {
        "level": 3,
        "id": "jHpeljkZ",
        "title": "Content strategy goals",
        "text": "- Increase *website traffic*\n- Make brand channels a destination for *organic traffic*\n- Increase *brand awareness*\n- Establish *brand authority*\n- Bring **value to users**\n  - Help users identify and understand the main challenges faced by the national and global statistical systems that result from the COVID-19 pandemic\n  - Help global and national statistical systems overcome these challenges\n\n\n"
    },
    {
        "level": 3,
        "id": "xFkKBQHT",
        "title": "Guest blogging",
        "text": "Guest blogs where third parties promote their own content allow to strengthen partnerships while attracting visitors to our website and making it visible to broader audiences.\n\n"
    },
    {
        "level": 2,
        "id": "1hLRzYSe",
        "title": "Content Production Team",
        "text": "A content production team is often an example of a cross-functional team, with members coming from different functional areas of the organization that  may include:\n- Content strategist\n- Content manager\n- Content writer(s)\n- Content editor(s)\n- Content analyst\n\n"
    },
    {
        "level": 3,
        "id": "PjMcjUgb",
        "title": "Team charter template",
        "text": "### Purpose\n\n- Why does our team exist?\n- What are we collectively working towards?\n- What are our personal goals?\n- What do we want to do in the next 15,30, 90 days?\n\n### Measures\n\n- What is the ultimate measure of success for the team?\n- How do we track progress towards our goals?\n\n### Roles\n\n- What are the roles we have in our team?\n- What are they responsible for?\n- Who will fill those roles?\n\n### Practices\n\n- How do we want to work together?\n- How do we communicate and meet?\n- What tools do we use?\n- How often do we revisit goals, retrospect, strategize?\n\n### Guardrails\n\n- What is safe to try?\n- How do we make decisions?\n- What rules do we want to put in place?\n- What other commitments do we have outside the team?\n\n"
    },
    {
        "level": 3,
        "id": "J4RjLkh4",
        "title": "Ad-hoc cross functional teams",
        "text": "Ad-hoc cross functional teams seek to integrate skills across organizational boundaries in order to accomplish a specific goal over a limited period of time. They are often called during a critical juncture to deal with a specific problem or explore a new opportunity.\n\nA critical success factor is having clearly defined, realistic outputs, timeline, and exit criteria for each phase of the project.  \n\nAd-hoc  cross functional teams are challenged by complex supervisory relationships and incentive structures, as their members formally report to different functional managers. This increases the need for **horizontal cooperation and coordination** among team members.\n\nTo  avoid turf battles and organizational power politics, the different members of an ad-hoc cross-functional team must be completely **focused on creating value to customers**.  Team members must therefore have adequate **incentives to collaborate** outside their own \"home teams\" in the organization.  This means that their performance needs to be measured and rewarded based on their contributions towards achieving the objectives of the cross-functional project.\n\nPutting together the **right mix of talents** is another key ingredient for the success of a cross-functional team.  Reluctance by managers from different functional areas to get personally involved and to commit their most qualified staff to a cross functional project team often leads to poor results. On the other hand, assigning key staff members to a cross-functional project is a clear signal of the commitment by senior management to the success of the project. \n\nAd-hoc cross functional teams must also have a clearly defined **\"sunsetting\" plan** for incorporating back people and the team outputs, innovations and learnings back into the organization's functional processes and structure when the time comes to dissolve the team.  In particular, it is important to identify opportunities for team members to take on new responsibilities based on the skills they developed while being part of the ad-hoc cross functional team.\n\n"
    },
    {
        "level": 3,
        "id": "S1rUnyfS",
        "title": "Responsibilities of the content editor",
        "text": "The role of the content editor is to make a writer's good work great, offering guidance and helping authors think differently about their topic and ensuring that only high-quality content gets published.\n\nThe content editor's responsibilities include: \n\n- Review all written materials to ensure that they:\n  - are free of errors\n  - comply with the style guide\n  - reflect the brand's voice \n  - meet the goals set in the content strategy\n- Suggest revisions\n- Approve publication\n\n\n"
    },
    {
        "level": 3,
        "id": "MMjEeFha",
        "title": "Skills required in a content manager",
        "text": "- Very good understanding of the needs of users\n- SEO and keyword research: Ability to identify topics and opportunities for improving organic traffic\n- Strong understanding of the organization's communication goals: Ability to generate meaningful topics and organize them in the editorial calendar\n\n"
    },
    {
        "level": 3,
        "id": "fVFIr67h",
        "title": "Digital product design",
        "text": "Digital product design is not only about visual layout and styling. It is also about content design.\n\n\"Design is putting form and content together\" (Paul Rand)\n\n"
    },
    {
        "level": 3,
        "id": "AP2e8YNz",
        "title": "Elements of a content strategy",
        "text": "A content strategy includes:\n- A list of **clear goals**\n- A method for measuring success\n- A competitor analysis\n- Persona development\n- A brand style guide\n- A list of types of content to be produced\n- An initial editorial calendar\n\n\n"
    },
    {
        "level": 3,
        "id": "Lb8FlQSj",
        "title": "Responsibilities of the content manager",
        "text": "Responsibilities of the content manager include:\n\n- Making day-to-day decisions about what should be pulbished, how and when\n- Explaining to content creators and project owners why a piece of content should be created\n- Finding topics that will perform well in various channels (blog articles, Twitter, email...)\n- Staying on top of the editorial calendar - overseeing that content getes finished and published on time\n- Knowing the workload of each content creator\n\n\n"
    },
    {
        "level": 2,
        "id": "bHaKr9ga",
        "title": "Types of content",
        "text": "There are many types of content:\n- Blogs\n- Emails\n- Videos\n- Infographics\n- Newsletters\n- Press releases\n- ...\n\n"
    },
    {
        "level": 3,
        "id": "G8wbe503",
        "title": "Content generation using smartphones",
        "text": "\"Smartphone-generated content may offer more ... accurate insights into consumer preferences.\"\n\n\"Tweets composed on smartphones [contain] higher proportions of first-person pronouns, references to family, ... negative emotional words [and display] a less-analytical writing style.\"\n\n\"People associate their smartphones with psychological comfort\"\n\n\n\n"
    },
    {
        "level": 3,
        "id": "i7m8d4YQ",
        "title": "Information architecture for the COVID-19 response platform",
        "text": "Guidance and resources provided through UNSD's COVID-19 response platforms could be organized according to the various phases of the statistical business process:\n- Data Collection\n- Data Validation\n- Data Processing \n- Data Analysis\n- Data Dissemination\n\nThey could also be organized according to the classification of statistical activities.\n\n\n"
    },
    {
        "level": 2,
        "id": "jHpeljkZ",
        "title": "Content strategy goals",
        "text": "- Increase *website traffic*\n- Make brand channels a destination for *organic traffic*\n- Increase *brand awareness*\n- Establish *brand authority*\n- Bring **value to users**\n  - Help users identify and understand the main challenges faced by the national and global statistical systems that result from the COVID-19 pandemic\n  - Help global and national statistical systems overcome these challenges\n\n\n"
    },
    {
        "level": 3,
        "id": "AGeVwwFM",
        "title": "Measuring value and impact of content",
        "text": "The value and impact of content is the extent to which it meets a defined **user need** and **business objective**.  \n\n"
    },
    {
        "level": 3,
        "id": "h1lCewmo",
        "title": "COVID-19 response website - analysis of users' goals",
        "text": "Once we know who our target audiences are, we need to know what are their own goals, and how we will be helping them attain those goals.  \n\nOur main constituency are National Statistical Offices and statistical offices of international organizations, whose main goals are to **(1) keep existing statistical programmes running**, and **(2) respond to new data requirements ** around the COVID-19 crisis from national and local governments, other institutional decision makers, and the public at large.\n\nThese goals translate into specific needs:\n\n- **Mobilize resources** to support regular statistical activities affected by the pandemic and to launch and run new statistical activities to satisfy new data demands\n- **Obtain access to methodological guidance** on how the use of new data sources, methods and technologies for the collection, processing, analysis, dissemination and communication of data and statistics\n  - Create opportunities for peer-to-peer **sharing of experiences** \n  - Provide access to relevant **training materials**\n  - Deliver **expert advice**\n\n- **Identify authoritative, reliable sources of data** to monitor the day-to-day evolution of the health crisis, to assess and monitor social, economic and environmental impact of the pandemic, and to inform recovery policies over the longer term\n- Identify **capacity building needs**\n- **Coordinate initiatives** that respond to the needs of national statistical systems\n  - Understand who is doing what across the global and national statistical systems\n\n\n"
    },
    {
        "level": 3,
        "id": "47lhW4l0",
        "title": "User-driven content design",
        "text": "\"Many organizations still design and create content based on what they *think* their audience wants, not on what the audience *actually* wants.\"  \n\n\"It's never really about [us]. ... People's interest in [a subject] doesn't begin and end with [our] content.  They come to look at [our] stuff because they need to [complete a task, find the answer to a question, or achieve a personal goal]\".\n\nContent design should be user-driven:\n\n - Focused on what users want\n - Using the same vocabulary/terminology as users \n\n\n\n\n"
    },
    {
        "level": 3,
        "id": "HGf6LS7H",
        "title": "Content syndication",
        "text": "Content syndication's principle can be summarized as \"write once, publish everywhere\".  It is an increasingly common approach in the multi-channel communications environment, which delivers content from a single source through diverse media in order to provide an array or experiences targeted at different audiences.\n\n"
    },
    {
        "level": 3,
        "id": "MNo9Ib3e",
        "title": "Forward-looking content design",
        "text": "Content should be \"**future friendly**\", i.e., it should be designed for longevity, maximizing the likelihood that it can be easily accessed, understood, stored, shared and re-used by multiple people and computer systems, and rendered and connected through multiple user interfaces and media, both today and in the future. \n\n"
    },
    {
        "level": 2,
        "id": "AP2e8YNz",
        "title": "Elements of a content strategy",
        "text": "A content strategy includes:\n- A list of **clear goals**\n- A method for measuring success\n- A competitor analysis\n- Persona development\n- A brand style guide\n- A list of types of content to be produced\n- An initial editorial calendar\n\n\n"
    },
    {
        "level": 3,
        "id": "tZWRLOUI",
        "title": "Elements of a results communication strategy",
        "text": "- Identify potential **users** and how the results of an innovation project help them achieve their goals.\n- Determine the **types of information products / formats / media** that are best suited to the needs of these users\n- Identify / develop **dissemination outlets** that can effectively bring the product of the project to the intended users.\n- Develop a clear policy specifying the terms under which users can access and utilize of both the results of the project and the underlying data inputs\n- Develop complete, concise and clear user documentation explaining the methodology and providing guidance on the appropriate interpretation of the results\n- Make all results available in all the languages in which the key user groups work (e.g., all languages in which the government works)\n\n"
    },
    {
        "level": 3,
        "id": "vs7XXxCE",
        "title": "User-centric web strategy",
        "text": "- **Understand the audience**\n  - Demographic characteristics\n  - What do they want from the website?\n  - Who are the \"best\" users (e.g., returning users, users who download and cite resources from the website, users who register to participate in events promoted in the website...)\n- Select content that is useful, interesting and compelling to key user groups\n- Provide an enjoyable user experience\n  - Uphold high quality writing and editing standards\n  - Provide a professional, consistent layout and presentation style\n  - Support fast and correct rendering in multiple types of devices\n- Continuously adapt to changing user profiles and to new types of content \n\n"
    },
    {
        "level": 1,
        "id": "TE8BuboA",
        "title": "Fact tables",
        "text": "A fact table in a dimensional model ... stores in a single place low-level measurement data from a business process.  \n\n- All the measurement rows in a fact table must be at the **same grain** in order to avoid inappropriate double counting.  \n- Fact tables tend to be **long-form** (deep in number of rows, narrow in number of columns)\n- By including only true activity (available data) fact tables tend to be **sparse**.\n- Fact tables have **two or more foreign keys** that connect to the dimension tables' primary keys.  \n- A fact table is **accessed via the dimension tables** joined to it\n- A fact table has generally its **own primary key, composed of a subset of foreign keys** (the rest of the dimensions take on a single value)\n\n\n"
    },
    {
        "level": 2,
        "id": "b9neMQ7B",
        "title": "Extensibility of the dimensional model",
        "text": "A dimensional model can be extended by:\n- Adding **new dimensions** to the schema, as long as a single value for that dimension is defined for every existing fact row\n- Adding new **rows to the fact table**, assuming that the level of detail is consistent with the existing records\n- Adding new **attributes to a dimension table**\n\n"
    },
    {
        "level": 3,
        "id": "nOMscG6O",
        "title": "Enterprise data warehouse bus architecture",
        "text": "Incremental architectural framework for building the data warehouse / business intelligence system of an enterprise\n\n- Aims to deliver integration via standardized conformed dimensions that are re-used across processes\n- It is technology- and database platform-independent\n\n"
    },
    {
        "level": 3,
        "id": "UHxZBMU2",
        "title": "Data Modeling",
        "text": "**Data Modeling** is \"a design process where one carefully defines table schemas and data relations to capture business metrics and dimensions\". (Chang, 2018b)\n\nData modelling is about optimizing data structures for the purpose at hand. \n\n- Data modelling patterns used in business intelligence or for analytic purposes often involve sacrificing data normalization (i.e., accepting more data redundancy and more complex ETL pipelines to maintain) in order to facilitate data queries from tables where metrics and dimensions are already pre-joined.\n\n"
    },
    {
        "level": 3,
        "id": "6OomPFuc",
        "title": "Star schema",
        "text": "Data warehouses generally implement a simple **star schema**, which consists of a fact table linked to multiple dimension tables.  \n\nA star schema design helps balance between ETL maintainability and ease of analytics.\n\n*Fact tables* - Contain the business metrics of interest\n*Dimension tables* - Contain slowly changing attributes (often organized in a hierarchical structure) that can be joined with the fact tables\n\n"
    },
    {
        "level": 3,
        "id": "vgy29Bfz",
        "title": "Dimensional tables",
        "text": "Dimensional tables contain a single primary key and multiple attribute columns providing with textual context associated with the measurements in a fact table.  \n\nThey serve as the primary source of:\n\n- query constraints\n- groupings\n- consistent report labels\n\nDimension tables are highly denormalized. They typically store redundant hierarchical descriptive information in the spirit of ease of use and query performance.\n\nDimension attributes should consist of **human-readable, verbose text**, rather than cryptic abbreviations.  In particular, standard decode values (labels) should be available as dimension attributes.\n\n\"Robust dimension attributes deliver robust analytic slicing-and-dicing capabilities\"\n\n"
    },
    {
        "level": 3,
        "id": "u1OSnHln",
        "title": "Enterprise data warehouse bus matrix",
        "text": "Tool for designing and communicating the enterprise Data Warehouse bus architecture.  It indicates which dimensions are associated with a given business process. \n\n- **Rows**:  Business processes (e.g., domains)\n- **Columns**: Dimensions\n\nIt allows to explore, for instance:\n\n1. Whether a candidate dimension (column) is well-defined for a given business process (row)\n2. Whether a particular dimension (column) should be conformed across multiple business processes (rows)\n\nIt also allows to prioritize specific Data Warehousing / Business Intelligence projects, by deciding which row of the matrix to implement first.\n\nEach row can be expanded to show specific fact tables.\n\n"
    },
    {
        "level": 3,
        "id": "3BmAWMzw",
        "title": "Changing data warehouse dimensions over time",
        "text": "In order to model changing dimensions in functional data warehouses without mutating data, one could use a collection of \"dimension snapshots\", whit each snapshot containing the full dimensions available at a specific point of time.\n\n \n\n\n"
    },
    {
        "level": 3,
        "id": "BNCuT1mI",
        "title": "Disaggregation and dimensionality",
        "text": "The most granular data has the most dimensionality.  Disaggregated data is the most expressive data; fact tables should be designed to handle unexpected user queries at the most disaggregated level.\n\n"
    },
    {
        "level": 3,
        "id": "5I864IRH",
        "title": "Benefits of a dimensional schema",
        "text": "Dimensional modeling is the preferred technique for presenting analytic data.  Due to its reduced number of tables and the availability of meaningful descriptions, the dimensional schema:\n\n- Makes it easy for users to navigate and understand the data.  \n- Allows the data to be processed more efficiently using fewer joints (fast query performance)\n\nMoreover, there is no built-in bias regarding the types of queries that can be made on the data (each dimension provides an equally valid entry point into the fact table).\n\n\"One of the marquee successes of the dimensional modeling approach has been to define a simple but powerful recipe for integrating data from different business processes.\"\n\n\"A simple dimensional structure matches the fundamental human need for simplicity\"\n\n"
    },
    {
        "level": 2,
        "id": "UHxZBMU2",
        "title": "Data Modeling",
        "text": "**Data Modeling** is \"a design process where one carefully defines table schemas and data relations to capture business metrics and dimensions\". (Chang, 2018b)\n\nData modelling is about optimizing data structures for the purpose at hand. \n\n- Data modelling patterns used in business intelligence or for analytic purposes often involve sacrificing data normalization (i.e., accepting more data redundancy and more complex ETL pipelines to maintain) in order to facilitate data queries from tables where metrics and dimensions are already pre-joined.\n\n"
    },
    {
        "level": 3,
        "id": "5k2KowcP",
        "title": "Business intelligence vs. transaction data processing",
        "text": "- Transaction processing typically deals with a few records at a time.  It is process-oriented, and relies on current data at the individual-record level.  \n- Business intelligence processing may deal with thousands or millions of records at a time. It is subject/topic-oriented, and relies on historical data at both the individual-record and aggregate/summarized levels. \n\n"
    },
    {
        "level": 3,
        "id": "8Yks1b9t",
        "title": "Modeling real-world relationships",
        "text": "When designing a domain model, it is important to capture the real-world relationships between the objects within and outside the domain.  \n\n- It is easier to make sense of any object by its real-world relationships to other known things. \n\n> \"You only understand something relative to something you already understand\" (Richard Saul Wurman)\n\n"
    },
    {
        "level": 3,
        "id": "IVjGuRmD",
        "title": "Challenges in semantic modeling",
        "text": "\"Semantic modeling is hard, because human language and perception is full of ambiguity, vagueness, uncertainty, and other phenomena, that make the formal and universally accepted representation of data semantics quite a difficult task.\"\n\nSemantic modelling requires clear criteria to capture facts as objective instances of discrete categorizations and relationships that remain valid in all relevant contexts.  A common pitfall in semantic modeling is to present subjective knowledge as objective.  \n\n**Vagueness**: Absence of sharp criteria to establish whether a predicate is applicable or not in a specific case. \n\n**Uncertainty**: When a statement's truth cannot be established due to lack of sufficient knowledge\n  - *Explicit uncertainty*: The statement contains keywords such as \"probably\", \"might\", \"perhaps\", etc. that indicate lack of absolute certainty. \n  - *Implicit uncertainty*: The statement is expressed in certain terms, but there is still reason to doubt it. \n\n**Ambiguity**: When the same piece of information can be plausibly interpreted in more than one way. \n\n"
    },
    {
        "level": 3,
        "id": "btA7ZkfT",
        "title": "Dealing with dimension hierarchies",
        "text": "- **Fixed-depth hierarchies**: When the hierarchy levels have agreed-upon names, they should appear as separate attributes in the dimension table\n\n- **Variable-depth hierarchies**: If the range of the levels is small, they can be forced into a fixed-depth design with separate dimension attributes for the maximum number of levels\n\n- **Ragged hierarchies of indeterminate depth**: Use a bridge table that contains a row for every possible path in the ragged hierarchy (this enables all forms of hierarchy traversal to be accomplished with standard SQL)\n\n"
    },
    {
        "level": 3,
        "id": "v7IESwgy",
        "title": "Operational codes or identifiers with embedded meaning",
        "text": "When identifiers or operational codes have meanings embedded in them, each embedded meaning should be presented to the user as a separate attribute with its corresponding user-friendly textual description.  \n\n"
    },
    {
        "level": 3,
        "id": "pRX0xiSF",
        "title": "Wiki-based ontology engineering",
        "text": "A semantically-enhanced wiki engine can by used to support the conceptual phase of the ontology engineering process\n\n- All stakeholders involved use the wiki as a collaborative tool to create, modify and discuss items of an ontology\n- Concepts are not only defined and described by a restricted team of experts; instead, this part of the process is open for everyone interested in the results\n- Based on the resulting concept descriptions, a formal version of the concept tree is built\n\n"
    },
    {
        "level": 3,
        "id": "lEHhU8Ma",
        "title": "ETL - Making data analysis-ready",
        "text": "The **'extract-tranform-load' (ETL)** pattern consists of the following steps:\n1. Extracting input data from their original sources\n2. Transforming them it into usable data structures through transcoding, filtering, joining, and aggregation operations\n3. Uploading the transformed data onto a controlled data management environment (such as a data warehouse)\n\nETL pipelines are used to **transform raw data into analysis-ready data**, i.e., making data from heterogeneous sources available to data analysis at a central location, following a specific schema, so they can easily run queries using SQL, feed the data into generic statistical packages, etc.\n\n**ETL tools** allow to pull or receive data from a source system,  perform modifications through a sequence of processing steps, and push the transformed data into a target destination.  \n\n**ETL scheduling** allows to run an ETL process at specific intervals to load raw data into a target system.\n\nOperational databases and other sources of data need to be cleaned, standardized and integrated.  A measure of success for the intermediate goal of preparing a set of analysis-ready data inputs is the ability to combine them in exploratory data visualizations, including map visualizations, \n\n\n"
    },
    {
        "level": 3,
        "id": "0hrzimEn",
        "title": "Conformed dimensions",
        "text": "Two fact tables are said to have \"**conformed dimensions**\" when they reference the same dimension table (or two dimension tables that have the same attributes and the same at attribute domains).\n\n When two fact tables have conformed dimensions, they can be easily integrated into a single report or analytical application.  This strengthens analytic consistency and reduces future development costs.\n\nTo be re-used across multiple fact tables, conformed dimensions must be defined collaboratively and follow common data governance rules.  \n\n\n"
    },
    {
        "level": 3,
        "id": "Vmp2TR2i",
        "title": "Data engineering curriculum for official statisticians",
        "text": "In the past, teams working on data innovation projects used to be able to get away with just knowing the basics of data storage and ETL infrastructure. Today, however, data innovation teams need to have a good understanding of how different types of databases can be set up, accessed and integrated, and how to setup and orchestrate the infrastructure and data management environments needed to develop, test and deploy different analytic methods. \n\nData innovation training curriculum for official statisticians should develop data engineering skills on the following topics:\n\n- **Data management and integration** \n    - Basic data modelling\n    - Transcoding and record-linking methods\n    - Statistical disclosure control methods\n    - Data warehousing / ETL pipeline design patterns and techniques\n- **Cloud computing**\n  - Understanding distributed computing\n  - How to evaluate and implement different cloud service models\n\n\n   \n\n"
    },
    {
        "level": 3,
        "id": "rqMQvKeV",
        "title": "SDMX information model vs. dimensional model",
        "text": "The SDMX information model provides a dimensional model for statistical data which differs from traditional data warehouse / business intelligence models.\n\nIn SDMX:\n\n- Fact tables can be of mixed granularity\n- Records in fact tables are usually not additive (their records can be aggregated using counts or averages only)\n- Queries usually return subsets of a fact table, and not aggregates thereof.\n\nWhen modeling statistical datasets, it is often not possible to fix ex-ante the granularity of the data, and it is difficult to enforce a uniform dimensional design across domains.  \n\n\n"
    },
    {
        "level": 3,
        "id": "spMEPkyg",
        "title": "Benefits of a schema-first approach",
        "text": "*\"The biggest challenge is to name things\"* (Rockowitz, 2020)\n\nBy having a well defined schema from the beginning, it is easier to:\n\n- Provide front-end developers the data they expect, in the formats they expect.\n- Implement a multi-channel content dissemination strategy (\"create-once-publish-everywhere\")\n- Create multiple user experiences targeted at different groups of users who are accessing the same content \n- Share content among independent websites and applications using common data formats and structures\n\n"
    },
    {
        "level": 3,
        "id": "Nc7QKzfk",
        "title": "Semantic modeling",
        "text": "\"In essence, semantic modeling is about linking words and terms to their senses.\"  \n\nA semantic model is an explicit representation of the meaning of the various elements of a data set, with the purpose of making the data understandable to humans and computer systems. \n\n- Semantic models consist of symbolic representations of knowledge and reasoning behavior \n- Semantic models aim to capture discrete facts and precise identities. \n\nBy linking the terms in a dataset to commonly-agreed, formal representations of knowledge, it is possible to obtain more meaningful results from data science applications. \n\n\n\n"
    },
    {
        "level": 3,
        "id": "XCDHEkDl",
        "title": "Subject analysis",
        "text": "The analysis of an information resource in order to identify what it is about.  \n\nSubject analysis is a matter of interpretation and depends both on the purpose at hand and on the available terms. \n\n"
    },
    {
        "level": 3,
        "id": "PKN0Keue",
        "title": "Metadata schema",
        "text": "A set of rules specifying the attributes that can be used to describe an information resource. \n\n"
    },
    {
        "level": 3,
        "id": "kdFuxQhH",
        "title": "Shrunken dimensions",
        "text": "A shrunken dimension is a (conformed) subset of rows and/or columns of a base dimension.  They are used to construct aggregate fact tables and allow to capture data at a higher level of granularity than the original fact table (e.g., forecasts by year and region instead of more disaggregated data by month and country). \n\n"
    },
    {
        "level": 3,
        "id": "DewbALxg",
        "title": "Schema-first strategy for content creation and management",
        "text": "A schema-first approach focuses first on structure, then on content, and finally on interfaces.  It requires conducting research in order to determine how best to formally represent the concepts, relationships and rules that constitute a particular domain of knowledge.  \n\nThe objective is to establish an framework for planning, designing and creating \"content as structured data\", so it can be efficiently and flexibly published to different users interfaces. \n\nStructure creates understanding by identifying, contextualizing, categorizing and interconnecting the individual data and information elements that belong to a common subject domain.   Moreover, by using common structural patterns across organizational and domain-area boundaries, it is possible to establish meaningful connections to other sources of content on the web. \n\n"
    },
    {
        "level": 3,
        "id": "WTSY6AIG",
        "title": "Multiple hierarchies in dimensions",
        "text": "A dimension may contain more than one natural hierarchy such as (day/week/fiscal period) and (day/month/year)\n\nLocation-intensive dimensions may have multiple geographic hierarchies.\n\nMultiple hierarchies can co-exist in the same dimension table.\n\n"
    },
    {
        "level": 3,
        "id": "OkCaKf6I",
        "title": "SDMX is the shipping container of official statistics",
        "text": "Paraphrasing https://trello.com/c/RnpNCnXf, \"\\[SDMX\\] is the shipping container of the \\[official statistics\\] world.  Instead of having different \\[mechanisms for the exchange of\\] different \\[datasets\\], everything goes into the same \\[multi-dimensional schema\\] and is standardized into the same format. (...) Everything is streamlined towards one thing only: \\[statistical data\\] that can be \\[easily exchanged with, and utilized by, users\\].\n\n"
    },
    {
        "level": 2,
        "id": "rqMQvKeV",
        "title": "SDMX information model vs. dimensional model",
        "text": "The SDMX information model provides a dimensional model for statistical data which differs from traditional data warehouse / business intelligence models.\n\nIn SDMX:\n\n- Fact tables can be of mixed granularity\n- Records in fact tables are usually not additive (their records can be aggregated using counts or averages only)\n- Queries usually return subsets of a fact table, and not aggregates thereof.\n\nWhen modeling statistical datasets, it is often not possible to fix ex-ante the granularity of the data, and it is difficult to enforce a uniform dimensional design across domains.  \n\n\n"
    },
    {
        "level": 3,
        "id": "roUFFqTS",
        "title": "Dimensional data modeling",
        "text": "Dimensional data modeling is the preferred technique for presenting analytic data.  \n\n"
    },
    {
        "level": 2,
        "id": "vgy29Bfz",
        "title": "Dimensional tables",
        "text": "Dimensional tables contain a single primary key and multiple attribute columns providing with textual context associated with the measurements in a fact table.  \n\nThey serve as the primary source of:\n\n- query constraints\n- groupings\n- consistent report labels\n\nDimension tables are highly denormalized. They typically store redundant hierarchical descriptive information in the spirit of ease of use and query performance.\n\nDimension attributes should consist of **human-readable, verbose text**, rather than cryptic abbreviations.  In particular, standard decode values (labels) should be available as dimension attributes.\n\n\"Robust dimension attributes deliver robust analytic slicing-and-dicing capabilities\"\n\n"
    },
    {
        "level": 3,
        "id": "1G7uNxim",
        "title": "Developing a shared vocabulary",
        "text": "The first step to gaining understanding is to define complex, vague or obfuscatory terms and acronyms. This helps people question assumptions, articulate meaning and align understanding.\n\n"
    },
    {
        "level": 2,
        "id": "6OomPFuc",
        "title": "Star schema",
        "text": "Data warehouses generally implement a simple **star schema**, which consists of a fact table linked to multiple dimension tables.  \n\nA star schema design helps balance between ETL maintainability and ease of analytics.\n\n*Fact tables* - Contain the business metrics of interest\n*Dimension tables* - Contain slowly changing attributes (often organized in a hierarchical structure) that can be joined with the fact tables\n\n"
    },
    {
        "level": 3,
        "id": "Wo49hyYS",
        "title": "Relational databases",
        "text": "Relational databases store data in tables with rows and columns. Each table has a pre-defined schema that strictly defines the number and type of attributes that can be stored in them, as well as the keys to identify and access specific rows.  Changes to the schema of a relational database are difficult and time-consuming, which makes them expensive to setup, maintain and grow. \n\nRelational database management systems (RDBMS)  use structured query language (SQL) statements to query and maintain the tables in a relational database.  \n\nRelational databases are optimized to ensure the relational integrity of the data, and to work with well known and well-structured datasets.  On the other hand, they are ill equipped to handle large, unstructured datasets. \n\n"
    },
    {
        "level": 3,
        "id": "Aslc8kRG",
        "title": "Data warehouse",
        "text": "Data warehouses are \"logically centralized data repositories where data from operational databases and other sources are integrated, cleaned up, standardized [and stored over the long run] to support business intelligence\".  They constitute \"central locations that data analysts ... can go to, to access all their data\"\n\nData warehouses implement data organization, access, and aggregation methods to support multidimensional views of the integrated data.  They are optimized for dealing with analytical queries (as opposed to transactional queries). In addition, they are designed for ease of understanding, so analysts can connect their analytic and visualization tools to them without having to invest much time understanding the underlying data structures. \n\nThe term Data Warehouse was coined by William Inmon in 1990, which he defined as \"a subject-oriented, integrated, time-variant and non-volatile collection of data in support of management's decision making process.\"   \n- *Subject Oriented:* Provides information about a particular subject.\n- *Integrated:* Merges data from multiple sources into a coherent whole.\n- *Time-variant:* Each data point refers to a specific time period.\n- *Non-volatile:* Data can be added but data is never removed from the warehouse. \n\n(Source: \"What is a Data Warehouse?\" W.H. Inmon, Prism, Volume 1, Number 1, 1995).\n\nData warehouses can run into problems with large volumnes of data -- they may need to truncate older data and keep only summary tables.  An query performance can be an issue.  Using a data warehouse database as the main interface for data is not optimal for some machine learning tools, since the data must be unloaded from the database before it can be operated on. \n\n"
    },
    {
        "level": 3,
        "id": "Iaba3RcP",
        "title": "Key role of data warehouses",
        "text": "A data warehouse is a place where data from different sources is stored in query-able forms, ready to be used by different users and applications to enable higher level analytics.  \n\nData warehouses make data science activities scalable. \n\n"
    },
    {
        "level": 2,
        "id": "roUFFqTS",
        "title": "Dimensional data modeling",
        "text": "Dimensional data modeling is the preferred technique for presenting analytic data.  \n\n"
    },
    {
        "level": 2,
        "id": "Aslc8kRG",
        "title": "Data warehouse",
        "text": "Data warehouses are \"logically centralized data repositories where data from operational databases and other sources are integrated, cleaned up, standardized [and stored over the long run] to support business intelligence\".  They constitute \"central locations that data analysts ... can go to, to access all their data\"\n\nData warehouses implement data organization, access, and aggregation methods to support multidimensional views of the integrated data.  They are optimized for dealing with analytical queries (as opposed to transactional queries). In addition, they are designed for ease of understanding, so analysts can connect their analytic and visualization tools to them without having to invest much time understanding the underlying data structures. \n\nThe term Data Warehouse was coined by William Inmon in 1990, which he defined as \"a subject-oriented, integrated, time-variant and non-volatile collection of data in support of management's decision making process.\"   \n- *Subject Oriented:* Provides information about a particular subject.\n- *Integrated:* Merges data from multiple sources into a coherent whole.\n- *Time-variant:* Each data point refers to a specific time period.\n- *Non-volatile:* Data can be added but data is never removed from the warehouse. \n\n(Source: \"What is a Data Warehouse?\" W.H. Inmon, Prism, Volume 1, Number 1, 1995).\n\nData warehouses can run into problems with large volumnes of data -- they may need to truncate older data and keep only summary tables.  An query performance can be an issue.  Using a data warehouse database as the main interface for data is not optimal for some machine learning tools, since the data must be unloaded from the database before it can be operated on. \n\n"
    },
    {
        "level": 3,
        "id": "YOp0ve5T",
        "title": "Changing data warehouse logic over time",
        "text": "Changes in data warehouse logic over time should be either \n- expressed with data (in the form of \"parameter tables\") using effective dates\n- captured in source control, so they can applied conditionally, allowing to build the full state of the data warehouse throughout all time periods, or\n\nBeauchemin (2018) illustrates this with the example of introducing a change in the way taxes are calculated in year t.  If a users \"back-fills\" data for year t-1, the change in tax calculation method should not be applied. \n\n"
    },
    {
        "level": 3,
        "id": "NPfQeP5J",
        "title": "Data integration",
        "text": "Data integration is the act of making data from disparate source systems align and conform to common standards, combining and presenting them in a unified view to users, so they are readily available for further value creation. Getting the raw data from various sources ready for modelling and analysis is usually 80 percent of the work.\n\nA key deliverable from a data integration effort is a set of source-to-target mappings (STTM).\n\nData integration is a critical task which should be tackled early in a data analysis project, way before development work begins.\n\nData integration relies on different patterns, including data replication, data virtualization, and ETL, which may or may not involve the physical transfer of data between storage systems. \n\nSuccessful data integration requires a thorough analysis of the overall system architecture, data stores, and business requirements. \n\nGood integration design is repeatable.  It also must include mechanisms for handling exceptions and monitoring the results of data integration tasks (e.g., providing users with regular success and error counts)\n\n"
    },
    {
        "level": 3,
        "id": "4HybrQML",
        "title": "Data integration: Single Point of Truth",
        "text": "Single Point of Truth (SPOT) or Master Data Management (MDM) system allows to reconcile data from multiple systems into a single data hub, while allowing users to trace back these reconciled data assets to their original sources. \n\nSuch a system provides a stable access point for analysis work aimed to generate value and insights from a variety of multiple data assets. \n\n"
    },
    {
        "level": 3,
        "id": "hKNO8E7M",
        "title": "Database management systems (DBMS)",
        "text": "Database management systems support the creation, use and maintenance of databases.  They provide efficient data storage and retrieval, as well as tools for data acquisition, maintenance, formatting and dissemination.\n\n"
    },
    {
        "level": 3,
        "id": "CvXRU020",
        "title": "Logical warehouse of data inputs",
        "text": "Most data innovation projects rely on a large variety of source data generated or compiled by many different governmental and non-governmental organizations.  These sources include many large sets of structured, semi-structured and unstructured data, ranging from earth observation data, call detail records, and sensor data, to microdata from administrative records and data from sample surveys or census programmes.\n\nAs data sources grow, performing data analytics with multiple databases can become inefficient and costly. Thus, as part of a data innovation project, it is necessary to establish \"logical warehouse\" of data inputs, a single point of entry providing access to **analysis-ready, geo-referenced data inputs** from multiple sources, organized according to a simple and commonly agreed taxonomy, such as the **fundamental geospatial data themes** \n\nA lot of work may be required to \"condition\" the different data inputs in order to make them ready for use and analysis, including the adoption of data **interoperability standards and best practices** across different data sources and systems.  \n\nUnderlying this logical warehouse of source data can be a mixture of enterprise data warehouses and data lakes, which work together to provide access to \"immutable' data inputs and allow to trace transformations at a specific step in the pipeline,  thus enabling to test and reproduce estimation results.  \n\n"
    },
    {
        "level": 3,
        "id": "Tw98NGDm",
        "title": "Determinants of success of data warehouse projects",
        "text": "- Focus on business needs\n- Present dimensionally structured data\n- Tackle manageable, interative projects\n\n"
    },
    {
        "level": 3,
        "id": "U1nPvhZy",
        "title": "Data virtualization",
        "text": "Data virtualization provides a single view of one or more data sources, allowing applications real-time access to to retrieve and manipulate data without imposing a single data model or having to specify how it is formatted or where it is physically located at the source.\n\nThe main objective is to provide quick and timely insights from multiple sources without having to embark on a major data project with extensive ETL and data storage\n\n- Reduce risks of data errors, \n- Avoid moving data around that may never be used\n- Bridge data across data warehouses, data marts, and data lakes without creating a new integrated physical data platform\n\nData virtualization uses various abstraction and transformation techniques to resolve differences in source and consumer formats and semantics. \n\nExisting data infrastructure can continue performing their core functions while the data virtualization layer just leverages the data from those sources. \n\n"
    },
    {
        "level": 3,
        "id": "ePGUzfl7",
        "title": "Non-relational (No-SQL) databases",
        "text": "Non-relational (No-SQL) databases are \"scheme-agnostic\". They support the storage and manipulation of large volumes of unstructured and semi-structured data.  It is not necessary to specify in advance the types of data that will be stored in a No-SQL database, as it can accommodate changes in data types and data schemas. \n\nNo-SQL databases are designed to distribute data across different nodes; as a consequence, in many cases data consistency is not guaranteed. \n\nSome types of No-SQL databases include:\n\n1. *Graph databases*: They represent data as a network of related nodes, and are particularly suited to analyze relationships between heterogeneous data points.\n\n2. *Document stores*: They store data in XML and JSON format, using the document name as key and the contents of the document as value.   These documents can contain many different value types and can be nested.  They are particularly useful to manage semi-structured data across distributed systems.   Examples include *MongoDB* and *CouchBase*.\n\n3. *Wide-column stores*: This type of database store data in column families or tables.  They are designed to handle very large volumes of distributed data.   Examples include *Cassandra*, *Scylla*, and *HBase*.\n\n4. *Key-value stores*: They store only key-value pairs and provide basic functionality for retrieving the value associated with a known key.  They work best with a simple database schema and when speed is important.  Examples include: *Redis*, *DynamoDB*, *Cosmos*.\n\n"
    },
    {
        "level": 3,
        "id": "68VjKPZb",
        "title": "Architectural design for a data innovation project",
        "text": "At the beginning of a data innovation project, it is necessary design the architecture of the data pipelines and data storage and management environments that will allow the intake of raw data from multiple sources and push them through various processing steps, culminating with the delivery of analytic outputs to the final users.  \n\nThis architectural design involves making decisions regarding technologies, platforms, and data management and analysis tools. This includes selecting the appropriate mix of on-premises and cloud infrastructure (depending on the type of data inputs and the different analysis and estimation methods that will be used in the project) as well as planning and design to incorporate the right type of data integration patterns (e.g., ETL, data virtualization...), storage strategies, and infrastructure/performance optimizations (e.g., streaming, in-memory, hybrid storage).\n\nThe engineering of data pipelines will need to cover both \"development\" and \"production\" environments, as well as a strategy for testing development operations before pushing them into production. \n\nMoreover, the data architecture should provide for data storage and data processing environments and tools that allow the members of cross-functional teams to collaborate in the development, testing, implementation and refinement of various data estimation methods.\n\nSome key requirements for the data architecture include:\n- Easy to maintain by the organization's own staff\n- Easy to replicate by people outside the project team\n\n\n"
    },
    {
        "level": 2,
        "id": "BNCuT1mI",
        "title": "Disaggregation and dimensionality",
        "text": "The most granular data has the most dimensionality.  Disaggregated data is the most expressive data; fact tables should be designed to handle unexpected user queries at the most disaggregated level.\n\n"
    },
    {
        "level": 2,
        "id": "5I864IRH",
        "title": "Benefits of a dimensional schema",
        "text": "Dimensional modeling is the preferred technique for presenting analytic data.  Due to its reduced number of tables and the availability of meaningful descriptions, the dimensional schema:\n\n- Makes it easy for users to navigate and understand the data.  \n- Allows the data to be processed more efficiently using fewer joints (fast query performance)\n\nMoreover, there is no built-in bias regarding the types of queries that can be made on the data (each dimension provides an equally valid entry point into the fact table).\n\n\"One of the marquee successes of the dimensional modeling approach has been to define a simple but powerful recipe for integrating data from different business processes.\"\n\n\"A simple dimensional structure matches the fundamental human need for simplicity\"\n\n"
    },
    {
        "level": 1,
        "id": "LeCLAkpp",
        "title": "Ontologies and knowledge bases",
        "text": "Ontologies provide \"the skeletal structure\" for explicitly representing the structure and content of knowledge bases. \n\n"
    },
    {
        "level": 2,
        "id": "j67aFtwq",
        "title": "Maintenance of Knowledge Organization Systems",
        "text": "Metadata schemas make reference to external artifacts such as controlled vocabularies, thesauri and ontologies.  All these KOS evolve over time, and need to be maintained independently of the metadata schemas that reference them.  \n\n"
    },
    {
        "level": 3,
        "id": "hJzMjaHq",
        "title": "Thesaurus",
        "text": "A thesaurus is a repository of domain-specific, cross-referenced terms, used to identify related meanings (e.g., synonyms and antonyms, broader and narrower terms) so the most appropriate one for the intended purpose can be chosen. \n\nA thesaurus usually includes also textual annotations (scope notes) that clarify the meaning and the context of the terms. \n\n"
    },
    {
        "level": 3,
        "id": "1DLZKOfK",
        "title": "Semantic web tasks",
        "text": "1. Create common metadata vocabularies (ontologies)  and maps between vocabularies that allow document creators to know how to annotate their documents\n2. Create documents annotated (marked up) with semantic metadata elements identified by URIs (e.g., title, description, creator...)\n3. Create web services and automated agents that perform tasks on the marked up documents\n  - Improve information retrieval (search engines)\n  - Facilitate the integration of information from different sources\n  - Provide decision making support\n  - ...\n\nThere is need for guidance for each of these tasks.\n\n"
    },
    {
        "level": 3,
        "id": "8H5GVsju",
        "title": "Ontologies",
        "text": "Ontologies are \"engineering artifacts that provide the domain vocabulary plus certain assumptions regarding the meaning of the vocabulary.\"  They are similar to taxonomies in that they represent relations among terms.  However, ontologies offer a much richer meaning-representation mechanism for the relationships among concepts.\n\nOntologies help link machine-processable content with human meaning. They provide a means to formalize and express a common understanding of a domain of knowledge in machine-processable form, so it can be communicated among organizations, individuals and software.  When a community of data producers and consumers commits to using a shared ontology, it provides them with a mechanism to represent and communicate knowledge in the context of specific interactions.   \n\n"
    },
    {
        "level": 3,
        "id": "u1sLOxVD",
        "title": "Controlled vocabulary",
        "text": "A controlled vocabulary is a list of preferred, pre-defined terms that can be used for categorizing content, building labeling systems, defining database schemas, etc. \n\n\n"
    },
    {
        "level": 3,
        "id": "loPY28wf",
        "title": "Taxonomy",
        "text": "A taxonomy is a set of controlled vocabulary terms, typically arranged in a hierarchical structure, which identify units of meaningful content in a given domain.\n\n"
    },
    {
        "level": 3,
        "id": "68TaDeSD",
        "title": "Ontology",
        "text": "A formal representation of the entities that are part of a specific domain of knowledge, and the relationships between them and other external entities. \n\nOntologies often use controlled vocabularies or other encoding schemes to name entities and relationships. \n\n"
    },
    {
        "level": 3,
        "id": "AmEjPu9O",
        "title": "Creating and maintaining ontologies",
        "text": "There are various semantic web languages for modelling and authoring ontologies e.g., RDF, OWL, SKOS).\n\nIt is key to choose the right semantic web language, based on the required levels of expressivity and an assessement of the complexity of the available tools.  \n\n"
    },
    {
        "level": 3,
        "id": "Hf4g15en",
        "title": "Lightweight vs. heavyweight ontologies",
        "text": "- **Heavyweight ontologies**: Make intense use of axioms to model knowledge and restrict domain semantics\n\n- **Leightweight ontologies**: Are predominantly taxonomies with few cross-taxonomical links and few logical relations between classes.\n\nLightweight ontologies are the most commonly occurring type in today's semantic web applications for knowledge management, document retrieval and data integration.\n\n"
    },
    {
        "level": 2,
        "id": "8H5GVsju",
        "title": "Ontologies",
        "text": "Ontologies are \"engineering artifacts that provide the domain vocabulary plus certain assumptions regarding the meaning of the vocabulary.\"  They are similar to taxonomies in that they represent relations among terms.  However, ontologies offer a much richer meaning-representation mechanism for the relationships among concepts.\n\nOntologies help link machine-processable content with human meaning. They provide a means to formalize and express a common understanding of a domain of knowledge in machine-processable form, so it can be communicated among organizations, individuals and software.  When a community of data producers and consumers commits to using a shared ontology, it provides them with a mechanism to represent and communicate knowledge in the context of specific interactions.   \n\n"
    },
    {
        "level": 3,
        "id": "locdYDfl",
        "title": "Metadata standards",
        "text": "There are various types of metadata standards, namely:\n\n1. Standards for data and metadata structures (schemas)\n2. Standards for data and metadata values (thesauri, controlled lists, ...)\n3. Standards for data catalogs (e.g., DCAT)\n4. Standards for data serializations and technical interchange\n\n"
    },
    {
        "level": 3,
        "id": "3GXVRbXt",
        "title": "Challenges of the semantic web",
        "text": "The adoption of semantic web metadata solutions represents a big challenge to both data providers and consumers.  These challenges include:\n\n- Deploying and maintaining dedicated IT infrastructure and specialized software applications\n- Building/acquiring specialized skills on the use richer, more complex metadata languages\n- Consensus building on the use of common / standardized knowledge organization systems\n\n\n"
    },
    {
        "level": 3,
        "id": "lfyBBLvY",
        "title": "Linked Data",
        "text": "An approach for publishing and sharing data on the web in which URIs are the key to link information resources, so that each link delivers useful information which in turn enables (human and automated) users to reach other related information. \n\nThis approach relies in the agreement among different communities to use common artifacts (Knowledge Organization Systems) such as vocabularies, thesauri and ontologies, to ensure semantic interoperability.\n\nThe main implementation technology behind Linked Data is the Resource Description Framework (RDF) to formalize semantic metadata. \n\n"
    },
    {
        "level": 3,
        "id": "VxjYzewa",
        "title": "Semantic web technologies",
        "text": "Semantic web technologies enable the encoding of concepts and relationships in the metadata about documents and other information resources, thus enabling user agents to make semantic inferences about them. \n\nThey include, for instance:\n\n- Resource Description Framework (RDF)\n- Web Ontology Language (OWL)\n\n\n"
    },
    {
        "level": 3,
        "id": "58g7bVyY",
        "title": "The semantic web is a socio-technical system",
        "text": "The semantic web involves not only technologies and tools, but also procedures, people and organizations.  It requires endorsement and adoption of technical standards by a user community.  \n\n- The adoption, survival and spread of particular metadata schemas and knowledge organization systems among a community of data producers and consumers is a social phenomenon, in which maturity and ease of use of the technical solutions is as important as the social nature of the web. \n\n"
    },
    {
        "level": 2,
        "id": "Hf4g15en",
        "title": "Lightweight vs. heavyweight ontologies",
        "text": "- **Heavyweight ontologies**: Make intense use of axioms to model knowledge and restrict domain semantics\n\n- **Leightweight ontologies**: Are predominantly taxonomies with few cross-taxonomical links and few logical relations between classes.\n\nLightweight ontologies are the most commonly occurring type in today's semantic web applications for knowledge management, document retrieval and data integration.\n\n"
    },
    {
        "level": 3,
        "id": "7es7Bwvk",
        "title": "Data catalog",
        "text": "A data catalog is a metadata management tool designed to help data users find, understand, access and use datasets efficiently.  It allows users to explore data autonomously, providing information about its context, quality and use cases.  \n\nData catalogs enable data sharing.  \n\nA data catalog typically includes the following functionalities:\n\n- Search engine (all the catalogued datasets are indexed and searchable)\n- Mechanisms to access the datasets\n- Ability to tag / categorize datasets using both standardized and custom taxonomies and vocabularies \n- Tools for maintaining metadata using standardized metadata templates\n- Ability to visualize links between multiple datasets\n- Mechanisms for sharing and commenting on datasets\n\n"
    },
    {
        "level": 2,
        "id": "68TaDeSD",
        "title": "Ontology",
        "text": "A formal representation of the entities that are part of a specific domain of knowledge, and the relationships between them and other external entities. \n\nOntologies often use controlled vocabularies or other encoding schemes to name entities and relationships. \n\n"
    },
    {
        "level": 3,
        "id": "PEtPbdMx",
        "title": "Good practices for ontology creation",
        "text": "- Make as few claims as possible about the domain being modelled\n- Use complete, objective definitions\n- Capture consensual knowledge accepted by a group\n- Document all definitions with natural language\n- Specify concepts independently of symbol-level encoding\n- Define new terms based on existing vocabularies\n\n"
    },
    {
        "level": 3,
        "id": "OpfIieXJ",
        "title": "Thesaurus",
        "text": "A thesaurus governs the relationship among various terms:\n\n- parent-child\n- synonymy/antonymy\n- broader/narrower \n- \"use for\" (preferred term)\n\n"
    },
    {
        "level": 3,
        "id": "Gvu1WMA8",
        "title": "Semantic interoperability",
        "text": "Semantic interoperability is about making the meaning of the various components of a dataset, document or information product visible and accessible to users (both humans and machines), in a standardized manner, so as to enable the decentralized creation of knowledge and insights through high-value information services.\n\n"
    },
    {
        "level": 3,
        "id": "n0A04PjF",
        "title": "Encoding scheme",
        "text": "Rules specifying how to construct the value of an attribute-value pair in a metadata statement.  \n\nThere are 2 types of encoding schemes:\n\n- Syntax encoding: Specifies how the string used as an attribute value must be formatted (e.g., data format)\n- Controlled vocabulary: Specifies a finite set of strings that may be used as a value of an attribute of a metadata schema (e.g., code list)\n\n"
    },
    {
        "level": 1,
        "id": "9PrGSBNy",
        "title": "Choosing cloud vendors",
        "text": "Choosing cloud service providers is a key strategic decision in setting up the architecture of a data science project.  This decision should consider a number of factors, including:\n\n- The types of data (structured and non-structured) and the volume of those data types that will need to be stored, accessed and analyzed\n- Whether data will be accessed and processed in batches or in real time\n- Whether storage and analytic capabilities are seamlessly integrated in the same cloud platform (e.g., the ability to deploy ML models directly on a cloud-based data warehouse). \n- Whether the cloud providers support of open-source technologies\n\n## Major cloud vendors include:\n\n- Google Cloud\n  - Storage:\n  - Machine learning: [BigQuery ML](https://cloud.google.com/bigquery-ml/docs/bigqueryml-intro)\n  - ...\n- Amazon Web Services\n  - Storage:\n  - Machine learning: [SageMaker](https://aws.amazon.com/sagemaker/)\n- Microsoft Azure\n  - Storage:\n  - Machine learning: \n- ArcGIS online\n  - Storage:\n  - Machine learning: \n\n"
    },
    {
        "level": 2,
        "id": "vKQFuLMb",
        "title": "The cloud",
        "text": "\"The cloud\" is the networks, services, systems and databases available to clients on the Internet to share, store, access and manipulate data.\n\nEnterprise systems are increasingly switching their focus from on-premise to cloud-based data repositories and processing capabilities. \n\n"
    },
    {
        "level": 3,
        "id": "eeJS3CIE",
        "title": "Serverless data pipelines",
        "text": "Instead of setting up a data lake and a data warehouse themselves, some organizations are choosing to use managed cloud services from outside vendors for data storage and querying. \n\n"
    },
    {
        "level": 3,
        "id": "dJBcDmmd",
        "title": "Cloud service models",
        "text": "Different cloud service models can be categorized into public/private/hybrid cloud and single-provider/multi-provider cloud. \n\n*Public cloud*\n\n- All hardware, software and supporting infrastructure is owned and managed by the cloude service provider\n- Cloud infrastructure is shared with other customers and accessed over the internet.\n\n*Private cloud*\n\n- Services and infrastructure are dedicated exclusively to one organization and maintained in a private network. \n- Can be on-premises or hosted by a third-party service provider\n- Often used by organizations with special security needs\n\n*Hybrid cloud*\n\n- Combines public and private clouds\n\nIn a multi-provider cloud infrastructure, different vendors provide specific cloud services. This approach is often used to avoid vendor lock-in.\n\n"
    },
    {
        "level": 3,
        "id": "yYla7ZKp",
        "title": "Big Data",
        "text": "Big data consists of information assets characterized by:\n\n- High volume\n- High velocity\n- High variety\n\nProjects that involve a massive amounts of data that cannot be handled by local machines require to host applications and databases on the cloud. \n\n"
    },
    {
        "level": 3,
        "id": "MfTvaMIL",
        "title": "Risks and challenges of adopting cloud applications",
        "text": "Moving systems currently hosted on-premises to cloud services is a very complex task that requires:\n\n- Vendor selection and procurement\n- Data cleaning and migration\n- Addressing user management and authentication issues\n- Data security and network connectivity through remote logins and VPNs\n\n"
    },
    {
        "level": 3,
        "id": "ulLdtnHy",
        "title": "Cloud computing",
        "text": "Cloud computing refers to the delivery of hosted IT services over the internet, usually on a pay-as-you-go basis.  It includes:\n\n- Software as a service (SaaS) \n- Infrastructure as a service (IaaS)\n- Platform as a service (PaaS)\n\nCloud service providers are responsible for all their hardware and software maintenance, and are backed by a large network of servers and staff to ensure the reliability of their services. \n\nDue to its reliance on hardware-independent virtualization technology, cloud computing enables organizations to quickly back up data, applications, and even operating systems to a remote data center, and to deploy them to multiple users distributed in many different locations.\n\nCloud services usually involve lower upfront costs and shorter time commitments, lowering the barriers of entry for organizations that seek to modernize their IT infrastructure, explore the adoption of new tools, or scale up their ability to handle larger volumes of data. Thus, managed cloud services can provide a good solution for new data innovation projects, which start small but need to be able to rapidly grow. \n\n"
    },
    {
        "level": 2,
        "id": "eeJS3CIE",
        "title": "Serverless data pipelines",
        "text": "Instead of setting up a data lake and a data warehouse themselves, some organizations are choosing to use managed cloud services from outside vendors for data storage and querying. \n\n"
    },
    {
        "level": 2,
        "id": "dJBcDmmd",
        "title": "Cloud service models",
        "text": "Different cloud service models can be categorized into public/private/hybrid cloud and single-provider/multi-provider cloud. \n\n*Public cloud*\n\n- All hardware, software and supporting infrastructure is owned and managed by the cloude service provider\n- Cloud infrastructure is shared with other customers and accessed over the internet.\n\n*Private cloud*\n\n- Services and infrastructure are dedicated exclusively to one organization and maintained in a private network. \n- Can be on-premises or hosted by a third-party service provider\n- Often used by organizations with special security needs\n\n*Hybrid cloud*\n\n- Combines public and private clouds\n\nIn a multi-provider cloud infrastructure, different vendors provide specific cloud services. This approach is often used to avoid vendor lock-in.\n\n"
    },
    {
        "level": 2,
        "id": "ulLdtnHy",
        "title": "Cloud computing",
        "text": "Cloud computing refers to the delivery of hosted IT services over the internet, usually on a pay-as-you-go basis.  It includes:\n\n- Software as a service (SaaS) \n- Infrastructure as a service (IaaS)\n- Platform as a service (PaaS)\n\nCloud service providers are responsible for all their hardware and software maintenance, and are backed by a large network of servers and staff to ensure the reliability of their services. \n\nDue to its reliance on hardware-independent virtualization technology, cloud computing enables organizations to quickly back up data, applications, and even operating systems to a remote data center, and to deploy them to multiple users distributed in many different locations.\n\nCloud services usually involve lower upfront costs and shorter time commitments, lowering the barriers of entry for organizations that seek to modernize their IT infrastructure, explore the adoption of new tools, or scale up their ability to handle larger volumes of data. Thus, managed cloud services can provide a good solution for new data innovation projects, which start small but need to be able to rapidly grow. \n\n"
    },
    {
        "level": 3,
        "id": "qEgpEYM3",
        "title": "Challenges related to the rapid implementation of telecommuting arrangements",
        "text": "- Procure, configure, and distribute computer equipment and software needed for remote collaboration among all staff members, including add-ons to enable secure remote data access and system administration capabilities (e.g., standardized end-point security software on all employee devises).  If staff has already begun telecommuting, it is necessary to work out how to distribute equipment and software to the locations where they are based.\n- Ensure voice connectivity and video-conferencing capabilities (e.g., forwarding calls to staff members' home or mobile phones; enabling soft-phone, voice-over-IP or video conference solutions). It is key to allow teams to regularly and easily talk to, and visually interact with, each other  \n- Ensure remote users have the necessary bandwidth. This may entail finding a way to upgrade the user's phone and/or internet access for a period of time.\n- Make sure there are processes in place to cover most common IT helpdesk support requests form staff members working remotely\n- Enable cloud-based backup services in all remote devices.  \n- Design, implement and implement new workflows\n\n"
    },
    {
        "level": 3,
        "id": "2amwq6Qe",
        "title": "Designing for deployment",
        "text": "Designing for deployment means to design a system in such a way that it will be possible to push frequent changes easily (e.g., to keep frameworks up-to-date and push security patches), without downtime and without breaking client applications. \n\nThis requires, among other things, a versioning and backward-compatibility policy. \n\n"
    },
    {
        "level": 3,
        "id": "3ONs7aFt",
        "title": "Software as a Service (SaaS)",
        "text": "Model in which software applications are hosted by a third-party and made available to users over the Internet, usually through a web browser.  Users do not have to install or configure anything, and the underlying cloud hardware is maintained by the service provider. \n\nMost SaaS providers offer flexible, on-demand pricing arrangements as well as tools for user management and data migratoin.\n\n- Examples of SaaS applications:\n  - Email, videoconferencing, and other basic communication tools\n  - File sharing and team collaboration. \n  - Human resource management\n  - Management of relationships with data providers\n  - Management or relationships with data users\n  - Specialized applications (e.g., statistical analysis software, GIS applications)\n  - E-Learning delivery\n\n\n"
    },
    {
        "level": 1,
        "id": "oEBwztHk",
        "title": "Designing connected content",
        "text": "\"The trick to design connected content is planning those connections in advance\"\n\n\"Structure, first suggested by research and then expressed through domain and content models, provides the foundation for creating connected content.\"\n\n"
    },
    {
        "level": 2,
        "id": "2lSUK9Pt",
        "title": "Bringing content to the semantic web",
        "text": "The main objective is to markup the knowledge embedded in a body of content in a technology and language-independent way, so it can be autonomously found, processed, linked, and acted upon by user applications on the web.  \"With some semantic markup, seemingly innocuous web pages can become the centre of a huge information network\"\n\nFollowing a **schema-first** approach, the process of bringing content to the semantic web starts with the definition of data structures (schemas, vobacularies, taxonomies, etc.)\n\n1. **Modeling content**\n  - Developing (or adapting and re-using) general data schemas that capture the main entities and relationships that characterize the content under study. \n2. **Connecting to key common vocabularies, taxonomies and ontologies**\n  - Identifying which content elements can be mapped to existing knowledge organization structures\n3. **Marking up content**\n  - Marking up (annotating) specific instances of content elements with references to (1) common data schemas and (2) common vocabularies (this requires adequate tools, formats and templates). \n\n\n"
    },
    {
        "level": 3,
        "id": "DZ2mbPBy",
        "title": "Semantic content generation",
        "text": "The question of how to efficiently create ontologies and semantic metadata is \"one of the core open issues in the current semantic content authoring landscape\".  \n\nThe creation of semantic web content requires the participation of several actors, including domain experts, knowledge engineers, and tool developers. It is necessary to understand the process of translating user needs into semantic artifacts.  And there is need for tools that allow to lower the cost and increase productivity in semantic content generation.  \n\nBuilding an ontology involves:\n- Identifying concepts\n- Organizing them into groups and hierarchies\n- Linking them by means of relationships and constrains. \n\n"
    },
    {
        "level": 3,
        "id": "ZGyXDeS1",
        "title": "The semantic web is still an unfulfilled promise",
        "text": "Over the last two decades, \"the dream of a Semantic Web has never quite taken off, but never quite disappeared either\" (O'Brian, 2020)\n\n"
    },
    {
        "level": 3,
        "id": "cl33lJOT",
        "title": "Making content ready for artificial intelligence",
        "text": "To facilitate the use of machine learning and artificial intelligence in processing, repurposing and connecting the content of an information resource, it is important to provide it with structure cues and to transform it into a machine-readable format.\n\n"
    },
    {
        "level": 3,
        "id": "ii9L9664",
        "title": "Schema support for content management systems",
        "text": "The most popular enterprise CMS offer schema support:\n- Drupal: Schema metatag module (Drupal is open-source leader for enterprise content management and user experience)\n- WordPress: Schema plugin\n- Adobe Experience Manager: Ability to create, add, and manage metadata schemas\n\n\n\n\n"
    },
    {
        "level": 3,
        "id": "r3Uyt04R",
        "title": "Applications of semantic models",
        "text": "Semantic models can be used for various purposes:\n\n- To standardize and align the meaning of heterogeneous datasets that are managed independently form each other\n- To provide context to data so it can be more easily discovered, interoperated and used\n\nExample: Imagine that you are building a social media analytics system that analyzes a large number of tweets in order to detect instances of street crime incidents of a certain type.  Because different tweets can refer to the same type of crime incidents using different terms, the system needs to know that these terms are synonymous.  A semantic model can provide this knowledge. \n\n"
    },
    {
        "level": 2,
        "id": "cl33lJOT",
        "title": "Making content ready for artificial intelligence",
        "text": "To facilitate the use of machine learning and artificial intelligence in processing, repurposing and connecting the content of an information resource, it is important to provide it with structure cues and to transform it into a machine-readable format.\n\n"
    },
    {
        "level": 3,
        "id": "YIFESZcI",
        "title": "Machine learning models",
        "text": "Machine learning models are latent representations of knowledge at the sub-symbolic level that do not have any obvious human interpretation.  Their aim is to capture \"fuzzy\" knowledge (e.g., through statistical regularities and similarities). \n\n\n\n"
    },
    {
        "level": 3,
        "id": "F55azj3z",
        "title": "Complementarity between semantic modeling and machine learning",
        "text": "Machine learning can help automate the development of semantic models, while semantic modeling can accelerate and enhance the development of machine learning algorithms.  \n\nA semantic model can enhance AI capabilities and the effectiveness of data science applications\n\n"
    },
    {
        "level": 3,
        "id": "N1Wqv7Tf",
        "title": "Designing for the semantic web",
        "text": "\"Designing for the semantic web makes subsequent design infinitely richer\"\n\nThe idea is to bake in structure into the content creation and publication process, abstracting and generalizing content elements as instances of commonly agreed concepts, classes and categories.\n\nBy applying a metadata structure \"under the hood\", it is easier to design and create content that is machine-readable, machine-understandable, and machine-actionable.  \n\n"
    },
    {
        "level": 1,
        "id": "kN4kH0a5",
        "title": "Middleware",
        "text": "Middleware is any tool for integrating systems that were never meant to work together, helping pass data and events back and forth between them, and also helping decouple them by removing the need to know unnecessary details of one system in the calls made to it by another system.  \n\n\"Middleware is inherently messy, since it must work with different business processes, different technologies, and even different definitions of the same logical concept.\"  \n\n"
    },
    {
        "level": 2,
        "id": "uKiS7iJH",
        "title": "Role of data integration architect",
        "text": "The integration architect works closely with ETL developers and stakeholder teams gathering and clarifying business and functional requirements, and conducts research to understand business rules and technical specifications of source and target systems. \n\nThe data integration architect needs to have good understanding of how all the parts of the NSS ecosystem work together, where and how can all relevant data sources be accessed by statisticians and data scientists, and what are the data storage and processing environments they need in order to do their job.  She or he also should have a clear idea of how will the outputs produced by statisticians and data scientists be fed into regular data dissemination channels. \n\nThe data integration architect will also help define the nature, frequency and schedule of data integration tasks (i.e., one-time data migration; nightly/weekly/monthly integration; on-demand batch integration, real-time/stream integration...)\n\nShe or he plays a key role in facilitating communication and common understanding among strategic and operational members of the broader data integration team.  After the discovery and testing phase, hand off work to developers. \n\n\n"
    },
    {
        "level": 3,
        "id": "QSfNE2W5",
        "title": "Data integration team",
        "text": "Data integration tasks are a key component of every data innovation project.  One person cannot be responsible for an entire integration effort, including requirements, design, and deployment.  \n\nA data integration team should include the following **roles**:\n\n- Project sponsor: Provides direction and resources\n- Stakeholder: Individual or group with a vested interest in the success of the project\n- Subject matter expert: Individuals knowledgeable of the fundamental technical and/or business details of the day-to-day operations and requirements (their time is limited but precious)\n- Product owner: Main point of contact for all decisions related to the project\n- Project manager: Responsible for overseeing buget, schedule, deliverables and handover to customer\n- Data integration architect: Plans, designs and oversees the construction of data pipelines and tools that enable the movement of data from one point to another.  Works closely with ETL developers as their technical lead.\n- ETL developers: Team responsible of building ETL solutions that satisfy the technical requirements set by the data integration architect and ensure they are free of bugs, errors and defects.\n\nSince the data integration task spans across systems with multiple purposes, it is imperative to bring in experts from different domain areas, who understand day-to-day processes, are closely familiar with the design, purpose and unique processes of source systems, and know where \"data landmines\" are.\n\nThe data integration team needs to include people with technical knowledge of different database management systems, cloud platforms, data formats and serializations (e.g., XML, JSON), and communication protocols (e.g. REST). \n\nIn addition, a successful data integration team requires strong communication and the ability to discuss, negotiate and compromise. \n\nEngaging a data integration team to define a common data architecture in the early phases of a data innovation project will help prevent delays due to interoperability issues later on. \n\n\n"
    },
    {
        "level": 3,
        "id": "62wtBu4C",
        "title": "Data integration for sustainable development",
        "text": "The integrated use of data and information from multiple sources (both traditional ones, such as census and survey programmes, and non-traditional ones, such as administrative records, satellite imagery, etc.) is crucial to enable all stakeholders to make smarter, evidence-based decisions towards achieving sustainable development.  \n\nThis includes the ability to effectively and efficiently bring together data assets which often have been developed in isolation from each other and only with a narrow set of operational or very short-term needs in mind, for their joint processing, analysis and dissemination throughout the data value chain.  \n\nMoreover, it includes the development of cross-sectoral reporting tools for tactical and strategic decision making and advocacy.\n\n\n\n"
    },
    {
        "level": 3,
        "id": "sKuMe1ca",
        "title": "Standardizing ETL tools and solutions",
        "text": "The adoption of different, often proprietary ETL tools for data integration in different departments of the same organization often leads to an increase in data silos.\n\nWhen \"one department of an organization, perhaps focusing on its own need, [adopts] a completely different ETL tool than another department[, the result is] an *increase* in data silos, rather than a decrease.\"\n\nOrganizations often work with multiple ETL solutions, each with its own framework, ... \"relegating the entire architecture into a big mess\"\n\n"
    },
    {
        "level": 3,
        "id": "wHkdn3us",
        "title": "Enterprise architecture skills",
        "text": "Roles and responsibilities of a enterprise architect include:\n\n- Evaluate the existing operating environment across the enterprise to identify redundancies, gaps and inefficiencies\n- Working with stakeholders to facilitate the discovery and documentation of the customer\u2019s business scenarios in order to understand real needs, as opposed to wants, and to translate them into business requirements\n- Conceptualize technical solutions to complex problems and meet business requirements, developing well-formulated models of the components of these solutions.\n- Verify stability, security, portability, scalability and interoperability, of the proposed enterprise architecture\n- Evaluate and guide the selection of technologies to implement the proposed enterprise architecture.\n- Collaborate with stakeholder to gain organizational commitments for all process, systems and application plans\n- Communicate project information through presentations, technical reports, or white papers \n\nDifferent roles of an enterprise architect and a solution designer/ builder:\n\n- The enterprise architect must remain at a a high level of abstraction, concentrating on the few critical components and their interfaces. \n- The solution designer/builder translates architecture into deliverable components.\n\n"
    },
    {
        "level": 3,
        "id": "jwCdelDE",
        "title": "Facilitating collaboration amongst data providers and users",
        "text": "Data integration activities require collaboration across providers and uses of data, both within and across organizations.  Each representative brings unique concerns and opinions, and different knowledge of the overall data ecosystem.  \"Everyone needs to be on the same page when it comes to build a data integration system\"\n\n- Ensure that data are well documented, consistent and of known quality\n- Harmonize the spatial and temporal scales over which data is collected\n- Agree on common metadata requirements\n- Identify tradeoffs and reach compromises regarding data formats, standards and conventions.\n\n"
    },
    {
        "level": 3,
        "id": "d3ru9pGK",
        "title": "Building data engineering skills in NSOs",
        "text": "The ability of NSOs to efficiently integrate multiple data inputs into valuable statistics is increasingly dependent on their data infrastructure and \"data engineering\" skills.  \n\nDifferent types of data repositories and architectures require vastly different technical skills.  Statisticians need to develop a broad rang of foundational data engineering skills to  mainstream new data sources, methods and technologies into regular statistical production programmes.\n\nStatisticians are rarely given analysis-ready data that can be directly used in statistical estimation and compilation.  One of the main challenges in official statistics is to design, build and maintain data integration, processing, and dissemination pipelines. \n\nPractical data innovation projects in official statistics require the ability to extract, organize and manipulate raw, unstructured source data, and to transform it into \"clean\" datasets that can be processed and analyzed using by standard software programmes and methodologies.\n\nMoreover, it is important to continuously develop and retain these skills over time, as new data inputs needs to be fed into the statistical production process on a regular basis. \n\n"
    },
    {
        "level": 3,
        "id": "jU3e4dIS",
        "title": "Role of data engineer",
        "text": "Data engineers needs a solid technical understanding of data integration and architecture modelling in order to:\n\n1. Develop **ETL pipelines** that are easy to understand, maintain, and replicate by others.  \n2. Implement **data storage and processing environments** that allow data scientists and other analysts to collaborate in the development and testing of estimation models. \n\n\n\n"
    },
    {
        "level": 3,
        "id": "LNILH6jK",
        "title": "Common Statistical Production Architecture (CSPA)",
        "text": "CSPA is a \"reference architecture for the statistical industry\", i.e., a set of **recommended patterns and approaches for the integration of business processes, applications, data, and IT infrastructure** for the creation and delivery of statistical products and services. \n\n- It covers all statistical production processes\n- It does not prescribe a particular technology environment\n\nCSPA defines a framework for common services for statistical production, providing savings opportunities for collaborative development, sharing and re-use across different statistical processes and across different organizations. \n\nA common statistical production architecture enables project managers, statisticians, IT specialists and other stakeholders **to collaborate and communicate effectively** about specific statistical productions projects and programmes.\n\nBy identifying patterns for the integration of technology and applications that have been successfully used across different statistical projects and organizations, CSPA helps teams **anticipate problems**,  **avoid errors and delays**, and **reduce costs**.\n\nA common enterprise architecture approach within and across statistical organizations allows them to respond to emerging risks and challenges, and to leverage new opportunities, by facilitating the reuse and sharing of solutions and services and enhancing standardization and interoperability of statistical production systems and processes. \n\n\n"
    },
    {
        "level": 3,
        "id": "NImn69Rs",
        "title": "Quality and integration of administrative and other data sources",
        "text": "**Requirement 13.4** of the \"National Quality Assurance Frameworks Manual for Official Statistics\" calls statistical agencies to promote the sharing, linking, and use of administrative and other data sources to minimize respondent burden, including by:\n\n- Creating and sharing documentation about the quality and other characteristics of those data\n- Developing and implementing technical tools for sharing and linking them, while protecting data confidentiality\n- Promoting them as an alternative to survey-based data\n\n\n"
    },
    {
        "level": 3,
        "id": "irmKQb3k",
        "title": "Roles in data innovation project team",
        "text": "- **Data scientist**: Understands data science modelling techniques\n- **Data engineer**: Understands dataflow architecture\n- **Data platform administrator**: \n- **Database administrator**: Understands individual DBMSs\n- **Data administrator**: Plans and sets policies related to data\n\n"
    },
    {
        "level": 2,
        "id": "sKuMe1ca",
        "title": "Standardizing ETL tools and solutions",
        "text": "The adoption of different, often proprietary ETL tools for data integration in different departments of the same organization often leads to an increase in data silos.\n\nWhen \"one department of an organization, perhaps focusing on its own need, [adopts] a completely different ETL tool than another department[, the result is] an *increase* in data silos, rather than a decrease.\"\n\nOrganizations often work with multiple ETL solutions, each with its own framework, ... \"relegating the entire architecture into a big mess\"\n\n"
    },
    {
        "level": 3,
        "id": "WnQYbC1N",
        "title": "Stream vs batch ETL pipelines",
        "text": "There are 2 main types of data pipelines:\n\n1. **Stream**: Transactional data is passed along almost as soon as the transaction occurs. As soon as a new record is added into the source database, it\u2019s passed along into the analytical system. Creating and maintaining streaming systems is often more challenging.\n2. **Batch**: The pipeline runs at a specific time interval; data is not live, but is loaded in \"batches\".\n\n\n\n\n"
    },
    {
        "level": 3,
        "id": "mcXbI3UY",
        "title": "Automation of routine statistical processes",
        "text": "Modern information and communication technologies can be used to improve performance of statistical processes through automation. \n\nFor example, routine clerical operations and statistical procedures (including ETL operations and and validation of data inputs and outputs) should be standardized and automated as much as possible.  \n\n"
    },
    {
        "level": 1,
        "id": "hv92HYqG",
        "title": "Dublin Core",
        "text": "A simple, easy to learn and easy to use metadata schema, designed to enable the description of any resource.   It consists of a \"core set of 15 metadata elements (attributes) that could be applied to describe any and all resources on the internet, namely:\n\n1. Contributor\n2. Coverage\n3. Creator\n4. Date\n5. Description\n6. Format\n7. Identifier\n8. Language\n9. Publisher\n10. Relation\n11. Rights\n12. Source\n13. Subject\n14. Title\n15. type\n\n- All elements are repeatable and optional.\n\n\n"
    },
    {
        "level": 2,
        "id": "Dv5g2Dfc",
        "title": "schema.org",
        "text": "Schema.org is a widely popular initiative that aims to create, maintain and promote the use of common schemas for publishing structured content on the web.  It includes thousands of schemas across all domains. \n\nIt provides a vocabulary and thousands of schemas covering entities, relationships between entities and actions across a many domains, available  RDFa, Microdata and JSON-LD. \n\nUsing schema.org, for instance, it is possible to include structured metadata in a website. \n\n"
    },
    {
        "level": 3,
        "id": "xqNWXkOj",
        "title": "schema.org",
        "text": "Schema.org is a joint effort launched on June 2, 2011 by Bing, Google, and Yahoo! 'for the creation of microcontents targeted to improving indexing and search systems.'  It is \"a KOS specifically targeted to prificing a degree of semantic interoperability to microdata\".\n\n"
    },
    {
        "level": 2,
        "id": "xqNWXkOj",
        "title": "schema.org",
        "text": "Schema.org is a joint effort launched on June 2, 2011 by Bing, Google, and Yahoo! 'for the creation of microcontents targeted to improving indexing and search systems.'  It is \"a KOS specifically targeted to prificing a degree of semantic interoperability to microdata\".\n\n"
    },
    {
        "level": 3,
        "id": "AUj0d0SV",
        "title": "Microdata",
        "text": "Microdata is 'structuredd markup included inside HTML pages ... to make them [more easily findable] by search engines.'\n\n"
    },
    {
        "level": 1,
        "id": "FUejJ5RV",
        "title": "Semantic web applications",
        "text": "Some possible applications of semantic web technologies include:\n\n- Cross-referencing systems for easy navigation across information assets\n- Knowledge organization systems consisting of a network of information assets\n- Wiki platforms that support the creation of inter-connected knowledge assets\n\n"
    },
    {
        "level": 2,
        "id": "qJ3EG7LL",
        "title": "Benefits of the semantic web",
        "text": "- Richer, more sophisticated user experiences\n- Eliminating content silos\n- Search-engine optimization\n- Democratizing content through multilingualism\n- Enabling more platforms to find, decipher, aggregate and present information from multiple providers that is relevant to different user groups\n\n"
    },
    {
        "level": 3,
        "id": "2RSnKnf1",
        "title": "Semantic web objectives",
        "text": "The goal of a semantic web is to make metadata about heterogeneous, distributed information resources machine readable and available through the web, so that third-party applications can find these resources, access them, reason over them and operate on them.   In other words, the objective is to improve the description of web resources so users can target and integrate them more easily to meet their own needs. \n\n\"Content and metadata providers allow and even encourage users to 'hack' their data and metadata by offering them a toolkit to re-use data and metadata for their own needs.\"\n\n"
    },
    {
        "level": 3,
        "id": "YCD59Qrv",
        "title": "Cambrian explosion analogy",
        "text": "\"The Cambrian Explosion was a seminal event in Earth's history, when the mostly unicellular organisms that existed diversified into multicellular organisms, all at one moment in the fossil record 541 million years ago. ... Digital experiences are in the midst of another Cambrian explosion.\"\n\n"
    },
    {
        "level": 2,
        "id": "2RSnKnf1",
        "title": "Semantic web objectives",
        "text": "The goal of a semantic web is to make metadata about heterogeneous, distributed information resources machine readable and available through the web, so that third-party applications can find these resources, access them, reason over them and operate on them.   In other words, the objective is to improve the description of web resources so users can target and integrate them more easily to meet their own needs. \n\n\"Content and metadata providers allow and even encourage users to 'hack' their data and metadata by offering them a toolkit to re-use data and metadata for their own needs.\"\n\n"
    },
    {
        "level": 3,
        "id": "OyHGHNbK",
        "title": "Open access and interoperability",
        "text": "Providing open access to a dataset or a document is more than just allowing users to obtain a physical or electronic copy of it for their perusal; it requires that their contents be described and structured in a standardized manner, broken down into meaningful elements that can be read and correctly interpreted by different people and by multiple software applications.\n\n"
    },
    {
        "level": 3,
        "id": "bzURsW8Z",
        "title": "Linked Open Data for statistical manuals and handbooks",
        "text": "*\"Embodying knowledge into a hard-cover book does not translate into useful and usable form in today's environment\"*\n\n\"Online publication\" is not about simply making available online the electronic counterparts of traditional paper publications, but about taking advantage of the opportunities provided by new Information and Communication Technologies (ICTs) and markup languages, including:\n- embedded cross-references (hypertextual access)\n- semantic search\n- cross-domain linking\n- automatic information extraction and knowledge elicitation\n- document life-cycle automation (from authoring to consumption)\n- creation of high-value information services / applications\n\nOne of the main challenges at present is the lack of uniformity in the preparation and structuring of methodological documents, including lack of shared citation and cross-referencing mechanisms.  There is a need to standardize methodological documents to support the exchange of information and collaboration. \n\n**Inter-linking statistical frameworks on the semantic web**\n\nThe objective is to ensure that the meaning and structure of the different elements of methodological documents are available in machine readable form to software applications.\n\n\n\n"
    },
    {
        "level": 3,
        "id": "HSwnGhxf",
        "title": "Semantic Web",
        "text": "The semantic web is a metadata-driven framework for sharing and re-using machine-readable content across applications, organizations and communities.  \n\nBy \"data-fying\" content, it allows to move from a web of documents to a web of data.\n\n"
    },
    {
        "level": 3,
        "id": "HxvI2lZv",
        "title": "Example of how linked data works",
        "text": "- The metadata pertaining to a dataset is expressed using DCAT\n- This leads to collection discovery\n- Inside a given collection, the user agent can navigate to similar resources by following RDF links using SPARQL queries. \n\n"
    },
    {
        "level": 3,
        "id": "eb3PPobe",
        "title": "Semantic web",
        "text": "The semantic web is \"an evolving extension of the web in which content can be expressed in [machine-processable, machine understandable ways, so it] can be interpreted and used by software agents... to find, share and integrate information more easily.\"\n\n"
    },
    {
        "level": 2,
        "id": "eb3PPobe",
        "title": "Semantic web",
        "text": "The semantic web is \"an evolving extension of the web in which content can be expressed in [machine-processable, machine understandable ways, so it] can be interpreted and used by software agents... to find, share and integrate information more easily.\"\n\n"
    },
    {
        "level": 3,
        "id": "AMvDfqeY",
        "title": "Metadata for the semantic web",
        "text": "In order to bring information resources into the semantic web, it is necessary to describe these resources by means of some form of standardized, machine-readable metadata schema. Semantic web technologies provide the means enhance information seeking, targeting and integration through metadata.\"\n\n"
    },
    {
        "level": 1,
        "id": "kOhhkZB8",
        "title": "Ivory-tower architecture",
        "text": "Putting too much emphasis on \"enterprise standards\", imposing specific technologies on the team of developers, without listening to them or to the users. \n\nThis is the opposite extreme of an \"accidental architecture\" situation. But is equally crippling. \n\n"
    },
    {
        "level": 2,
        "id": "kjBXVcZc",
        "title": "\"Accidental architecture\"",
        "text": "Situation in which technical solutions are \"built for a very specific purpose with little regard for ability to share information with other adjacent applications in the statistical cycle and with limited ability to handle simlar but slightly different processes and tasks.\"\n\n\"Historically, statistical organizations have developed their own business processes and IT-systems for producing statistical products. Although the products and the processes conceptually are very similar, the individual solutions are not\" (https://statswiki.unece.org/display/CSPA/I.++CSPA+2.0+The+Problem+Statement)\n\n"
    },
    {
        "level": 3,
        "id": "HzaOGY5M",
        "title": "Enterprise architecture",
        "text": "An enterprise architecture is a framework (i.e., a set of principles and practices) for the integration of business processes, data, software applications and hardware infrastructure to achieve desired business results. \n\nIt is composed of:\n\n- **Business architecture:** Elements of the enterprise's business strategy, including business functions, processes and services (\"What the enterprise does, why it is done, and how\")\n- **Data architecture:** Definition of the organization's data structures, data flows, and data storage and management principles and practices\n- **Application architecture:** Principles and practices for selecting, designing and deploying software applications in accordance with business goals and processes\n- **Technology architecture:** Description of all hardware, and IT infrastructure necessary to develop and deploy business applications.\n\nAn enterprise architecture \"helps remove silos, improves collaboration [within] an organization and ensures that the technology is aligned to the business needs.\" \n\nHaving a well-defined enterprise architecture enables an organization to transform its legacy processes and applications and to adapt to emerging technology trends.  \n\nOther benefits of an enterprise architecture:\n\n- Better collaboration between IT and business units\n- Ability to prioritize IT investments\n- Effective evaluation and procurement of IT solutions\n- Common understanding of IT infrastructure and systems across all business units in the organization\n\nAn enterprise architecture addresses the entire organization in a holistic manner, instead of focusing on the needs and problems of individual business units.  It is akin to a \"master urban plan\" that provides the framework for a well-functioning city, beyond the \"blueprints\" specifying the design of a single building.  Following this analogy, one could say that the lack of an enterprise architecture leads to the proliferation of incoherent and inefficient solutions across the enterprise, similar to the effects of \"unconstrained urban sprawl\". \n\n\n\n"
    },
    {
        "level": 3,
        "id": "OGAyJjdF",
        "title": "Every system's model is incomplete",
        "text": "At any moment in time, our mental models of the systems we work with are wrong or incomplete, as real-world systems are complex, with many moving parts and internal dependencies. \n\n\"Tightly coupled systems are the rule rather than the exception.\"\n\n"
    },
    {
        "level": 3,
        "id": "zhPZeYYd",
        "title": "Pragmatic architecture",
        "text": "- Give due consideration to issues such as memory usage, CPU requirements, bandwidth needs, etc. \n- Focus on practical feasibility of proposed solutions\n- Think in terms of the dynamics of gradual improvement\n- Inspect the inner workings of any abstractions or frameworks and modify or eliminate them if necessary \n\n"
    },
    {
        "level": 3,
        "id": "9QXT3pVK",
        "title": "Reference architecture",
        "text": "\"A reference architecture is a document or set of documents that provides recommended structures and integrations of IT products and services to form a solution. The reference architecture embodies accepted industry best practices, typically suggesting the optimal delivery method for specific technologies.\"\n\n\"Reference architectures help project managers, software developers, enterprise architects, and IT managers collaborate and communicate effectively about an implementation project. A reference architecture anticipates\u2014and answers\u2014the most common questions that arise. As a result, they help teams avoid errors and delays that may occur without the use of a tested set of best practices and solution approaches.\"\n\n"
    },
    {
        "level": 2,
        "id": "HzaOGY5M",
        "title": "Enterprise architecture",
        "text": "An enterprise architecture is a framework (i.e., a set of principles and practices) for the integration of business processes, data, software applications and hardware infrastructure to achieve desired business results. \n\nIt is composed of:\n\n- **Business architecture:** Elements of the enterprise's business strategy, including business functions, processes and services (\"What the enterprise does, why it is done, and how\")\n- **Data architecture:** Definition of the organization's data structures, data flows, and data storage and management principles and practices\n- **Application architecture:** Principles and practices for selecting, designing and deploying software applications in accordance with business goals and processes\n- **Technology architecture:** Description of all hardware, and IT infrastructure necessary to develop and deploy business applications.\n\nAn enterprise architecture \"helps remove silos, improves collaboration [within] an organization and ensures that the technology is aligned to the business needs.\" \n\nHaving a well-defined enterprise architecture enables an organization to transform its legacy processes and applications and to adapt to emerging technology trends.  \n\nOther benefits of an enterprise architecture:\n\n- Better collaboration between IT and business units\n- Ability to prioritize IT investments\n- Effective evaluation and procurement of IT solutions\n- Common understanding of IT infrastructure and systems across all business units in the organization\n\nAn enterprise architecture addresses the entire organization in a holistic manner, instead of focusing on the needs and problems of individual business units.  It is akin to a \"master urban plan\" that provides the framework for a well-functioning city, beyond the \"blueprints\" specifying the design of a single building.  Following this analogy, one could say that the lack of an enterprise architecture leads to the proliferation of incoherent and inefficient solutions across the enterprise, similar to the effects of \"unconstrained urban sprawl\". \n\n\n\n"
    },
    {
        "level": 3,
        "id": "BbRh3H8s",
        "title": "\"Control plane\" software",
        "text": "Software whose purpose is not to directly deliver user functionality, but to help manage the infrastructure and applications of an enterprise. It includes  logging, monitoring, schedulers, scalers, load balancers, and configuration management tools.\n\n"
    },
    {
        "level": 3,
        "id": "ysFKrz35",
        "title": "Organization map",
        "text": "An organization map identifies the business units and stakeholders of an enterprise, describing their capabilities and roles in the information and value streams within the context of the whole enterprise architecture.\n\nIt shows:\n\n1. Main organizational units, suppliers, partners, customers and other stakeholder groups that participate in the business model of the enterprise\n2. The network of formal and informal cooperative and collaborative relationships and interactions between each of those entities that are critical to the business, including:\n  - Value flows\n  - Information flows\n\nIn addition, an organization map shows:\n\n- Which organization units have which business capabilities (in a many-to-many mapping)\n- Relationships between organization units and geographic location\n\nAn organization map describes an enterprise as a social network, showing how its organizational units, suppliers, partners, customers and other stakeholder groups actually interact and operate across formal boundaries and siloes. \n\nIt includes individual organizational units as well as collaborative teams operating across two or more organizational units. \n\nPurpose of organizational mapping:\n\n1. Improve understanding of how different business units may be impacted by changes in business conditions and the implementation of new systems, applications, data models and infrastructure\n2. Support a holistic approach to strategic planning and investment analysis, showing how an initiative sponsored by one business unit may affect others. \n3. Maximize asset reuse across multiple business units, identifying common requirements and solutions.\n4. Identify which business unites need to communicate and collaborate in order to realize the goals of specific initiatives\n\nThe creation of an organization map is a key step in the development of a business architecture aimed to \"ensure that the real problem is solved\" thorugh changes in business processes, roles and responsibilities and investments in data, applications and IT infrastructure.\n\n"
    },
    {
        "level": 3,
        "id": "oVj1d155",
        "title": "IT and data quality",
        "text": "**Requirement 11.4** of the \"National Quality Assurance Frameworks Manual for Official Statistics\" calls for the application of modern information and communication technologies to improve the effectiveness and efficacy of statistical processes. \n\n- This requirement underlines the importance of having an **IT strategy, architecture and infrastructure** that are regularly reviewed and updated, wiht a view to continuously identify possibilities for innovation and modernization.  \n- Moreover, it underlines the value of **automating routine clerical operations** and statistical processes such as data capture, coding, editing, validation and exchange). \n- Finally, it emphasizes the benefits of **pooling IT resources and investments** across different areas of a statistical organization. \n\n"
    },
    {
        "level": 3,
        "id": "4lWg4cUa",
        "title": "Business Architecture: Functions, Processes and Services",
        "text": "A **business function** is something that an enterprise does, or needs to do, to achieve its objectives. \n\nA **business process** is a sequence of steps required to perform one or more business functions.  Each step may be a sub-processes consisting of various steps. \n\nA **business service** provides a means for accessing a business function by performing one or more business processes.  It has a defined interface that delivers a specific output given a particular set of inputs.    \n\n\n\n"
    },
    {
        "level": 3,
        "id": "gFelA64o",
        "title": "Central IT and methodological services",
        "text": "Centralized IT and methodological units allow to pool resources and investments\n\n"
    },
    {
        "level": 2,
        "id": "zhPZeYYd",
        "title": "Pragmatic architecture",
        "text": "- Give due consideration to issues such as memory usage, CPU requirements, bandwidth needs, etc. \n- Focus on practical feasibility of proposed solutions\n- Think in terms of the dynamics of gradual improvement\n- Inspect the inner workings of any abstractions or frameworks and modify or eliminate them if necessary \n\n"
    },
    {
        "level": 3,
        "id": "37KS4Ape",
        "title": "Importance of architecture decisions",
        "text": "\"Design and architecture decisions are also financial decisions\" (p.4)\n\n"
    },
    {
        "level": 3,
        "id": "sFjOZvSp",
        "title": "Agile development",
        "text": "Agile development emphasizes \n\n- early delivery\n- learning\n- incremental improvement\n\n\"Production is the only place to learn how [a product] will respond to real-world stimuli\"\n\n"
    },
    {
        "level": 3,
        "id": "CqXLv3Sw",
        "title": "Evolutionary architecture",
        "text": "The product owner and development team should plan from the beginning for the growth and continuous development of the system after its first release.  The objective should be to build a systems architecture that can grow and adapt to changing needs and conditions over time. \n\n"
    },
    {
        "level": 1,
        "id": "x97FBvWU",
        "title": "Managing a crisis communication programme",
        "text": "Managing a crisis communications program requires the same level of dedication and resources typically given to other functions. \n\nIn times of extreme crisis, internal communications take precedence. Key priorities include reestablishing communication with any groups of staff that are stranded or isolated, and rebuilding staff morale.\n\n"
    },
    {
        "level": 2,
        "id": "SRFvQdH5",
        "title": "Importance of organization's mission, vision and values in times of crisis",
        "text": "In times of crisis, the most valuable assets of an organization are its people and its reputation.  An organization cannot start communicating its mission, vision and values during a crisis.  Staff will know what to do only if they have already internalized the organization's guiding principles.  \n\nA strong culture enables an organization to maintain focus in a time of crisis.\n\n\"Our principles will never fail us as lon as we do not fail to live up to them\" (Henry Paulson, Goldman Sachs CEO)\n\n"
    },
    {
        "level": 3,
        "id": "b2eiNqSx",
        "title": "Scrum values",
        "text": "- **Honesty**: Adherence to the facts, refusing to deceive others or oneself in any way. \n\n>*\"Agile teams only agree to take on tasks they believe they can complete, so they are careful not to overcommit\" and \"are also honest when they need help\".*\n\n- **Openness**: Willingness to engage with others and to hear and consider different opinions. \n\n>*\"When team members aren\u2019t sure how work is going, they ask\" and they \"consistently seek out new ideas and opportunities to learn.\"*\n\n- **Courage**: Self-confidence and moral strength to relentlessly expose any type of organizational dysfunction, waste and to learn from failure. \n\n>*\"Scrum teams must feel safe enough to say no, to ask for help, and to try new things. Agile teams must be brave enough to question the status quo when it hampers their ability to succeed.\"*\n\n- **Respect**: Recognition and appreciation of the intrinsic worth of the opinions and contributions of every team member and every stakeholder. \n\n>*Scrum team members know that \"everyone has a distinct contribution to make toward completing the work of the sprint. They respect each other\u2019s ideas, give each other permission to have a bad day once in a while, and recognize each other\u2019s accomplishments.\"*\n\n- **Focus**:  Continuous effort to avoid low-value distractions and direct everyone's attention and energy to what matters most.  \n\n>*Scrum teams finish whatever they start and \"are relentless about limiting the amount of work in process\". *\n\n- **Trust**: Reliance on the good faith, truthfulness, knowledge and skills of each other. \n\n>*\"Scrum and agile teams trust each other to follow through on what they say they are going to do.\"*\n\n- **Empowerment**: Ability of self-organizing teams to ask and answer their own questions and to define for themselves how to do the work necessary to meet project goals.\n\n- **Collaboration**: Capacity of team members to effectively cooperate and assist each other in achieving a common goal. \n\n>*\"Scrum teams work together as a unit.\"*\n\n\n\n"
    },
    {
        "level": 3,
        "id": "lCHnOEAE",
        "title": "Managing projects that are at risk of failure",
        "text": "Projects at risk of failing are actually very common in most organizations, but most of them recover when action is taken.  \n\n**Why projects become \"at risk\"**\n\n- Requirements (poorly defined, not prioritized, contradictory or not agreed upon)\n- Resources (not available, tied up in conflicts, or not properly planned for)\n- Schedules (unrealistic).\n- Planning (based on incomplete data, bad estimates, and insufficiently detailed).\n- Risks (not properly identified or measured)\n\n**Critical determinants of project recovery**\n\nThe project manager has a large impact on whether a project at risk of failing ultimately succeeds or not, since she or he decides which root causes to address and which actions to undertake.\n\nHaving a standardized project management methodology significantly reduces the likelihood of project failure in an organization. \n\n**Most frequent interventions that lead to successful recovery**\n\n- Improve communication\n- Redefine the scope or business case of the project\n- Right-size resources (add if too little, reduce is too many resources had been allocated)\n- Address technical issues trough in-depth analysis\n- Shift talent (e.g. replace project manager or bring in a consultant)\n\n**Steps to recovery**\n\n1. Review the project's history, purpose, goals, assumptions, and team composition\n2. Facilitate team communication about issues and root causes (without finger pointing)\n3. Address incentives and verify that stakeholders are still vested in the project\n4. Assess tradeoffs between time, cost and scope, subject to reputation, quality and value constraints\n5. Obtain buy in from stakeholders into the proposed solution\n6. Restart the project by communicating learnings from past mistakes, updated plan, roles and responsibilities\n\n\n"
    },
    {
        "level": 3,
        "id": "q5I394w7",
        "title": "Organizational culture",
        "text": "The organizational culture is what gives meaning to the work that people do in an organization.  It includes:\n- Shared sense of roles and hierarchy\n- Belief system\n- Values\n- Expectations\n\nA strong culture enables an organization to maintain focus in a time of crisis.\n\n"
    },
    {
        "level": 3,
        "id": "YJqwphec",
        "title": "Critical communication infrastructure",
        "text": "Heads of NSOs need to be able to communicate easily with their staff during the crisis.  Staff needs to have adequate connectivity, equipment and infrastructure, not only to be able to work from home, but also to stay in touch with colleagues and stay appraised of current developments. \n\n"
    },
    {
        "level": 3,
        "id": "84OVqhJD",
        "title": "Leading in times of trauma",
        "text": "In times of trauma, leaders of an organization can help staff channel their desire to help and get back into their normal routine by fostering their pride in the organization and what they do, and providing context for grieving, meaning and action. \n\n1. *Context for grieving*: Create an environment where people can freely express and discuss the way they feel, and seek or provide comfort\n\n2.  *Context for meaning*:  Communicate and reinforce organizational values, reminding people about the larger purpose of their work and helping make sense of the pain\n\n3. *Context for action*: Create an environment where those who experience or witness pain can imagine a more hopeful future and find ways to alleviate their own and other's suffering.\n\n\n\n"
    },
    {
        "level": 3,
        "id": "bay5lBgW",
        "title": "Need for crisis management skills",
        "text": "The current situation demands strong crisis management skills and expertise, and agility in planning, designing and implementing innovative approaches to carry out critical statistical operations. \n\n\n"
    },
    {
        "level": 2,
        "id": "YJqwphec",
        "title": "Critical communication infrastructure",
        "text": "Heads of NSOs need to be able to communicate easily with their staff during the crisis.  Staff needs to have adequate connectivity, equipment and infrastructure, not only to be able to work from home, but also to stay in touch with colleagues and stay appraised of current developments. \n\n"
    },
    {
        "level": 3,
        "id": "hGt3as0b",
        "title": "Digital collaboration tools",
        "text": "Modern online collaboration tools are increasingly important for virtual and cross-functional teams to achieve greater levels of transparency, optimize resources and deliver results.\n\nThe selection of collaboration tools should be based on business needs and the budget available to the project or organization.\n\nTypical requirements include:\n\n- Team communication:\n  - Chat / instant messaging\n  - Screen sharing\n  - Audio/video conferencing\n  - Discussion forums\n- File and data sharing\n- Project management\n  - Planning\n  - Budget management\n  - Procurement management\n  - Contact management\n  - Calendar management\n  - Note taking\n  - Monitoring and evaluation\n- Code development / versioning\n- Content creation and management\n- Workflow management / automation\n\n"
    },
    {
        "level": 3,
        "id": "xUr89DOT",
        "title": "Priority objectives of COVID-19 contingency plans for statistical programmes",
        "text": "Priority areas are:\n\n- Maintain adequate coverage of the target population\n- Ensure high questionnaire- and item-response\n- Guarantee internal consistency, comparability, and overall quality of the data collected\n- Maintain timely data collection, processing and dissemination\n- Minimize response burden on information providers\n- Use resources efficiently / minimize cost of statistical operations in the new environment\n\n"
    },
    {
        "level": 3,
        "id": "BQwQetnc",
        "title": "Business continuity of statistical organizations",
        "text": "The current COVID-19 crisis is affecting critical operations of across the entire global statistical system, and national and international statistical organizations need to urgently develop and implement action plans to ensure the continuity of key statistical compilation activities and the continued availability of data to inform emergency mitigation actions by governments and all sectors of society. \n\nSenior management in statistical organizations need to define guidelines, in consultation with front-line managers and IT teams, to deal with the contingency.  This includes establishing new procedures and workflows on issues like:\n\n- **Management of virtual teams (task tracking and performance management)**: Can leaders of NSOs communicate easily with their staff? Is there a centralized location from which critical information is accessible to staff?\n- **Secure remote data access and data exchange**: Can staff stay connected through internet from home?  Are critical data safely stored and securely accessible?\n- **Responding to users' most urgent needs**: Can users reach service and response teams through dedicated phone lines and email address? Are  specific instructions on how to obtain immediate assistance available through social media and the organization's website? \n\n\n\n"
    },
    {
        "level": 3,
        "id": "HCYXJekR",
        "title": "Technological foundations",
        "text": "\"Everything starts with the physical infrastructure ... the machines and wires that everything else builds upon.\" (Nygard, 2018)\n\n"
    },
    {
        "level": 2,
        "id": "bay5lBgW",
        "title": "Need for crisis management skills",
        "text": "The current situation demands strong crisis management skills and expertise, and agility in planning, designing and implementing innovative approaches to carry out critical statistical operations. \n\n\n"
    },
    {
        "level": 3,
        "id": "5dUHywzA",
        "title": "Adapting infrastructure and operations of statistical organizations in times of crisis",
        "text": "National and international statistical organizations must adapt to the uncertainties of a new reality characterized by health emergencies, environmental crises, economic recession and political instability. \n\nFor instance, the central information systems of National Statistical Offices need to adapt quickly to effectively manage and monitor statistical operations in the context of the COVID-19 crisis, such as staff recruitment, training, data collection logistics, and supervision and gathering of operational intelligence. \n\n\n\n"
    },
    {
        "level": 3,
        "id": "4kDQlxl8",
        "title": "Leading the response of NSOs to the COVID-19 pandemic",
        "text": "The worldwide spread of COVID-19 is having vast, long-term consequences for people, organizations, economies and society at large, which call for a new kind of leadership.  \n\nLeaders from governments and organizations at all levels must not only make decisions to keep people safe and ensure continuity of operations in the middle of the crisis. They must also keep an eye on the long-term recovery and look for opportunities to innovate, create growth and \"build back better\".\n\nEven after the health crisis is over, things will not \"return to normal\", and national and international statistical organizations will have to adapt to a new reality, characterized by new demands from users and lasting changes in the configuration of the entire data value chain. \n\nIn responding to the crisis, heads of national statistical offices cannot default to known approaches, as the unprecedented nature and scale of the challenge surpasses anyone's past experience.  NSO staff at all levels need to quickly adapt and experiment with \"next practices\" in their day-to-day statistical operations, acknowledging that many of the traditional \"best practices\" are no longer relevant. \n\nMoreover, any success in the initial response by NSOs should not be followed by complacency or the illusion of a return to normalcy. Instead, statistical organizations must continue focusing on innovation and sustain their efforts to understand and adaptation to the new reality in the global data ecosystem. \n\nIn this context, heads of statistical organizations must identify the principles and practices that must be preserved in the compilation of official statistics, as well as those that must be abandoned or modified in order to move forward.\n\n\n"
    },
    {
        "level": 3,
        "id": "gxrbaZc1",
        "title": "Small, bottom-up initiatives in response to COVID-19",
        "text": "Small, bottom-up initiatives can play an important role in a statistical organization's response to the disruptions created by the COVID-19 pandemic.   Instead of attempting to develop and implement grand and detailed plans, in a crisis like this it is better to learn quickly and often from the successes and failures of many small-scale experiments.\n\nAdaptability usually comes \"from the accumulation of microadaptations originating throught [the organization] in response to its many microenvironments\".\n\nIn times of crisis, information sharing, listening and learning must take precedence over hierarchy and formal authority, so as to be able to draw from the collective knowledge and skills across diverse functions and locations to generate solutions, \n\n"
    },
    {
        "level": 1,
        "id": "r7WsYDhx",
        "title": "Individual vs team productivity",
        "text": "How can we become \"faster, better, and more productive versions of ourselves\"?  We have today a much better understanding of what drives personal and team productivity. \n\nAnalyzing and improving individual productivity (\"employee performance optimization\") is not enough.   Organizations need to look not only at how people work, but also how they work together. \"Teams are now the fundamental unit of organization\" \n\nWhen people collaborate effectively in teams, they:\n\n- Innovate faster\n- See mistakes more quickly\n- Find better solutions to problems\n- Achieve higher job satisfaction\n- Increase profitability\n\n\n\n"
    },
    {
        "level": 2,
        "id": "1MfDXpXo",
        "title": "What makes a good team",
        "text": "Working in a teams where members jockey for the leadership position or criticize one another's ideas can often be a source of stress.  \n\nConflicts over who is in charge and who gets to represent the group may lead to teammates trying to show authority by speaking louder or talking over each other. \n\nWhat distinguishes \"good\" from \"dysfunctional\" teams is how teammates treat one another.   \"In the best teams, members listen to one another and show sensitivity to feelings and needs\"\n\n"
    },
    {
        "level": 3,
        "id": "YGFtQDVn",
        "title": "Group norms",
        "text": "Group norms are \"traditions, behavioral standards and unwritten rules that govern how we function when we gather\". They \"can be unspoken or openly acknowledged\", and \"typically override individual proclivities and encourage deference to the team. \n\nA key to improve a teams within an organization is to understand and influence group norms. \n\n\n\n"
    },
    {
        "level": 3,
        "id": "dELQJ204",
        "title": "Psychological safety is key to building an effective team",
        "text": "Two behaviors generally observed in effective teams:\n\n1. **Equal distribution of conversational turn-taking\"**: Members speak in roughly the same proportion; everyone gets a chance to talk, instead of having one person or a small group speak all the time. \n\n2. **High social sensitivity\"**: Members are \"skilled at intuiting how others feel based on their tone of voice, their expressions and other nonverbal cues.\"\n\nBoth are aspects of \"**psychological safety**\"-- when team members share the belief that it is safe to speak up without fearing that the team will \"embarrass, reject or punish\" them.  \"A team climate characterized by **interpersonal trust and mutual respect** in which people are comfortable being themselves.\" (Prof. Amy  Edmondson, 1999).  \n\nHuman bonds are as important at work as anywhere else.  Most people spend the majority of their time working; if we are not able to be open and honest at work, we are not really living a fulfilling life. Psychological safety allows to have emotional conversations at work. \"No one wants to put on a 'work face' when they get to the office. (..) No one wants to leave part of their personality and inner life at home. (...) We must be able to talk about what is messy or sad, to have hard conversations with colleagues who are driving us crazy.\"\n\n\"Google's data indicated that psychological saftey, more than anything else, was critical to making a team work.\"\n\n\n\n"
    },
    {
        "level": 3,
        "id": "rR6VySYD",
        "title": "Virtual teams",
        "text": "A virtual team is composed of members from different cultures and languages\nwho working remotely from one another.  They require intensive use of communication technology and virtual collaboration tools. \n\nThe early stages of a project are particularly challenging for virtual team, as the gathering of requirements often requires face-to-face conversations where body language and nuanced interpretation of voiced guidance are particularly important.   For instance, what would usually take a few minutes to cover in a face-to-face meeting, could take hours to explain in a series of emails or video conversations.  \n\nDelays in having questions answered may lead to a growing backlog.\n\n"
    },
    {
        "level": 3,
        "id": "8TivtZUx",
        "title": "Study groups in MBA programmes",
        "text": "\"Study groups have become a rite of passage at M.B.A. programs, a way for students to practice working in teams. (...) Business schools around the country have revised their curriculums to emphasize team-focused learning.\"\n\n"
    },
    {
        "level": 2,
        "id": "rR6VySYD",
        "title": "Virtual teams",
        "text": "A virtual team is composed of members from different cultures and languages\nwho working remotely from one another.  They require intensive use of communication technology and virtual collaboration tools. \n\nThe early stages of a project are particularly challenging for virtual team, as the gathering of requirements often requires face-to-face conversations where body language and nuanced interpretation of voiced guidance are particularly important.   For instance, what would usually take a few minutes to cover in a face-to-face meeting, could take hours to explain in a series of emails or video conversations.  \n\nDelays in having questions answered may lead to a growing backlog.\n\n"
    },
    {
        "level": 3,
        "id": "kfu8nyho",
        "title": "COVID-19 - Sudden spike in need for telecommuting",
        "text": "To limit the COVID-19 epidemic, organizations are requiring all or most of their staff work from home. This has created a huge challenge of having to manage \"a very large and sudden spike\" in the number of remote workers, even for organizations that already support certain number of telecommuters. National Statistical Offices are facing the prospect of a protracted telecommuting crisis.\n\n> \"Until now, telecommuting has been voluntary or even a reward of sorts. Not it's mandatory...\"  (Rist, 2011)\n\nSuch challenges include:\n- Meeting increased demand for IT help-desk support\n- Enabling staff to assume additional responsibilities regarding device and data security\n- Migrating on-premises workloads to cloud services\n- Providing remote access and remote management solutions for applications that need to remain served form on-premises servers\n- Adapting to remote performance tracking and virtual team collaboration\n\n"
    },
    {
        "level": 3,
        "id": "lh9J280B",
        "title": "Bandwidth requirements for telecommuting",
        "text": "Voice and video-conferencing are essential tools for effective telecommuting. However they require a minimum level of bandwidth that is not always present.\n\n\n"
    },
    {
        "level": 3,
        "id": "7dELQecN",
        "title": "Updating telecommuting policy",
        "text": "In many organizations, existing telecommuting policies were written under the assumption that telecommuting was offered only to certain employees under special circumstances.  \n\nIn the face of the COVID-19 crisis, telecommuting will be the most common work arrangement for many organizations over the foreseeable future. \n\nThere is an urgent need to review and adapt existing telecommunication policies so organizations can nimbly adjust to the new situation. \n\n- Telecommuting as the rule and not the exception\n- Less cumbersome process for formal agreements with staff regarding use of corporate infrastructure and services\n\n"
    },
    {
        "level": 3,
        "id": "yHIpNa5l",
        "title": "Online collaboration tools",
        "text": "As remote working becomes the rule rather than the exception, modern online collaboration tools are increasingly important for teams to achieve greater levels of transparency, optimize resources and deliver results.\n\nThe selection of collaboration tools should be based on the organization's business needs and budget.\n\nMost online collaboration tools support more than one means of communication:\n- Chat / instant messaging\n- Email\n- Screen sharing\n- Audio/video conferencing\n\n\n\n"
    },
    {
        "level": 2,
        "id": "JXu4aYg9",
        "title": "Impact of COVID-19 lockdown on knowledge workers' productivity",
        "text": "Due to the lockdown, day-to-day schedules of knowledge workers have changed and teams have developed new ways of working. \n\nThe results of a recent survey (Birkinshaw, Cohen and Stach, 2020) found that the COVID-19 lockdown has helped knowledge workers devote more time to interacting with customers and external partners, while reducing time spent on large meetings.  The survey also shows that, during lockdown, knowledge workers are being more intrinsically motivated and taking more personal ownership of their work.\n\nOn the negative side, working remotely makes it more difficult to kick off new projects, resolve internal conflicts, and focus on long-term staff development. \"While time spent on self-education went up...\", e.g., through webinars and online courses, this type of learning is does not encourage the same level of active experimentation and personal reflection as face-to-face interactions. \n\n"
    },
    {
        "level": 3,
        "id": "sO5SxY53",
        "title": "Knowledge workers",
        "text": "Knowledge workers are employed for what they know. They  should be evaluated on their outputs, instead of their inputs.\n\n\"Knowledge workers apply subjective judgment to tasks, they decide what to do when, and they can withhold effort ... often without anyone noticing\"\n\nIt is difficult to manage a team of knowledge workers using a traditional hierarchical, top-down approach. Instead, teams of knowledge workers are often more effective when they are self-organizing and are encouraged to come up with their own plan on how to deliver the product, with managers playing a supportive rather than directive role.  \n\n"
    },
    {
        "level": 3,
        "id": "88RTR3eB",
        "title": "Business continuity teams",
        "text": "The development and implementation of a business continuity plan requires the establishment of a cross-functional team, composed of:\n\n- Senior managemers\n- Front-line managers\n- Information Technology department\n- Legal department\n\n"
    },
    {
        "level": 1,
        "id": "8B67V92I",
        "title": "Africa's Programme on Accelerated Improvement of CRVS",
        "text": "Africa's Programme on Accelerated Improvement of Civil Registration and Vital Statistics was created under the directive of African Ministers Responsible for Civil Registration in 2010.  \n\nIts secretariat is based at UN ECA\n\n"
    },
    {
        "level": 2,
        "id": "Yx0ZUlq2",
        "title": "Decade for repositioninig CRVS in Africa",
        "text": "2017-2026 has been designated as the \"decade for repositioning CRVS in Africa\" by the Executive Councl of the African Union in Kigali. \n\n"
    },
    {
        "level": 3,
        "id": "1i1bh7AA",
        "title": "CRVS systems as a development imperative",
        "text": "There has been significant progress in recognizing CRVS systems as a development imperative\n\n"
    },
    {
        "level": 3,
        "id": "66kUhExT",
        "title": "Importance of CRVS Systems",
        "text": "CRVS Systems are crucially important to:\n- Build a modern public administration\n- Uphold human rights\n- Support national development initiatives\n- Improve service delivery to all people\n\n"
    },
    {
        "level": 2,
        "id": "1i1bh7AA",
        "title": "CRVS systems as a development imperative",
        "text": "There has been significant progress in recognizing CRVS systems as a development imperative\n\n"
    },
    {
        "level": 3,
        "id": "Gjve1WgL",
        "title": "CRVS systems and the 2030 Agenda",
        "text": "The 2030 Agenda for Sustainable Development's pleadge to leave no one behind means that no one should remain invisible.  Target 16.9 reads: \"By 2030, provide legal identity for all, including birth registration\".   \n\nHowever, many developing countries still do not have a comprehensive and complete CRVS system aligned with international standards.\n\n"
    },
    {
        "level": 2,
        "id": "66kUhExT",
        "title": "Importance of CRVS Systems",
        "text": "CRVS Systems are crucially important to:\n- Build a modern public administration\n- Uphold human rights\n- Support national development initiatives\n- Improve service delivery to all people\n\n"
    },
    {
        "level": 3,
        "id": "AMu8kLNJ",
        "title": "How does a good CRVS system look like?",
        "text": "- Universal\n- Continuing / permanent\n- Compulsory\n- Confidential\n- Every vital event (but primarily birth and death) is registered upon occurrence\n- Vital statistics are produced and used to guide policy\n\n\n"
    },
    {
        "level": 1,
        "id": "XkfA6AFU",
        "title": "Exergonic vs endergonic workflows",
        "text": "Ahrens (2017) explains that workflows can be characterized as either \"exergonic\" (requiring constant addition of energy to keep them going) or \"endergonic\" (once triggered, they continue by themselves and even release energy).\n\nGood (endergonic) workflows turn into **virtuous cycles** where the experience of becoming better at what we do motivates us to take on the next task (Ahrens, 2017, p.53). \n\nSuch workflows need to include a **learning system** based on actionable **feedback loops**.\n\n"
    },
    {
        "level": 2,
        "id": "nBYBmeNa",
        "title": "Innovation: Plans vs structured workflows",
        "text": "- **Innovation requires flexibility**. Detailed plans often impose too much structure for open-ended research or innovation projects that require flexibility.  \n- **Innovation cannot be predetermined**: Initial ideas are necessarily vague and change when we put them into practice.\n- Accidental encounters make up the majority of what we learn\n- The challenge is to have **overarching workflows** that allow for new ideas and insights to be generated, tested, adapted and mainstreamed\n\n"
    },
    {
        "level": 3,
        "id": "eac4gaf6",
        "title": "Why workflows become complicated",
        "text": "- Workflows become clogged over time as we try to apply a variety of new approaches and techniques, each promising to make something easier or better, but which combined have the opposite effects.\n- When new techniques are used without regard to the overarching workflow, \"nothing really fits together\", every little step suddenly becomes its own project, and it becomes very difficult to get things done.\n\n\n"
    },
    {
        "level": 3,
        "id": "6bvsiUUz",
        "title": "Importance of an overarching workflow",
        "text": "- It is crucial to maintain a \"holistic perspective\" so everything that needs to be taken care of is in one place and can be processed in a standardized way\n- Having a simple, overarching and streamlined workflow in place helps to stay in control by focusing on the important things and being able to pick up tasks quickly where they are left off.\n\n\n"
    },
    {
        "level": 3,
        "id": "d700Xe9w",
        "title": "Innovation is not a linear process",
        "text": "- The quest for innovation requires to constantly iterate between different tasks\n- The search for meaningful connections is a crucial part of any innovation processes (p. 114).\n- Any attempt to squeeze a non-linear process into a linear order only leads to problems and frustrations\n\n"
    },
    {
        "level": 3,
        "id": "QZ4e6WSS",
        "title": "Organization of data innovation",
        "text": "Success in data innovation depends to a large extent on the adequate organization of workflows for the practical implementation of new sources, technologies and methodologies. \n\nIt requires **breaking down the amorphous task of \"data innovation\" into  separable tasks**, which can be completed within reasonable time, and which are clearly connected to the delivery of specific, tangible outputs, and finally to the achievement of well-defined outcomes.\n\n"
    },
    {
        "level": 3,
        "id": "94L4EBHO",
        "title": "Agile project management",
        "text": "The deliverables (user stories) of a project are specified collaboratively over **short, time-blocked iterations** called sprints.  \n\nMembers of **self-organized, cross-functional teams** work together on common tasks, instead of working in isolation. Agility emphasizes frequent communication and empowerment of team members, as well as flexibility to change course at any stage of the project in order to meet the evolving stakeholders' needs. \n\nAgile project management **avoids big up-front architecture design decisions**, seeking instead to combine a small measure of up-front design with a healthy dose of emergent, just-in-time design.  Teams are encouraged to quickly explore new ideas and approaches and to learn fast whether a potential solution is viable or not. \n\nAn iterative process allows customers to discover and refine their own requirements as they gain information and knowledge in successive sprints. At the start of each sprint, the team decides which high-priority deliverables to work on so as to ensure constant forward momentum.  Each sprint includes activities for designing, building, testing, reviewing and launching specific product features, and the focus is on **delivering working features at the end of each iteration** that contribute to the final product. \n\n\n"
    },
    {
        "level": 3,
        "id": "qETSvcUN",
        "title": "Challenge planning big projects",
        "text": "When planning projects that will involve many people and will extend over a long period of time, it is difficult to predict all the activities and work streams that will be needed. \n\nManagers can't plan in advance for all the variables in a complex project.\n\n"
    },
    {
        "level": 3,
        "id": "uhh1kRu3",
        "title": "Innovation starts with what you have",
        "text": "- Nobody ever starts from scratch. Innovation projects should start with what you *have*, and not with an unfounded idea about what the data, technology or methods that you are planning to acquire or develop might eventually provide.\n\n"
    },
    {
        "level": 3,
        "id": "4BQNmycG",
        "title": "Adaptability vs. up-front planning",
        "text": "Project teams need to balance the need for up-front planning (predictive work) against the need for adaptability.  \n\nUp-front planning should be helpful without being excessive.   Lack of any planning leads to a state of constant, chaotic change.  But too much predictive planning creates sunk costs that limit adaptability to new circumstances or new information.  (It is cheaper to make changes in plans sooner than later). \n\nRequirements, designs, test cases, etc should be produced \"just in time\", keeping all options open until the last responsible moment (when the cost of not making a decision exceeds the cost of making a decision). The idea is to avoid rushing to make decisions that are difficult to reverse, and instead gathering as much information as economically feasible while avoiding costly delays in delivering value to customers.  \n\nIn other words, the goal is to keep the cost-of-change curve flat for as long as possible by limiting the volume of started-but-not-yet-finished work.  Important decisions should not be delayed for ever, but if possible should be broken down into smaller decisions, some of which can wait until more information is available to validate any underlying assumptions.\n\n\n\n\n\n"
    },
    {
        "level": 3,
        "id": "MJyDsMeW",
        "title": "Interrupt-driven environments",
        "text": "In an \"interrupt-driven\" environment, teams are not able to reliably plan work beyond one week or so--it's not possible to know what the work will be that far into the future.  \n\nNew, critical requirements come in on a continuous basis, forcing the team to abandon early plans and to re-prioritize.  At any given moment there is a high probability or receiving a high-priority request that will force to re-assign resources that had already been committed to other tasks.\n\nWhen the content and order of the product backlog changes very frequently (even hourly or every few minutes), it is impossible to plan for iterations longer than a couple of days.\n\nIn this case, scrum may not be the best alternative.  Other agile tools, such as Kanban, may be more appropriate. \n\n\n"
    },
    {
        "level": 2,
        "id": "eac4gaf6",
        "title": "Why workflows become complicated",
        "text": "- Workflows become clogged over time as we try to apply a variety of new approaches and techniques, each promising to make something easier or better, but which combined have the opposite effects.\n- When new techniques are used without regard to the overarching workflow, \"nothing really fits together\", every little step suddenly becomes its own project, and it becomes very difficult to get things done.\n\n\n"
    },
    {
        "level": 3,
        "id": "oqMxwKBR",
        "title": "Cynefin Framework",
        "text": "A framework that classify operations into 5 main types of environments:\n\n- **Chaotic**:\n  - *Strategy*: **Act -> Sense -> Respond**\n  - *What is required*: Leadership and immediate action to quickly identify a practical solution, instead of trying to find the best solution.  \n  - *Objective*: To stabilize the situation, re-establishing order and migrating to the \"complex\" domain as soon as possible\n  - *Tools*: Kanban\n\n- **Complex**:\n  - *Strategy*: **Probe -> Sense -> Respond**\n  - *What is required*: Creative / innovative approaches; a fail-safe environment for experimentation; intense interaction and communication\n  - *Objective*: Discovering patterns and documenting emerging practices\n  - *Tools*: Scrum, Kanban\n\n- **Complicated**:\n  - * Strategy*: **Sense -> Analyze -> Respond**\n  - *What is required*: Investigating several options, applying good practices, using metrics and relying on experts\n  - *Objective*: Gain insight and control; discover cause-effect relations\n  - *Tools*: Six sigma\n\n- **Simple**:\n  - *Strategy*: **Sense -> Categorize -> Respond**\n  - *What is required*: Assessing and categorizing the situation based on known facts; applying best practices\n  - *Objective*: Implementing the optimal solution\n  - *Tools*: Off-the-shelf solutions\n\n- **Disorder**: Don't know in what environment we find ourselves\n\n"
    },
    {
        "level": 3,
        "id": "Nwuk7TBb",
        "title": "Kanban",
        "text": "Kanban is an approach focused on measuring and visualizing the flow of work through the system in order to identify opportunities for continuous, gradual improvement.  It emphasizes the elimination of over-burden and the reduction of variability in workflows.  \n\nIt originated as a work scheduling system in lean manufacturing. \n\nIn contrast to scrum, kanban is well suited to manage \"interrupt-driven\" work environments. The idea is to monitor the \"work in process\" (WIP) at each step, ensuring that teams are not doing more work than they have the capacity to do.  \n\nA **kanban board** helps organize work by moving user stories in sequence, from \"to do\", to \"doing\", to \"done\".  Stories at the top of the \"to do\" column have the highest costumer value, and the team works on top stories until they are finished, before moving to next stories.  \n\n- It is important to make sure that there is not too much work under any of the three columns at any given moment. \n- All the work in the board should be self-assigned\n\n\n"
    },
    {
        "level": 3,
        "id": "JExDAHlM",
        "title": "Simplicity is paramount",
        "text": "- Big transformations start with simple ideas.  What matters is how well these simple ideas fit in the overall workflows of a system or organization.\n- To avoid undesired side effects, it is important to focus on small units of work.\n\n\n"
    },
    {
        "level": 2,
        "id": "6bvsiUUz",
        "title": "Importance of an overarching workflow",
        "text": "- It is crucial to maintain a \"holistic perspective\" so everything that needs to be taken care of is in one place and can be processed in a standardized way\n- Having a simple, overarching and streamlined workflow in place helps to stay in control by focusing on the important things and being able to pick up tasks quickly where they are left off.\n\n\n"
    },
    {
        "level": 3,
        "id": "pS33XHDA",
        "title": "Knowledge management systems",
        "text": "- We need to compensate for the limitations of our brains by relying on external structures ('scafolding') to capture ideas and supports our thinking process.\n- Knowledge management systems help keep track of ever-increasing volume of information and relieve brain capacity to focus on what is important\n\n\n"
    },
    {
        "level": 3,
        "id": "nDxqu3EV",
        "title": "Lessons from the shipping container",
        "text": "Ahrens (2017) explains how the initial attempts to introduce the use of the shipping container --a very simple solution--failed as long as ship owners failed to change their infrastructure and routines and to recognize that what mattered was the entire transport chain, from packaging of goods at the point of production to their delivery at the final destination.\n\n> It wasn\u2019t just another way of shipping goods. It was a whole new way of doing business. \n>(Ahrens, 2017, p. 40) \n\nSimilarly, simple innovations in statistical production can only be mainstreamed if they are accompanied by necessary changes and adaptations along the whole data value chain.\n\nFor example... (?)\n\n"
    },
    {
        "level": 1,
        "id": "Dl66v10U",
        "title": "Scrum",
        "text": "Scrum is an empirical **framework** consisting of values, principles and practices for organizing and managing work. Scrum is an agile approach for delivering high-quality products and services that maximize customer value in a timely and economical manner.  \n\nIt focuses on delivering smaller, more frequent releases that \"give customers what they really want, not just features they might have specified on the first day when they knew the least about their needs.\"\n\n\"Scrum ... assumes that the process necessary to create the product is complex and therefore would defy a complete upfront definition.\"\n\nIt is an **empirical** framework, in the sense that continuously **inspects** results to derive lessons learned and **adapt** processes and practices. \n\n\n"
    },
    {
        "level": 2,
        "id": "cPIa75Kg",
        "title": "How Spotify customized the use of Scrum",
        "text": "Spotify decided to make some of the standard scrum practices (e.g., sprint planning meeting, task breakdown, estimation, \u2026) optional.  They decided that \u201cAgile matters more than Scrum\u201d and that \u201cagile principles matter more than specific practices\u201d.  Some of the changes they introduced include:\n\n- Spotify renamed the role of \u201cscrum master\u201d to \u201cagile coach\u201d, emphasizing the need of servant-leaders rather than process masters, and instead of \u201cscrum teams\u201d they refer to their cross-functional, self-organizing teams as \u201cautonomous squads\u201d.  Each squad decides what to build, how to build it, and how to work together, within the limits of the squad\u2019s long-term mission, the product strategy, and short-term goals that are negotiated every quarter.  \n- Squads are loosely coupled but tightly aligned teams.  \u201cAlignment enables autonomy\u201d. The leader\u2019s job is to communicate what problem needs to be solved and why. The squad\u2019s job is to collaborate with each other to find the best solution. \n- Squads are grouped into \u201ctribes\u201d. Each person is simultaneously a member of a squad (product dimension), and a chapter (a competency area) that cuts across multiple squads in a tribe. \n- There are also \u201cguilds\u201d \u2013 or light-weight, company-wide communities of interest where people share knowledge on a specific area (leadership, web development, \u2026) through mailing lists, bi-annual conferences, and other informal communication methods. \n- Cross pollination rather than standardization: Instead of prescribing the use of particular tools, When enough squads use the same tool, that becomes a de-facto standard. This allows for a healthy balance between consistency and flexibility.\n- Internal open-source model. Anyone can edit any code. However, there is a culture of peer code review, which improves quality and promotes knowledge sharing.  \n- Culture of trust and mutual respect. Give credit to each other for great work, and seldom take credit for oneself. Agile at scale requires trust at scale, and that means no politics, and no fear. \n- Focus on speed: Small and frequent releases; investment in test automation and continuous delivery infrastructure. \n- Three types of squads:\n  - Feature squads: Focus on building features in one specific area (e.g., \u2018search\u2019), to be released across all platforms\n  - Client app squads: Focus on making all feature releases on one specific platform easy (e.g., desktop, iOS, Android)\n  - Infrastructure squads: Focus on providing tools and routines that make other squads more effective (e.g., continuous delivery, testing\u2026) \n\n\n"
    },
    {
        "level": 3,
        "id": "uRu9vpcC",
        "title": "Scrum practices",
        "text": "Scrum practices include:\n\n1. Roles:\n    - **Product owner**:\n      - Executes product leadership, deciding what features and functionalities will be developed and in what order\n      - Maintains and communicates a clear vision of what the scrum team is trying to achieve\n      - Makes sure that the most valuable work is always performed first\n      - Is available to answer questions of the team as soon as they are posted\n    - **Scrum master**:\n      - Helps everyone understand and embrace scrum values, principles and practices\n      - Protects the team from outside interference\n      - Works to remove impediments that inhibit team productivity (when the team cannot reasonably resolve them)\n      - Does administrative work necessary to make sure everyone in the team has what they need to do their work\n      - Acts as a coach and leader during change-management process\n      - Helps the team resolve issues and make improvements in their scrum\n      - Facilitates self-management and guides the team in creating and following its own process\n    - **Development team**:\n      - Is responsible for designing, building, and testing the desired product\n      - Self-organizes to determine the best way to deliver what the product owner has asked for\n\n2. Activities\n   - Product backlog grooming\n   - **Sprint planning**:\n      - At the beginning of each sprint, the product owner, the scrum master, and the development team must jointly determine the subset of product backlog items they believe can completed in the next sprint. They also agree on a sprint goal and commit to deliver a specific set of features, creating a sprint backlog. \"Sprint planning results in both a forecast and a commitment.\"  This commitment helps build trust between the product owner and the development team, supports short-term planning and decision making in the organization, and enables teams to make decisions based on what other teams have committed to.\n   - **Sprint execution**:\n      - The development team, coached by the scrum master, performs all the task-level work specified in the sprint backlog.  Through daily scrum meetings, the team members help mange the flow of work. \n   - **Sprint review**:\n      - Stakeholders and scrum team inspect the product being built in a scheduled meeting, where they inspect the product features just completed and discuss to guide and adapt forthcoming development efforts. It is also an opportunity to get a deeper appreciation of the business side needs and expectations.\n   - **Sprint retrospective**:\n      - After the sprint review, and before the next sprint planning meeting, the scrum team inspects the scrum process being used to create the product. As a result of the retrospective, the team identifies and commits to a practical number of process improvement actions for the next sprint.\n\n3. Artifacts\n   - **Product backlog**\n   - **Sprint backlog**: Features of the product backlog that should be delivered by the end of the current sprint, broken down into more granular tasks, each of which has a detailed description and an effort-hour estimate.  \n\n4. Rules\n  - As a general rule, no changes in scope or personnel are permitted during the sprint\n  - Sprints always have a fixed start and end date, and are generally of the same duration (in order to establish a cadence)\n\n\n"
    },
    {
        "level": 2,
        "id": "EO7wzRFs",
        "title": "Lean concepts: Continuous improvement",
        "text": "Focus on what is valuable to the customer and then continuously improve the product over time\n\n"
    },
    {
        "level": 3,
        "id": "rZRfKJd2",
        "title": "Lean concepts: Respect for people",
        "text": "Emphasize people over process; respect people and trust them to deliver high-quality work\n\n"
    },
    {
        "level": 3,
        "id": "nJYCl4LV",
        "title": "Quality testing is a continuous task",
        "text": "Quality is not something that should be delegated to a \"test team\" at the end of the process.  \n\nQuality should be owned by the whole cross-functional team, and be continuously built in and verified in every iteration of the product. \n\n"
    },
    {
        "level": 2,
        "id": "uRu9vpcC",
        "title": "Scrum practices",
        "text": "Scrum practices include:\n\n1. Roles:\n    - **Product owner**:\n      - Executes product leadership, deciding what features and functionalities will be developed and in what order\n      - Maintains and communicates a clear vision of what the scrum team is trying to achieve\n      - Makes sure that the most valuable work is always performed first\n      - Is available to answer questions of the team as soon as they are posted\n    - **Scrum master**:\n      - Helps everyone understand and embrace scrum values, principles and practices\n      - Protects the team from outside interference\n      - Works to remove impediments that inhibit team productivity (when the team cannot reasonably resolve them)\n      - Does administrative work necessary to make sure everyone in the team has what they need to do their work\n      - Acts as a coach and leader during change-management process\n      - Helps the team resolve issues and make improvements in their scrum\n      - Facilitates self-management and guides the team in creating and following its own process\n    - **Development team**:\n      - Is responsible for designing, building, and testing the desired product\n      - Self-organizes to determine the best way to deliver what the product owner has asked for\n\n2. Activities\n   - Product backlog grooming\n   - **Sprint planning**:\n      - At the beginning of each sprint, the product owner, the scrum master, and the development team must jointly determine the subset of product backlog items they believe can completed in the next sprint. They also agree on a sprint goal and commit to deliver a specific set of features, creating a sprint backlog. \"Sprint planning results in both a forecast and a commitment.\"  This commitment helps build trust between the product owner and the development team, supports short-term planning and decision making in the organization, and enables teams to make decisions based on what other teams have committed to.\n   - **Sprint execution**:\n      - The development team, coached by the scrum master, performs all the task-level work specified in the sprint backlog.  Through daily scrum meetings, the team members help mange the flow of work. \n   - **Sprint review**:\n      - Stakeholders and scrum team inspect the product being built in a scheduled meeting, where they inspect the product features just completed and discuss to guide and adapt forthcoming development efforts. It is also an opportunity to get a deeper appreciation of the business side needs and expectations.\n   - **Sprint retrospective**:\n      - After the sprint review, and before the next sprint planning meeting, the scrum team inspects the scrum process being used to create the product. As a result of the retrospective, the team identifies and commits to a practical number of process improvement actions for the next sprint.\n\n3. Artifacts\n   - **Product backlog**\n   - **Sprint backlog**: Features of the product backlog that should be delivered by the end of the current sprint, broken down into more granular tasks, each of which has a detailed description and an effort-hour estimate.  \n\n4. Rules\n  - As a general rule, no changes in scope or personnel are permitted during the sprint\n  - Sprints always have a fixed start and end date, and are generally of the same duration (in order to establish a cadence)\n\n\n"
    },
    {
        "level": 3,
        "id": "DCfr2zpK",
        "title": "Estimating size of features for product backlog",
        "text": "Product owners need to know an item's cost to properly determine its priority.  \n\nScrum teams usually employ a relative size measure (e.g., story points or ideal days) to estimate the size of a product feature. \n\n"
    },
    {
        "level": 3,
        "id": "14mcJE3E",
        "title": "User stories",
        "text": "User stories are a \"note\" to have a future conversation.  Their purpose is to get the planning process started, by helping the team ask the right questions and stay focused on value delivery. \n\nA user story is a non-technical, short description of a feature, written in the language of the customer.  \n\n"
    },
    {
        "level": 3,
        "id": "lu7UXT1g",
        "title": "Inspection and adaptation",
        "text": "It is important to inspect and adapt not only \"what\" we are building (product), but also \"how\" we are building it (process).  \n\n"
    },
    {
        "level": 3,
        "id": "PoFWMOPo",
        "title": "Agile approach",
        "text": "Agile is an empirical approach for value delivery through team collaboration, based on a series of values, principles, practices and tools.\n\n- Teams are cross-functional and self-organizing\n- Upfront planning is limited to what is absolutely necessary to get the work started\n- The starting point is a product backlog, and the team always works  in short, time-boxed iterations / increments, focusing first on the most important or highest-priority items\n- At the end of each iteration:\n  - The team delivers a potentially shippable product\n  - The team reviews the completed features with the stakeholders to get their feedback\n- Based on feedback, product owner and team can change priorities on what to work next and decide how to do the work\n- There is no multi-tasking: everybody is focused on moving forward the same ball, minimizing hand-offs\n- Teams strive to work at a predictable, steady and sustainable pace\n- \"Inspect and adapt\" activities focus on continuous improvement, both of products and processes\n\n\n"
    },
    {
        "level": 3,
        "id": "sqiecPWI",
        "title": "Daily scrum / daily standup",
        "text": "A daily, timeboxed (15-minutes-or-less) meeting of the development team members, facilitated by the scrum master, to discuss:\n\n- What did I accomplish since the last daily scrum?\n- What do I plan to work on?\n- What obstacles or impediments are preventing me from making progress?\n\nThis is a synchronization, inspection and adaptive planning activity; not a traditional status meeting, nor a problem-solving meeting.\n\n"
    },
    {
        "level": 3,
        "id": "r6RqDLiO",
        "title": "Sprint",
        "text": "Time-boxed (weekly to monthly) iterations of work that deliver something of value to the customer. \n\nSprints have always fixed start and end dates, and are generally of the same duration.  \n\nNo goal-altering changes are permitted during a sprint\n\nWorking in short, regular sprints eliminates uncertainty by delivering value predictably every few weeks.\n\n"
    },
    {
        "level": 3,
        "id": "LQ7sc3RD",
        "title": "Definition of done",
        "text": "Baseline criteria to establish with confidence that the work completed is of good quality and potentially shippable\n\n- For example: \"A complete slice of functionality that is designed, built, integrated, tested and documented\"\n\n- In early, exploratory stages of a project, it could mean \"a slice of functionality that is sufficiently usable to generate actionable feedback\"\n\n"
    },
    {
        "level": 3,
        "id": "TlYLFIPL",
        "title": "Product Owner role",
        "text": "The product owner is responsible to manage the backlog of items that form the requirements and the shared understanding of the product's problem and solution.  \n\nThe product owner represents the customer, but does not only \"hand off\" requirements.  Instead, the product owner sets the direction for the development efforts, defines priorities, and shares responsibility with the development team for the success of the project. \n\n\n\n"
    },
    {
        "level": 2,
        "id": "PoFWMOPo",
        "title": "Agile approach",
        "text": "Agile is an empirical approach for value delivery through team collaboration, based on a series of values, principles, practices and tools.\n\n- Teams are cross-functional and self-organizing\n- Upfront planning is limited to what is absolutely necessary to get the work started\n- The starting point is a product backlog, and the team always works  in short, time-boxed iterations / increments, focusing first on the most important or highest-priority items\n- At the end of each iteration:\n  - The team delivers a potentially shippable product\n  - The team reviews the completed features with the stakeholders to get their feedback\n- Based on feedback, product owner and team can change priorities on what to work next and decide how to do the work\n- There is no multi-tasking: everybody is focused on moving forward the same ball, minimizing hand-offs\n- Teams strive to work at a predictable, steady and sustainable pace\n- \"Inspect and adapt\" activities focus on continuous improvement, both of products and processes\n\n\n"
    },
    {
        "level": 3,
        "id": "ERfoyuMI",
        "title": "Being fast vs. being hurried",
        "text": "The outcome is enhanced with speed.  However, hurrying with the intent of getting things done substantially reduces effectiveness.  High-performance teams move swiftly, nimbly and deliberately while carving out enough time for inspect-and-adapt activities along the way. \n\nThe objective is to deliver small increments quickly, in order to obtain feedback fast and to get value into the hands of the customer sooner.  \n\nThe point of velocity in agility is the ability to learn and react quickly. \n\n"
    },
    {
        "level": 3,
        "id": "wXeI3ZuL",
        "title": "Reducing requirements uncertainty thorugh probing and exploration",
        "text": "At the beginning of any project, many requirements are specified without sufficient knowledge about the desired features of the final product, the means to achieve them, or even knowledge of who the final customer is (low-quality requirements). \n\nIt's impossible to get all the requirements right up front; and even if at the beginning of a project the requirements were perfectly specified, changes in the environment during execution can make them obsolete before value is delivered to the customer.\n\nAgile teams try to specify just enough requirements up front so they can start building working solutions that allow them to obtain early feedback from stakeholders to fill-in the details, validate assumptions and learn about any unknown unknowns. Moreover, in the face of too much uncertainty, agile teams may choose to \"learn by doing\", building a prototype, creating a proof-of-concept model, performing a study or conducting an experiment. \n\n"
    },
    {
        "level": 3,
        "id": "8IoQFIMa",
        "title": "Cone of uncertainty",
        "text": "Our knowledge of what it takes to deliver a product increases as we near the final stages of production.  Therefore, many of the decisions that are made at the start of a project will need to be revised later on, as more is learned about the product, the process, the client, etc. \n\nOn the other hand, decisions are usually more expensive to change the more time has passed since they were made. \n\n**Delaying decisions to the \"latest responsible moment\"**\nThe objective is to make *just enough* early decisions so the work can get started, while keeping as much flexibility as possible. \n\n"
    },
    {
        "level": 3,
        "id": "sYzTxdf1",
        "title": "Short feedback loops",
        "text": "Fast feedback closes the learning loop quickly and allows to truncate bad development paths before they can cause serious economic damage.  Errors compound when we delay feedback, resulting in exponentially larger failures. The longer we validate assumptions, the greater the number of risky dependencies built into the product.  On the other hand, fast feedback allows to quickly uncover and exploit time-sensitive opportunities. \n\n"
    },
    {
        "level": 3,
        "id": "Yfs8Mlld",
        "title": "Thinking like an agile team",
        "text": "- ask tough questions about the way they work and be open about continuous improvement.  \n- don't focus only on the practices.  Focus on the **why** of these practices.\n- embrace changing requirements and experimentation\n- emphasize having the right conversations, not so much having the right format\n- think in terms of products instead of projects\n \n\n"
    },
    {
        "level": 3,
        "id": "OW8rpKkt",
        "title": "Overview of Agile principles",
        "text": "- Change is welcome (Principle 2)\n- Progress is measured in terms of continuous delivery (Principles 3 and 7)\n- Developers and customers work closely together (Principle 4)\n- Small, motivated team of generalists are self-organized (Principles 5 and 11)\n- Face-to-face interaction is maximized (Principle 6)\n- High-value work is prioritized (Principles 1, 8, 9, and 10)\n- Process improvement is discussed frequently (Principle 12)\n\n"
    },
    {
        "level": 3,
        "id": "LzaHiJbM",
        "title": "Incremental development",
        "text": "Break down deliverables into small increments. Each increment covers the full development cycle (analysis/design/build/test/integrate) for a specific feature, allowing to obtain feedback, to validate assumptions and to adapt.  \"In scrum we don't work on a phase at a time; we work on a feature at a time.\"\n\n\n"
    },
    {
        "level": 3,
        "id": "OEaPVROo",
        "title": "Rapid results initiatives",
        "text": "Series of mini-projects weaved into the overall plan of a complex project, each of which is staffed with cross-functional team responsible for a delivering a version of the expected overall result in a short period of time (generally no more than 100 days).\n\nWhile rapid results projects deliver quick wins, their main value lies in that they change the way teams approach their work: while leaders establish goals, frontline workers are responsible to figure out how to achieve them.\n\nTheir short time frame forsters a sense of personal challenge and a sense of urgency among team members right from the start, leaving no time to squander on big studies or inter-organizational bickering. \n\n\n\n"
    },
    {
        "level": 2,
        "id": "sqiecPWI",
        "title": "Daily scrum / daily standup",
        "text": "A daily, timeboxed (15-minutes-or-less) meeting of the development team members, facilitated by the scrum master, to discuss:\n\n- What did I accomplish since the last daily scrum?\n- What do I plan to work on?\n- What obstacles or impediments are preventing me from making progress?\n\nThis is a synchronization, inspection and adaptive planning activity; not a traditional status meeting, nor a problem-solving meeting.\n\n"
    },
    {
        "level": 1,
        "id": "7UvCpDNd",
        "title": "Data pipeline requirements",
        "text": "Some important data pipeline requirements include:\n\n- **Low event latency**: Ability to query the data as soon as it has been collected\n- **Interactive querying**: Support both batch queries and smaller interactive queries allowing data scientists to explore the tables and schemas\n- **Scalability**: Handle increasing data volumes as the project scales\n- **Versioning**: Ability to make changes to the pipeline without bringing it down and losing data\n- **Monitoring**: Generate alerts when data stops coming into, or flowing through, the pipeline.\n- **Testing**: Ability to perform tests on the pipeline without interrupting the work of others\n- **Clear distinction between development and production environments**: The project should not interfere with daily business operations\n\n"
    },
    {
        "level": 2,
        "id": "zDox7KV4",
        "title": "Data migration patterns",
        "text": "In a typical data migration / integration architecture, one or more source systems deliver a document or message to a middleware component (ETL tool) that then pushes to one or more target systems. \n\n**\"Kill and fill\"**: First delete existing data in the target system, and then  replace it with new data \n\n"
    },
    {
        "level": 3,
        "id": "MhXSqPKk",
        "title": "Managing ETL pipelines",
        "text": "A core component of any data innovation project is to set up a scalable data architecture, including a set of scalable ETL pipelines that move data from one system to another.  \n\nOne of the main roles of a data engineer is to design, build and run ETL processes (data pipelines) that send input data to data lakes, data warehouses and subscription services for use in the development/compilation of data products.\n\nKey questions that need to be investigated before deploying a data pipeline include:\n\n- Who owns the data pipeline?\n- Which teams will be consuming the data?\n- Who will monitor and maintain the the pipeline? (e.g., will the maintenance of the pipeline be under the responsibility of the data users, or will there be an infrastructure team keeping it operational?\n- What data pipeline and workflow automation tools are best suited to the purpose at hand?\n\n"
    },
    {
        "level": 2,
        "id": "MhXSqPKk",
        "title": "Managing ETL pipelines",
        "text": "A core component of any data innovation project is to set up a scalable data architecture, including a set of scalable ETL pipelines that move data from one system to another.  \n\nOne of the main roles of a data engineer is to design, build and run ETL processes (data pipelines) that send input data to data lakes, data warehouses and subscription services for use in the development/compilation of data products.\n\nKey questions that need to be investigated before deploying a data pipeline include:\n\n- Who owns the data pipeline?\n- Which teams will be consuming the data?\n- Who will monitor and maintain the the pipeline? (e.g., will the maintenance of the pipeline be under the responsibility of the data users, or will there be an infrastructure team keeping it operational?\n- What data pipeline and workflow automation tools are best suited to the purpose at hand?\n\n"
    },
    {
        "level": 3,
        "id": "2GTwGKsm",
        "title": "ETL Frameworks and tools",
        "text": "Developing and implementing ETL (extract-tranform-load) processes is \"time-consuming, brittle, and often unrewarding\" (Beauchemin, 2018)\n\nThe data flows of real-life statistical production processes can be very complex, and ETL frameworks and tools help address common problems in building ETL pipelines:\n\n- Documentation - Succinctly describe the data flow\n- Automation/Scheduling\n- Monitoring - Track the progress of long running processes and alert when errors arise\n- Backfilling - Ability to re-process historical data\n\nETL Frameworks are often implemented in Python (e.g., Airflow and Luigi).  Examples include:\n\n  - **Google's Cloud Composer**: Python-based workflow orchestration service to connect data, processing, and services between on-premises and the public cloud. Built on Apache Airflow. Pipelines are configured as directed acyclic graphs (DAGs). Includes library of connectors and multiple graphical representations of workflows.  Allows to set up a continuous integration/continuous deployment (CI/CD) pipeline for processing data on Google Cloud.\n\n  - **Google's Cloud Dataflow**: Unified, serverless stream and batch data processing service.  Allows for flexible resource scheduling (FlexRS).\n\nThere are plenty of \"drag and drop\" data pipeline and workflow automation tools that do not require knowledge of the underlying code.:\n\n- **Informatica** - Offers a portfolio of data integration products as well as tools for master data management, data quality, data cataloging, and API management. \n\n- **SQL Server Integration Services (SSIS)** - Microsoft tool for data integration tied to SQL Server.\n\n- **Stitch** - cloud-based ETL platform \n\n- **Google's Cloud Data Fusion**: Cloud-native, scalable data integration service with a visual point-and-click interface, enabling code-free deployment of ETL/ELT data pipelines.  Library of 150+ preconfigured connectors and transformations, plus ability to create custom connections and transformations that can be validated, shared, and reused across teams.\n\n"
    },
    {
        "level": 3,
        "id": "OX8Uv2GF",
        "title": "Components of a data integration process",
        "text": "**Target system** receive records from multiple **source systems**. \n\n**Data profiling** is used to discover and document the structure, content, and relationships among source data.  \n\n**Data matching** is used to identify records in different source systems that refer to the same entity\n\n**Source-to-target mapping** provides the metadata and transformation rules needed to convert data from each source system into the structure and content required by the target system.  (building correspondences between different sources, including correspondences between sources with different geographic and temporal granularity)\n\n**Business rules for ETL processes** are criteria and conditions for transforming data from source to target, including exception handling \n\n**Testing source data** allows to discover inconsistencies with respect to ETL business rules\n\n**Version control system** records changes to ETL pipelines and data over time, allowing to recall specific versions at any given moment. \n\n"
    },
    {
        "level": 1,
        "id": "6WFYziDD",
        "title": "Comparability across data sources",
        "text": "The comparability of variables measured across different data sources is a key data quality issue.  \n\nChanges over time that may adversely impact comparability of variables across different data sets include:\n\n- Changes in the definition and coverage of reference geographic areas (e.g., creation or re-drawing of boundaries of new administrative units)\n- Changes in statistical classifications\n- Changes in sampling methodology\n- Changes in definition of reference time periods (e.g., calendar vs. fiscal year)\n- Changes in survey questions or measurement instruments\n\n"
    },
    {
        "level": 2,
        "id": "yHy01yYb",
        "title": "Assessing quality of source data",
        "text": "The choice of data sources for the production of statistics should be based on multiple criteria, including:\n\n- Cost effectiveness / Sustainability of the data source\n- Extent to which the source satifies the information needs of users (relevance)\n- Verify quality of individual data sources\n  - Appropriate concepts and definitions viz-a-viz the question at hand\n  - Level of disaggregation\n    - Geographic granularity\n    - Temporal frequency\n    - Disaggregation by sex, age, product, activity, etc.\n  - Availability / Completeness of data\n    - Missing values\n    - Absence of values for key variables\n    - % of empty cells\n  - Timeliness\n  - Coverage / Sample bias / representativity: Verify that the underlying population of the data generating process is consistent with the statistical output requirements \n    - % or records in the data that do not belong to the target population\n    - % of units in the target population that are not represented in the dataset\n    - Population subgroups that are over- or under-represented in the dataset\n  - Interoperability:\n    - Machine readability; standard serialization / formatting; use of standard classifications\n  - Internal consistency / Outlier detection\n  - Completeness and clarity of reference metadata\n  - Units of measurement are appropriate, well defined, identified\n- Verify comparability / consistency across data sources\n  - Consistent use of definitions of geographic areas\n  - Alignment of reference periods\n  - Consistent use of variable definitions\n  - Consistent use of standard classifications / levels of disaggregation\n- Data integrity / data security\n  - Duplicate records\n  - Processes in place to mitigate risk of disclosure of confidential information\n  - Security and integrity of data and their transmission\n\n\n"
    },
    {
        "level": 3,
        "id": "9h1AmchL",
        "title": "Establishing trust in results of data innovation projects",
        "text": "It is crucial to establish trust in the results of data innovation projects, particularly from key users such as policy and decision makers.\n\nThe quality of estimation results \"is only as good as the quality of the input data and the methodologies employed.\"  Therefore, the reliability of the sources and methods involved in data innovation projects is central to their widespread acceptance and use. \n\nThis highlights the importance of **transparent and participatory validation**:\n\n- Validate quality of data inputs\n- Validate methodology\n- Ensure that data production process are reproducible by independent reviewers\n- Check for internal and external consistency of results\n- Compare pre-existing users' perceptions with project results\n\n\n"
    },
    {
        "level": 3,
        "id": "nqqQlCkb",
        "title": "Poverty maps production: data sources",
        "text": "The production of small-area poverty estimates usually relies on the following two major data sources of household welfare:\n\n1. Detailed household surveys which collect a measure of welfare (typically consumption per capita)\n2. A national census or large national survey that includes a significant share of the country's population\n\nOther data sources that may be used to approximate individual welfare (in approximately real time):\n\n- Individual consumption of mobile phone services\n- Measures of mobility (e.g., derived from mobile phone records)\n- Social network metrics (e.g., derived from social media or mobile phone records)\n- Financial transactions (e.g., derived from mobile phone or credit card records)\n\n"
    },
    {
        "level": 3,
        "id": "oBtYWEfG",
        "title": "Data profiling",
        "text": "Process of reviewing source data to discover its structure, content and relationships. It involves:\n\n- Calculating descriptive statistics (e.g., min, max, count, sum...).\n- Collecting data types, length and recurring patterns.\n- Tagging data with keywords, descriptions or categories.\n- Assessing data quality.\n- Discovering metadata and assessing its accuracy.\n- Identifying key candidates, foreign-key candidates, functional dependencies, embedded value dependencies, \n- Performing inter-table analysis.\n- Document connection/access details\n  - If the data is available as text file, is it comma delimited, tab-delimited, or something else?\n  - If the data is available from a database, what are the connection details?\n  - If the data is available through a web service, what are the API endpoints and in what is the serialization format of the response message?\n  - What are the authentication mechanisms granting access to the data?\n\n\n**Data profiling good practices**\n\n1. Conduct data profiling at project start to discover if data is suitable for analysis\u2014and make a \u201cgo / no go\u201d decision.\n2. Identify and correct data quality issues in source data, even before starting to move it into target database.\n3. Identify data quality issues that can be corrected by Extract-Transform-Load (ETL), while data is moved from source to target. \n4. Identify any additional manual processing that might be needed.\n5. Identify unanticipated business rules, hierarchical structures and foreign key / private key relationships, use them to fine-tune the ETL process.\n\n**Basic data profiling techniques**\n\n- Distinct count and percent\u2014identifies natural keys, distinct values in each column that can help process inserts and updates. \n- Percent of zero / blank / null values\u2014identifies missing or unknown data. \n- Minimum / maximum / average string length\u2014helps select appropriate data types and sizes in target database. \n\n**Advanced data profiling techniques**\n\n- Key integrity\u2014ensure keys are always present and identify orphan keys\n- Cardinality\u2014check for one-to-one, one-to-many, many-to-many relationships between related data sets. \n- Pattern and frequency distributions\u2014check if data fields are formatted correctly (e.g., emails, postal codes...)\n- Record matching\u2014Identify records that refer to the same entity by analyzing across fields that provide partial identification\n\n"
    },
    {
        "level": 3,
        "id": "Lr60zbkO",
        "title": "Understanding multiple sources of data",
        "text": "No single individual (or organization) is ever familiar with all the attributes and caveats of all the data inputs that are required in a data innovation project.   Therefore, it is crucial to involve from the beginning all relevant experts who have helped produced various data inputs.\n\n"
    },
    {
        "level": 3,
        "id": "QJsx3KmY",
        "title": "Data innovation road map",
        "text": "A road map of consisting of country-specific activities, intermediate outputs and expected outcomes\n\n**Pre-production:**\n\n1. Define **scope**\n1. Identify **key stakeholders**\n1. Build **suport** \n1. Reach out to **potential users**\n1. Secure **resources**\n1. Establish **project team**\n1. Procure/setup/configure the necessary **hardware and software** tools\n1. Secure **access to data** inputs\n1. Design, build and run source data integration process\n1. Assess **quality of source data**\n1. Transform original source data into **analysis-ready datasets**\n1. Set up **data inputs clearinghouse**\n1. Provide **practical training**\n\n**Production:**\n\n1. Compute intermediate indicators to be used as (geospatial) covariates in statistical estimation models (e.g., distance to service-delivery points, distance to roads, elevation, ...)\n\n\n**Post-production:**\n\n1. Communicate project outputs to key audiences \n\n"
    },
    {
        "level": 3,
        "id": "dva8i69T",
        "title": "Standard classifications",
        "text": "Standard classifications enable the integration of multiple data sets, as well as their consistent analysis and interpretations.  \n\nThey provide the taxonomical basis for managing and describing statistical data, and are fundamental components of key frameworks for the compilation of official statistics (e.g., System of National Accounts).\n\nStatistical classifications need to be easy to understand and to use, and be revised periodically to remain relevant in a changing world.\n\nThey also need to be accessible from a central repository, and leverage new metadata modelling approaches and linked open data technologies. \n\n\n\n"
    },
    {
        "level": 3,
        "id": "hp8Foh4V",
        "title": "Capacity bottlenecks in data innovation projects",
        "text": "Common capacity bottlenecks in data innovation projects include:\n \n- Inadequate **access to data** sources\n- Shortage of **facilities or equipment**\n- Gaps in technical **skills**\n- Lack of **staff time**\n- **Data quality** issues\n- Shortage of modern **software** tools\n\n\n"
    },
    {
        "level": 3,
        "id": "81UJY4nw",
        "title": "Data validations",
        "text": "Some automatic data validations tests may include:\n\n- No duplicates in primary key\n- No orphan foreign keys\n- Only valid values within each field\n- All values in a field comply with formatting rules\n- Values of various field are consistent with each other \n\n"
    },
    {
        "level": 3,
        "id": "cTjLfkWN",
        "title": "How to improve interoperability across data sources",
        "text": "To make various data sources ready-to-use, it is necessary to transform them to ensure they are interoperable. This includes:\n\n- Use of canonical data models\n- Geo-reference all data inputs using common boundaries and use consistent location-identification codes \n- Use common vocabularies, classifications and code lists\n- Develop of standardized API documentation\n\n\n"
    },
    {
        "level": 3,
        "id": "aeKIUGRa",
        "title": "Quality",
        "text": "Quality is the degree to which the characteristics of a product or service meet the needs of its users.  \n\nSince different users have different, multiple needs that must be balanced against each other, a practical analysis of quality must break this definition down into multiple dimensions, so as to address different, but inter-related and often conflicting, types of user needs.\n\n"
    },
    {
        "level": 3,
        "id": "Eg5ejSSM",
        "title": "Lean concepts: Quality at the source (Jidoka)",
        "text": "\"Lean teaches us that quality inputs lead to quality outputs.  Focus on inputs allows for actionable metrics and direction for continuous improvement. (...)  \n\nA by-product of lean's overall drive for continuous improvement is teamwork, as \"the borders around responsibility and blame fade quickly when each person strives for improvement\".\n\n\n\n"
    },
    {
        "level": 3,
        "id": "DdmZ5wq7",
        "title": "Data integration challenges",
        "text": "\"Some enterprise landscapes are filled with disparate data sources including multiple data warehouses, data marts, and/or data lakes, even though a Data Warehouse, if implemented correctly, should be unique and a single source of truth.\" \n\nThe simple task of moving data from A to B \"can turn quite messy very fast and without warning\". \n\n- Lack of standardization of ETL solutions and tools\n- Mismatch between data from source system, existing ETL pipelines, and target system schema.\n- Insufficient, incomplete, or poorly communicated metadata\n- Data quality issues\n\nNeed to research, document and test inbound data\n\n"
    },
    {
        "level": 2,
        "id": "Lr60zbkO",
        "title": "Understanding multiple sources of data",
        "text": "No single individual (or organization) is ever familiar with all the attributes and caveats of all the data inputs that are required in a data innovation project.   Therefore, it is crucial to involve from the beginning all relevant experts who have helped produced various data inputs.\n\n"
    },
    {
        "level": 3,
        "id": "bR2Woaf4",
        "title": "Types of source data required for nowcasting",
        "text": "Nowcasting is about producing \"near real-time\" estimates.  This means being able to project in over time the values of variables measured in the past.\n\nThis in turn requires to leverage any \"panel components\" in input data form household surveys, administrative records, etc., which track the same individuals, households or statistical units with repeated measurement of the same variable(s) at different moments in time.\n\n"
    },
    {
        "level": 2,
        "id": "tltgpLsL",
        "title": "Levels of disaggregation of administrative units",
        "text": "The hierarchy of political divisions of a country's territory is comprised by administrative units, each of which is delineated by specific geographic boundaries.\n\n- The country is the highest-level administrative unit, and it is referred to as the \"**Level 0**\" administrative unit.  \n- The first level of subdivision of administrative units within a country is referred to as '**Level 1**\".\n- Administrative units at further levels of geographic disaggregation are denoted as \"**Level 2**\", \"**Level 3**\", etc.\n\n\n\n"
    },
    {
        "level": 3,
        "id": "qpy6M6vh",
        "title": "Geo-spatial disaggregation",
        "text": "The geographic scale at which data are disaggregated affects the type of information that can be derived from it.  \n\nData collected at more granular level of geographic detail often provide more useful information about patterns of geographic variation and correlation, and can be more flexibly aggregated into broader geographic areas suitable for different types of analysis.\n\nExamples of geographic scales:\n\n- Administrative unit (level 0, level 1, level 2...)\n- Populated place\n- Point location\n- Grid location\n- Area/Line features\n\nDefining a minimum level of geospatial disaggregation facilitates data integration and analysis.  Such minimum level of geospatial disaggregation should be established considering factors such as:\n\n- **Analytic objectives** (e.g., informing policies or decisions at national or local levels)\n- **Country context** (e.g., how large are the different levels of administrative units)\n- Expected **variability** within and across different units at each level of disaggregation (e.g., too much variability within units / too little variability between units may require more granular disaggregation)\n- Type of **data collection process** (e.g., earth observation, on-site measurements, or administrative/business records). \n- Potential **privacy** implications (e.g., whether combining data at a specific level of geographic disaggregation with other datasets could lead to re-identification of personal or individual-level information)\n- **Resources** (e.g., expertise and technology required to collect, process, and analyze data at a specific level of geographic desaggregation).\n\n"
    },
    {
        "level": 3,
        "id": "etUj8mX6",
        "title": "Map visualizations",
        "text": "Map visualizations allow to:\n- Identify outliers in the spatial distribution of individual variables\n- Identify and highlight spatial correlations across multiple datasets.  \n- Identify and highlight spatial patterns of inequality at the subnational level (e.g., across districts, municipalities, and communities)\n\n"
    },
    {
        "level": 3,
        "id": "ttc01lbK",
        "title": "Defining data disaggregation priorities",
        "text": "The definition of national data priorities include the definition of priority dimensions for which disaggregated data needs to be available:\n\n- Key population groups (e.g., women, urban/rural population, indigenous groups, youth, elderly population, people living with disabilities, ...)\n- Required level of granularity in geographic disaggregation\n\n"
    },
    {
        "level": 3,
        "id": "pXxz1MHR",
        "title": "Geo-referencing",
        "text": "Geo-referencing makes it relatively easy to bring together, overlay and analyze information from multiple sources based on different units of analysis. \n\n"
    },
    {
        "level": 2,
        "id": "cTjLfkWN",
        "title": "How to improve interoperability across data sources",
        "text": "To make various data sources ready-to-use, it is necessary to transform them to ensure they are interoperable. This includes:\n\n- Use of canonical data models\n- Geo-reference all data inputs using common boundaries and use consistent location-identification codes \n- Use common vocabularies, classifications and code lists\n- Develop of standardized API documentation\n\n\n"
    },
    {
        "level": 3,
        "id": "MUnZi5qo",
        "title": "Unique identifiers best practices",
        "text": "Identifiers are part of the basic data infrastructure of a country or organization. \nThey help structure and link data together.  They are *boundary objects*, i.e., information objects used in different ways by different communities.  Therefore, they enable cross-disciplinary work and collaboration. \n\nTo be useful, they need to be well documented. \n\nUnique identifiers should be:\n\n- unique\n- valid over the entire lifetime of entities\n- issued from a central authority\n- stored in all relevant systems / databases\n- **never** re-issued\n- assigned to all entities\n\n"
    },
    {
        "level": 1,
        "id": "kX1ixbFK",
        "title": "Cross-functional teams",
        "text": "The production of increasingly complex software and knowledge products, requires the collaboration of multiple hyper-specialized individuals. It is nearly impossible for everyone in a team to be equally skilled in all functional areas involved, or to find project managers who understand all the different specialties involved in the delivery of the organization's products and services. \n\nIn cross-functional teams:\n\n- Everyone strives to be *good enough* so as to be able to contribute *something* to every bit of the team's work\n- Each person focuses on the work that needs to be done, and not on their functional role\n- Everyone seeks to spread expertise across the team through cross-training (everyone invents some of their time time in training and learning from others)\n- Everyone is able to ask challenging questions about the product\n\nBy having cross-functional teams, the need for hand-offs and backups is minimized. Each team member has to train others in his or her own area of expertise, to ensure that the team can deliver value at a predictable, sustainable pace.   \n\n\n\n"
    },
    {
        "level": 2,
        "id": "duUZSb3j",
        "title": "Collaborative data innovation projects",
        "text": "Collaboration across multiple organizations towards producing common outputs and achieving a shared outcomes allows data innovation projects to draw a wide range of data assets, skills and resources. \n\nData innovation projects require the collaboration of multiple government agencies and partners, with multi-stakeholder, multi-disciplinary teams working together to:\n\n- address methodological and institutional challenges\n- process and analyze different data inputs\n- share best practices\n- promote experimentation and co-creation of information products\n\n\n\n"
    },
    {
        "level": 3,
        "id": "BLH4F9z3",
        "title": "Knowledge sharing in the public sector",
        "text": "- \"Knowledge sharing in the public sector tends to focus on connections across organizations that must coordinate to achieve a particular goal\"\n- But communities of practice can facilitate knowledge sharing across different public sector organizations that do similar things in different jurisdictions and \"simply need to know what others know\" even when they do not necessarily work towards a common goal or common service delivery.  \n- Such knowledge sharing can lead to improved performance. \n\n"
    },
    {
        "level": 3,
        "id": "e7WKiFlE",
        "title": "Collaboration and decentralization",
        "text": "Collaboration is about decentralized decision making - so people closest to the work make the decisions.\n\n"
    },
    {
        "level": 3,
        "id": "ey9luVUC",
        "title": "Purpose of \"collaborative\" groups in official statistics",
        "text": "Over the years, several \"collaborative\" groups and \"working groups\" composed by practitioners from national and international statistical organizations have been established with the objective of helping official statisticians address the challenges they face by:\n  - Learning what peers do in similar situations\n  - Seeking and sharing knowledge\n\n\"Belonging to an international community reduces individual risks for new developments through additional scrutiny and testing [and allows to] harness capacity from across the community to insulate individual statistical organizations from budgetary shock.\"  (By Th\u00e9r\u00e8se Lalor, [Introducing CSPA!](https://statswiki.unece.org/pages/viewpage.action?pageId=112132835))\n\n"
    },
    {
        "level": 3,
        "id": "H2cZYvUV",
        "title": "Command and control",
        "text": "A command-and-control organizational culture is based on a clear hierarchy  where people at the top make decisions and direct how work is to be executed by those under their supervision.  In this setting, everyone knows who they work for and who works for them, and predictability of projects with clear scope, budget and schedules is highly valued.  \n\n"
    },
    {
        "level": 3,
        "id": "mmwn1oEU",
        "title": "Establishing a national project team",
        "text": "Each national data innovation project requires dedicated personal, both from the NSO and from other participating agencies, to carry out the work at the country level.  The national data innovation project team should consist of:\n\n- A national project coordinator (bridge between technical team at NSO and other stakeholders)\n- Two statisticians (closely familiar with source data and statistical analysis prcesses)\n- One econometricians (with expertise in nocasting and/or small area estimation techniques)\n- One GIS expert (with expertise in GIS analysis and map visualization tools)\n- One IT focal point (with admin rights and familiar with data security protocols)\n- Two policy experts\n\n"
    },
    {
        "level": 2,
        "id": "wiYBU1Oc",
        "title": "Responsibilities of the project coordinator in a cross-functional team",
        "text": "- Create and track against a project plan\n- Drive team members towards the completion of milestones and project deliverables\n- Budget project resources and expenditures\n- Manage risks\n- Find ways to prevent time-sharing overload of team members between the project and their regular functional duties.\n\n"
    },
    {
        "level": 3,
        "id": "4hQReuCs",
        "title": "Role of coordinators of a community of practice",
        "text": "Coordinators of a community of practice must fulfill the following tasks:\n\n- Plan and announce group meetings\n- Gather relevant data through research and through pre- and post-meeting surveys of members\n- Analyze data\n- Facilitate discussions\n- Ensure timeliness and consistency of the group's activities\n- Manage time\n\n\n\n"
    },
    {
        "level": 1,
        "id": "JiEdFdtg",
        "title": "Engaging local universities and training institutes",
        "text": "To promote sustainability and national ownership, data innovation project teams need to partner with local universities or training institutes in order to train analysts at national statistical offices and other government agencies.\n\n"
    },
    {
        "level": 2,
        "id": "MvmET0Kx",
        "title": "Supporting local action and innovation",
        "text": "- Policies, budgets, institutions and regulatory frameworks by local governments \n- Local entrepreneurship\n- Community-based initiatives\n- Work of local research, academic and training institutions\n\n\n"
    },
    {
        "level": 2,
        "id": "ilCtHTAx",
        "title": "Training for data innovation projects",
        "text": "Data innovation teams need to build a wide range of **data analysis and management** skills, including:\n- data engineering\n- data science\n- IT infrastructure engineering\n- statistical and econometric modeling\n- GIS analysis\n\nIt also requires building **\"soft\" skills**, such as project management, fund-raising, and communication.\n\n\n\n"
    },
    {
        "level": 3,
        "id": "zGHMrgZh",
        "title": "Capacity building and sustainability",
        "text": "One of the main objectives of capacity building is to enable local staff to become technically **self-reliant** and to gain **confidence** and a strong sense of **ownership**.\n\nLocal staff should be able to understand and to explain to others not only the final outputs, but all aspects of the process, from beginning to end.\n\nCountry officers need to be able to replicate and update the outputs on a regular basis by their own means.\n\n"
    },
    {
        "level": 3,
        "id": "P43SdKCs",
        "title": "Training materials for national staff",
        "text": "To enable national staff to reproduce the outputs of data innovation projects and to utilize innovative data sources, technologies and methods on a sustainable basis, it is crucial to develop training materials and knowledge resources tailored to their own needs and context.\n\n- In their own language\n- Applicable in their existing technological infrastructure\n\n"
    },
    {
        "level": 3,
        "id": "53FNlsMn",
        "title": "Training in statistical and econometric modeling",
        "text": "Depending on the specific objectives of a data innovation project for the production of official statistics, there is need to provide specialized statistical and econometric modeling training.  For instance:\n\n- Population density estimation\n- Household consumption estimation\n- Crop-yield estimation\n\nTO DO: What are the most needed types of statistical and econometric modeling methods that project teams need to be able to apply?\n\n"
    },
    {
        "level": 2,
        "id": "P43SdKCs",
        "title": "Training materials for national staff",
        "text": "To enable national staff to reproduce the outputs of data innovation projects and to utilize innovative data sources, technologies and methods on a sustainable basis, it is crucial to develop training materials and knowledge resources tailored to their own needs and context.\n\n- In their own language\n- Applicable in their existing technological infrastructure\n\n"
    },
    {
        "level": 3,
        "id": "CNiPnq0T",
        "title": "Long-term relevance of data innovation projects",
        "text": "**Risk:**\nProject outputs and the data sources, methods and technologies used to generate them can become quickly outdated, due to rapid social, economic, environmental or technological changes.\n\n**Mitigation**:\n\n- Setup a continuous maintenance programme to keep IT infrastructure and systems up to date (e.g., software updates)\n- Update and release new results as new data inputs become available\n\n"
    },
    {
        "level": 2,
        "id": "2Daj1ZPE",
        "title": "Data science curriculum for official statisticians",
        "text": "- **Programming and coding**\n    - Python and/or R\n    - SQL\n- **GIS**\n    - Feature extraction from EO imagery\n    - Map visualizatoins\n- **Statistical / econometric estimation methods**\n    - Population-density estimation methods\n    - Small-area estimation methods\n    - Crop-yield estimation methods\n    - Household consumption estimation methods\n    - Poverty maps\n\n"
    },
    {
        "level": 1,
        "id": "WV9JV2CP",
        "title": "Data storage layer in a data science project",
        "text": "Every data innovation project needs a clear strategy for saving and accessing data.  One of the first steps in a data science project is to design and build the data storage layer, consisting of one or several data warehouses and data lakes that allow to capture, organize and to access structured and un-structured data from multiple sources.\n\nThe data storage layer needs to support the analytic tools and technologies used by the data science team. \n\nThe choice of a data storage solution will determine, for instance, whether data scientists will need to unload the data before working on it, or whether they can perform in-database analytics.\n\nA data storage strategy needs to be based on:\n\n- Types of data that will be used in the project\n- Financial, technical and human resources available for the project\n- Existing data management processes and data storage infrastructure\n- Project priorities (i.e., speed, referential integrity, data security, backups...)\n\n"
    },
    {
        "level": 2,
        "id": "yerBxsCk",
        "title": "Hybrid data warehouse / data lake architecture",
        "text": "Setting up both a data warehouse and a data lake allows to work with both structured/persistent and unstructured/ephemeral data in the analytics pipeline.\n\nThe general pattern is to store semi-structured data in a distributed database (the data lake) and run ETL processes to extract the most relevant data to an analytics database (the data warehouse).  But although most analysts interact with the data through the data warehouse, some may work in the data lake environment directly. \n\n"
    },
    {
        "level": 3,
        "id": "TvEdCbEX",
        "title": "Data Lakes",
        "text": "A data lake is designed to capture and store multiple sets of raw data at scale for a low cost.  It stores many different types of data in the same repository, and may also allow to perform transformations on the data. \n\nData lakes are designed to handle large volumes of streaming data and to make many different types of data readily available for analysis,  without having to move the data to a separate environment.   They tend to be more ephemeral (data is housed for the short-term) and generate outputs \n\nData lakes are setup without having to first define the data structure, following a **schema on read** principle. This means that  the structure of the data is only defined at the time it is used.   \n\nGiven the large variety of data that can be poured in them, **data lakes require a sound governance framework**, including clear data quality and metadata management protocols so they don't become \"data swamps\". \n\n"
    },
    {
        "level": 1,
        "id": "YnYMyw5F",
        "title": "Resource Description Framework (RDF)",
        "text": "RDF is a general-purpose language that uses triple subject-predicate-object statements to describe resources on the web, whereby each element of the triple statement can be individually represented by means of a unique resource identifier (URI).   With RDF, it is possible to represent simple statements about resources in the form of a graph. \n\n"
    },
    {
        "level": 2,
        "id": "Wjsr6pb0",
        "title": "Simple Knowledge Organization System (SKOS)",
        "text": "SKOS is an RDF vocabulary that includes only a small number of classes and properties necessary to supports the creation of thesauri, folksonomies and lightweight ontologies (including the expression of semantic relationships such as \"broader\", \"narrower\" or \"related\").   It can be used in combination with other RDF vocabularies to cover the requirements of specific domains.  \n\n"
    },
    {
        "level": 1,
        "id": "6bbdyyR1",
        "title": "Chaos engineering",
        "text": "Using randomness and deliberate stress on a system to improve it.\n\nThis approach acknowledges the fact that 'users do crazy, unpredictable things\", and that \"software must survive contact with the real world\".\n\n"
    },
    {
        "level": 2,
        "id": "VvkY3R0u",
        "title": "Limits of testing",
        "text": "\"once you know where to look, it's simple to make a test that finds \\[an error\\].\"\n\nIt's a \"fantasy to expect every single bug (will be) driven out \\[through testing\\]. Bugs will happen. They cannot be eliminated, so they must be survived instead.\"  (Nygard, 2018)\n\n"
    },
    {
        "level": 3,
        "id": "JoNWn0JQ",
        "title": "Resilience",
        "text": "The resilience of a system is the degree to which (1) it can continue operating and enabling users to get work done under transient shocks, persistent stresses or component failures, and (2) it can quickly recover from unanticipated, severe failures.\n\n\"Software must survive contact with the real world\"\n\nResilience is particularly important for mission-critical systems.\n\n"
    },
    {
        "level": 3,
        "id": "qKdPIZ98",
        "title": "Error handling",
        "text": "Adding error handling to the code increases complexity, but if done well, it strengthens resilience in production environments. \n\n"
    },
    {
        "level": 3,
        "id": "VURXQNM8",
        "title": "Test-driven development",
        "text": "Test-driven development (TTD) is a method of developing software in which tests are written before the actual code.  This helps developers understand how software should work before they start coding. \n\nTTD can be implemented using the `Red-Green-Refactor` technique:\n\n1. Write a failing test (**red**)\n2. Write just enough code to make the test past (**green**)\n3. **Refactor** the code to improve the design while ensuring that all the tests are still passing\n\n\n"
    },
    {
        "level": 3,
        "id": "aB7rvzQ0",
        "title": "Unbounded result sets",
        "text": "Situation in which a back-end database suddenly provides a huge number of records far beyond what is expected by the client.  Unless the client application explicitly limits the number of results that it is willing to process, this can exhaust its memory resources or left hanging in a while loop \"long after the user loses interest.\"\n\n\"Because datasets in development tend to be small, the application developers may never experience negative outcomes.  After a system is in production for a year, however, even a [routine query operation can yield] a huge return set.\"\n\n\"An unbounded result set occurs when the caller allows the other system to dictate the terms.\" ... \"The caller should [always] indicate how much of a response it's prepared to accept.\"\n\n\n"
    },
    {
        "level": 3,
        "id": "HCHhyZym",
        "title": "Testing code",
        "text": "Testing is an integral part of the development process.  If developers have any difficulties consuming their own code when creating tests, then most likely everyone else will also have difficulties using the code.  \n\n** Testing good practices**:\n\n- Avoid writing useless tests\n- Keep the test suite healthy by adding missing test cases and removing obsolete or useless tests\n- Think in terms of use case coverage, instead of how many lines of code are covered by the tests\n- Automate tests as much as possible (running manual tests becomes tedious, time-consuming and error-prone as applications grow).  \n\n**Types of tests**\n\n1. **Unit tests**: Tests that focus on individual/atomic methods.  They should not rely on any infrastructure (such as a database); should be fast and test a specific code path only.  \n\n2. **Integration tests**: Tests that focus on the interaction among components, e.g., what happens when a component queries a database, or when two components interact with each other.  Integration tests often involve some infrastructure. \n\n3. **Functional tests**: Tests that focus on testing the whole application, from a user's perspective. \n\n"
    },
    {
        "level": 3,
        "id": "QTFhTNC8",
        "title": "System longevity",
        "text": "The longevity of a system is its capacity to continue operating in production (processing user transactions) for a long time (e.g., \"during the full length of time between code deploys\").   \n\nThe main dangers to a system's longevity are (1) memory leaks and (2) data growth.  **Both are rarely caught during testing** (\"applications never run long enough in the development environment to reveal their longevity bugs\"). \n\n"
    },
    {
        "level": 2,
        "id": "QTFhTNC8",
        "title": "System longevity",
        "text": "The longevity of a system is its capacity to continue operating in production (processing user transactions) for a long time (e.g., \"during the full length of time between code deploys\").   \n\nThe main dangers to a system's longevity are (1) memory leaks and (2) data growth.  **Both are rarely caught during testing** (\"applications never run long enough in the development environment to reveal their longevity bugs\"). \n\n"
    },
    {
        "level": 3,
        "id": "jpsEMQHC",
        "title": "System transactions",
        "text": "A transaction is \"an abstract unit of work processed by a system\" (not the same as a database transaction.  Transactions are \"the reason the system exists\".  \n\n- A *dedicated system* processes only one type of transaction\n- A *mixed workload system* processes different types of transactions.\n\n"
    },
    {
        "level": 3,
        "id": "8R4IPjbe",
        "title": "Mitigating the probability of a failure chain",
        "text": "Every enterprise is \"a mesh of interconnected, interdependent systems\". Usually, the chain of events that leads to a catastrophic system failure are not independent from each other:  Sudden shocks and excessive strain may cause a component to fail.  And a failure in one part of the system may in turn cause strain to other parts of the system, increasing the probability of further component failures.\n\nEvery integration point is a potential point of failure propagation.  This is particularly a risk in the context of API-first application development. A **cascading failure** occurs when a crack in one layer triggers a crack in a calling layer. \n\n\"It is impossible to predict and eliminate all possible bad events\". The objective should be to contain the damage from individual failures in order to protect the rest of the system. \n\nBad events are inevitable, but they must be survived. Some ways to mitigate the risk of cascading failures is to introduce timeouts, handshaking and decoupling middleware in all integration points.  \n\n"
    },
    {
        "level": 1,
        "id": "7ICJcKmg",
        "title": "Examples of Small Area Estimation (SAE) for poverty indicators",
        "text": "Examples of utilization of SAE to generate disaggregated poverty statistics. \n\n- United States: \n  - Estimation of poverty and housing unit characteristics in small areas using a hierarchical logistic model that included housing units and personal information (Malec 2005). \n- Philippines: \n  - Synthetic estimation procedure applied to the Family Income and Expenditure Survey (FIES) and Census of Population and Housing (CPH) data to estimate poverty incidence at the provincial or city level (Albacea 1999). \n  - In 2003, this procedure was modified to generate provincial poverty estimates (Albacea and Pacificador 2003). \n  - Both studies led to the proposed procedure for estimating local poverty statistics in the Philippines (Albacea 2006). \n  - In 2007, estimation of incidence of food poverty in provincial agricultural and fisheries households, using direct, regression synthetic, and empirical best linear unbiased prediction on data from survey, census, and administrative sources (Cosico 2007). \n  - Municipal- and city-level poverty statistics estimated using the World Bank's   Elbers, Lanjouw and Lanjouw (ELL) methodology (Philippine Statistics Authority). \n  - Estimation of the number of poor households in municipalities and cities within a region (Bulan 2011; Yuson 2012; Alcanices 2013; Bernasol 2013; Caisido 2013; and De Guzman 2013). \n- Thailand:\n  - Use of ELL methodology to generate district and subdistrict poverty indicators using CPH and the Household Socio-Economic Survey.\n\n"
    },
    {
        "level": 2,
        "id": "EnB13Qse",
        "title": "Small area estimation for disaggregated SDG data",
        "text": "Disaggregated data is key for the effective formulation and monitoring of policies targeted at specific population groups. Thus, the global indicator framework of the SDGs recommends disaggregating data according to income, sex, age, race, ethnicity, migration status, disability, geographic location, and other relevant dimensions. \n\n"
    },
    {
        "level": 3,
        "id": "erjIYpKT",
        "title": "Making the case for Small Area Estimation",
        "text": "Before conducting a SAE project, it's important to establish whether there is a strong case justifying the investment of resources in it.  \"SAE is not a simple solution that can address all disaggregated data requirements of every user\" (ADB, 2020)\n\n- Is there a strong demand for disaggregated statistics?\n  - What are the users/stakeholders\u2019 strategic context, goals and desired results?\n  - What key policies, funding decisions, or questions require granular data? \n  - What specific granular data do users need? \n  - At what geographic or sub-population level of disaggregation?\n  - What is an acceptable level of precision? (Would deviations of 5%, 10%, 20% between the estimated and real values be acceptable)? \n\n- Is SAE really needed?\n  - Are there any reasons to suspect significant variation across small areas?\n  - Are there any proxy data available at a sufficient level of disaggregation?\n\n- Is SAE feasible?\n  - What physical, social or economic models and explanatory variables can be used to estimate the variable of interest?\n  - What additional administrative-level datasets that can be used as additional covariates for the small area model? \n  - How is the overall quality and accuracy of these datasets?\n\n"
    },
    {
        "level": 3,
        "id": "XVq7u8bY",
        "title": "Poverty map production: basic steps",
        "text": "Estimation of small-area poverty maps usually follows methodology developed by the World Bank over more than 20 years, combining census data with household survey data. This methodology usually includes the following general steps:\n\n1. Verify quality and comparability of Household survey data and census data\n2. Verify quality of additional geospatial co-variates\n3. Estimate a model of household consumption, based on data sample from household survey (using only variables that are available in both survey and census data) and geospatial co-variates.\n4. Apply estimated parameters to census data and geospatial co-variates in order to estimate consumption per capita for all households in the census\n5. Apply appropriate poverty lines to estimate poverty rates at various levels of aggregation\n6. Build confidence intervals using standard errors\n7. Use GIS to produce visualizations of the results\n\n\n"
    },
    {
        "level": 3,
        "id": "9AuA1kBK",
        "title": "When is Small Area Estimation (SAE) a good approach?",
        "text": "SAE is relevant when \n\n1. The indicator of interest is compiled from survey data with a sample size that is not large enough to reliably provide very granular estimates, **and**\n2. Auxiliary data sources are available which do not suffer from sampling\nerror and which is correlated with the main indicator of interest\n\nSAE may not be relevant when:\n\n1. The indicator of interest is not compiled from survey data **or**\n2. There is no other source of auxiliary data to enhance the granularity of the survey estimates.\n\n\n"
    },
    {
        "level": 2,
        "id": "XVq7u8bY",
        "title": "Poverty map production: basic steps",
        "text": "Estimation of small-area poverty maps usually follows methodology developed by the World Bank over more than 20 years, combining census data with household survey data. This methodology usually includes the following general steps:\n\n1. Verify quality and comparability of Household survey data and census data\n2. Verify quality of additional geospatial co-variates\n3. Estimate a model of household consumption, based on data sample from household survey (using only variables that are available in both survey and census data) and geospatial co-variates.\n4. Apply estimated parameters to census data and geospatial co-variates in order to estimate consumption per capita for all households in the census\n5. Apply appropriate poverty lines to estimate poverty rates at various levels of aggregation\n6. Build confidence intervals using standard errors\n7. Use GIS to produce visualizations of the results\n\n\n"
    },
    {
        "level": 3,
        "id": "5Y5WIm31",
        "title": "Poverty maps production: intermediate indicators",
        "text": "Poverty map production usually requires the computation of intermediate indicators to be used as covariates in poverty estimation methods:\n\n- Distance to nearby markets\n- Distance to nearby cities\n- Distance to roads\n- Distance to service delivery points (e.g., schools, health centers...)\n...\n\n"
    },
    {
        "level": 3,
        "id": "RHCUOmoR",
        "title": "Use of EO in poverty mapping",
        "text": "Poverty mapping projects typically use geospatial datasets derived from high-resolution EO imagery, including:\n\n- Land cover\n- Land use\n- Objects (cars, buildings, ...)\n- Night-time lights\n- Road networks\n- ...\n\n"
    },
    {
        "level": 3,
        "id": "aGJZz3gi",
        "title": "Survey-based poverty statistics",
        "text": "Official poverty statistics at the national, regional, and/or provincial level are usually based on data from Household Income and Expenditure Surveys (HIES) or Living Standards Measurement Surveys. \n\nThe sample sizes of these surveys in general do not allow for reliable poverty estimates disaggregated by gender, age, occupation, location, etc. \n\n"
    },
    {
        "level": 1,
        "id": "IjQtj0Ou",
        "title": "Tools for the creation of semantic annotations",
        "text": "There are various approaches for the creation of semantic annotations:\n\n- Manual annotation of existing documents, supported by user-friendly tools for concept and resource selection.\n- Automatic annotation of existing documents, using information extraction and natural language processing techniques\n- Automatic annotation of existing documents, applying machine-learning algorithms that have been trained on a small set of user-generated annotations.\n- Work-embedded annotation (annotation is integrated into the authoring process)\n\n"
    },
    {
        "level": 2,
        "id": "VqCXyzeM",
        "title": "Semantic annotation",
        "text": "Semantic annotation is the process of assigning terms from a standard vocabulary or ontology to describe elements of an information resources in a machine understandable way.\n\nThere are software tools that allow to automate some annotation-specific tasks, including:\n\n- **Decoration** - Adding simple commentaries\n- **Linking** - Creating link anchors\n- **Instance identification** - Making assertion of a resource as an instance of a particular ontological primitive\n- **Instance reference** - Making assertions about the subject of a resource\n- **Aboutness** - Establishing loose associations between ontological concepts and information resources\n- **Pertinence** - Linking to additional information which is not the subject of the resource itself.\n\n"
    },
    {
        "level": 1,
        "id": "dYOI5e5t",
        "title": "Library and Information Science",
        "text": "\"Library and Information Science and Computer Science communities... have been the main agents dealing with metadata for information discovery.\"\n\n"
    },
    {
        "level": 2,
        "id": "eyUbJCjM",
        "title": "Library science",
        "text": "\"Librarians have been in the business of describing things for more than 2,000 years, and have inevitably learned a thing or two.  The discipline of Library Science has given the rest of the world a lot of insight into how to describe things effectively.\"\n\n"
    },
    {
        "level": 3,
        "id": "tgKP9dAB",
        "title": "Metadata",
        "text": "A set of statements that describe some essential characteristics of an information resource, so it can be found, accessed and used in a specific context.  Formally, each one of such statement has three parts: \n\n- Subject: The entity being described\n- Object: Another entity used to describe the subject\n- Predicate: A type of relationship between the subject and the object\n\nIn practice, subject-predicate-object statements are expressed as \"attribute-value\" pairs in conformity with a pre-specified metadata schema.\n\nEach \"attribute-value\" pair is potentially an access point to discover the information resource it describes. \n\n"
    },
    {
        "level": 3,
        "id": "OvGiLcvj",
        "title": "Resource discovery",
        "text": "Process of identifying information resources that might be relevant for [a user's] information need. \n\nRelevance is highly subjective\n\n"
    },
    {
        "level": 3,
        "id": "1ETwadmJ",
        "title": "Catalogs",
        "text": "\"A roomful of books is not a library.  The catalog provides the library user with a simplified representation of the materials in the library collection,\" one \"which enables the user to move from the record to the actual object described by the record.\"\n\n"
    },
    {
        "level": 3,
        "id": "fj14TYXf",
        "title": "Importance of metadata",
        "text": "\"Why store data about an object, when you have the object itself? Because without data about the objects contained in a space, any sufficiently complex space is indistinguishable from chaos.\"\n\n"
    },
    {
        "level": 2,
        "id": "OvGiLcvj",
        "title": "Resource discovery",
        "text": "Process of identifying information resources that might be relevant for [a user's] information need. \n\nRelevance is highly subjective\n\n"
    },
    {
        "level": 1,
        "id": "R1P7bkXv",
        "title": "Types of metadata",
        "text": "Metadata can be clasified in various ways:\n\n1. **By way of creation**: \n  - Manually (by humans) or Automatically (by machines)\n  - By specialists or by non-experts (e.g. user-generated metadata standards)\n\n2. **By their time of creation**:\n  - At the same time or after the described resource is created \n\n3. **By type of storage**:\n  - Stand-alone or embedded in the described resource. \n\n4. **By level of structure**:\n  - Unstructured, semi-structured or structured\n\n5. **By domain**:\n  - General purpose (domain independent) or for specific purposes (domain-oriented)\n\n6. **By purpose**:\n  - Resource discovery or resource use\n\n"
    },
    {
        "level": 2,
        "id": "xhN8Oe7a",
        "title": "Types of metadata",
        "text": "- **Use metadata**: Information about how an object has been used (e.g., number of downloads, profile of users that downloaded the data...)\n- **Structural metadata**: Information about how an object is organized\n- **Administrative metadata**: Information about the origin and maintenance of an object\n- **Descriptive metadata**: Information about objective features an object, aimed to help users discover relevant resources\n\n"
    },
    {
        "level": 1,
        "id": "ixwl7E49",
        "title": "Shared resource effect",
        "text": "A shared resource is some facility that all members of a horizontally scalable layer need to use.  When it becomes overloaded, it limits the capacity of the whole layer.  \n\nCrucial services with many callers can spread their problems widely.\n\n"
    },
    {
        "level": 2,
        "id": "rWHWA54y",
        "title": "Tightly coupled systems are more prone to failure",
        "text": "\"A highly complex system with many degrees of coupling offers more pathways for faults to propagate.\" Morover, \"tight couplings accelerate the propagation of faults ... across system boundaries.\"\n\n"
    },
    {
        "level": 3,
        "id": "1uaeyP7Q",
        "title": "Risks of complexity",
        "text": "\"Adding complexity to solve one problem creates the risk of entirely new failure modes.\"\n\n"
    },
    {
        "level": 1,
        "id": "PLOihSYD",
        "title": "Metadata as a map",
        "text": "Alfred Korzybski wrote that \"the map is not the territory,\" in the sense that \"the word for a thing is not the thing itself.\"\n\nAnalogously, \"metadata is a map ... by which the complexity of an object is represented in simpler form.\"\n\n"
    },
    {
        "level": 2,
        "id": "mHRx01RX",
        "title": "Metadata is part of today's basic information infrastructure",
        "text": "\"Metadata, like the electrical grid and the highway system, fades into the background of everyday life, taken for granted as just part of what makes modern life run smoothly\"\n\n"
    },
    {
        "level": 3,
        "id": "WfAhbZ0X",
        "title": "Metadata as an asset",
        "text": "\"Valuing and cost-justifying metadata is a difficult task\"\n\nMetadata is an asset that needs to be properly managed.  Its value (and the cost of crating it and maintaining it), however, are difficult to quantify, especially since it involves the use of artifacts that are created and maintained as a shared public good in the context of the broader data infrastructure. \n\n"
    },
    {
        "level": 2,
        "id": "93gFVrhq",
        "title": "Metadata's referential nature",
        "text": "\"The main characteristic of metadata is its referential nature, i.e., metadata predicates about some other thing.\"\n\n"
    },
    {
        "level": 1,
        "id": "qF3b4dYY",
        "title": "Foreseeing excessive demand",
        "text": "As user traffic grows, it may eventually surpass a system's maximum workload capacity.   It is important to have in place a strategy establishing how the system will react to excessive demand.  \n\n"
    },
    {
        "level": 2,
        "id": "RQldgqdN",
        "title": "System stability",
        "text": "The stability of a system refers to its ability to continue operating without interruption. It is a prerequisite to any other concern. \n\n"
    },
    {
        "level": 3,
        "id": "O5SpnH7f",
        "title": "Validating responses from web services",
        "text": "\"Treat a response as data until you've confirmed it meets your expectations. It's just text in maps (also called dictionaries) and lists until you decide what to extract.\"\n\n"
    },
    {
        "level": 3,
        "id": "OIXlYnEe",
        "title": "Mission-critical systems",
        "text": "Mission-critical systems are those whose failure will result in an existential threat to the organization. \"People have to go home for the day if they stop working\". \n\n"
    },
    {
        "level": 1,
        "id": "yL2U1V1P",
        "title": "Web services",
        "text": "\"REST with JSON over HTTP is the lingua franca for services today\"\n\n"
    },
    {
        "level": 2,
        "id": "N24uOOOR",
        "title": "JSON",
        "text": "Javascript Object Notation is a data serialization format  commonly used for client-server communication over web service APIs.  It is a natural fit for client-side Javascript code and is easier to parse than XML.\n\n"
    },
    {
        "level": 3,
        "id": "FsYtLhqB",
        "title": "Serializing / Deserializing",
        "text": "Serializing is the process of transforming structured data into a storable format (e.g., files or transmissions across networks) that can be consumed by other applications.  Deserializing is the process of reconstructing data structures from data that is formatted for storage.  \n\n\n"
    },
    {
        "level": 3,
        "id": "AjIAvKmR",
        "title": "Headless Drupal",
        "text": "Decoupling back-end from front-end offers the ability for teams with specialized knowledge of modern front-end JavaScript frameworks to work in parallel with content management/creation teams, without compromising the integrity of the project. \n\nIn this approach, Drupal is used to deliver content as a service, to be consumed on demand by other applications via web APIs.  Drupal handles the creation and management of structured content (including content workflows and user management), while other client apps flexibly handle the presentation of content and the creation of user experiences targeted at the needs of specific user groups.  \n\nDrupal 8 offers a robust variety of features handling the encoding and serialization of API resources.  Drupal's API can accept HTTP requests and issue HTTP responses in JSON or XML to a client located in a decoupled application.\n\n- The default REST API available out-of-the-box in Drupal 8 Core is REST-Compliant.  \n- JSON API is a stable contributed module included in Drupal core that implements the RESTful API of the same name.\n- GraphQL\n\nThere are, however, some disadvantages associated with the use of Drupal as a decoupled CMS.  This include: \n\n- Introduction of additional points of failure by introducing an additional hosting stack for front-end applications\n- Need to handle client/front-end security\n- Ability to immediately view live results while adjusting content not always available\n- Ability to preview content in \"review status\" requires a separate staging environment\n- Layout management is a developer's concern, not of the editor. \n\n\n"
    },
    {
        "level": 2,
        "id": "sxbK7yRl",
        "title": "Web service",
        "text": "A web service enables a client to call a pre-formatted data message \"on demand\" from a source, using open standards such as HTML, XML, JSON, REST and SOAP. \n\nA web service API allows for machine-to-machine communication among web servers, or among web servers and clients.  They are \"the conduit through with data travel on its way from the server to the consumer\". \n\n"
    },
    {
        "level": 3,
        "id": "EBB8zWKu",
        "title": "Separation between content and presentation",
        "text": "By separating the structuring and creation of content from the mechanisms used to deliver it to users allows to establish multi-channel publishing workflows and user experience catered to different audiences. \n\n"
    },
    {
        "level": 3,
        "id": "K0HU3owB",
        "title": "How the web works",
        "text": "- A client sends an *HTTP request* to a server, and then the server responds to that client.\n\n- The most commonly used HTTP methods are GET and POST.  \n\n- An HTTP request is composed of a *header* and an optional *body*. \n  - The HTTP header consists of key-value pairs representing metadata that the client wants to send to the server (e.g., including cookies)  \n  - GET requests do not allow to include a body.  \n\n- When the server decides to respond to the request, it returns a *header* and an optional *body* as well. The first line of the response indicates the status of the request. \n\n- **HTTP is stateless**: After this cycle is completed, the server is no longer aware of the client, and if the client sends another request, the server is not aware that it responded to a request earlier from that same client. \n\n- There are mechanisms, such as cookies, that enable a server to become \"aware\" of its clients,  creating a sense of persistence between their requests. \n \n\n"
    },
    {
        "level": 3,
        "id": "spNOWLtX",
        "title": "Service-oriented architecture (SOA)",
        "text": "A service is a business activity that generates a specific outcome. It is self-contained and can be re-used by a number of business processes within and across different organizations. \n\nServices can be classified according to their level of granularity:\n\n- An atomic service encapsulates a fine-grained piece of functionality\n- An aggregate service contains larger, more complex or composite functionalities\n\nWhile aggregate services can be optimized to attain greater efficiency, fine-grained services allow greater flexibility for sharing and re-use. \n\n"
    },
    {
        "level": 3,
        "id": "Bjf1vADd",
        "title": "API-first",
        "text": "An approach in which web services are considered the most essential feature of a system, and in which the most important users are the consumers of those APIs.  \n\n"
    },
    {
        "level": 3,
        "id": "GfjvHf60",
        "title": "Network failures",
        "text": "One of the simplest failure modes occurs when a remote system refuses connections.  \n\nIf there are no timeouts, it can take a long time to discover that a client cannot connect. \n\n"
    },
    {
        "level": 2,
        "id": "K0HU3owB",
        "title": "How the web works",
        "text": "- A client sends an *HTTP request* to a server, and then the server responds to that client.\n\n- The most commonly used HTTP methods are GET and POST.  \n\n- An HTTP request is composed of a *header* and an optional *body*. \n  - The HTTP header consists of key-value pairs representing metadata that the client wants to send to the server (e.g., including cookies)  \n  - GET requests do not allow to include a body.  \n\n- When the server decides to respond to the request, it returns a *header* and an optional *body* as well. The first line of the response indicates the status of the request. \n\n- **HTTP is stateless**: After this cycle is completed, the server is no longer aware of the client, and if the client sends another request, the server is not aware that it responded to a request earlier from that same client. \n\n- There are mechanisms, such as cookies, that enable a server to become \"aware\" of its clients,  creating a sense of persistence between their requests. \n \n\n"
    },
    {
        "level": 2,
        "id": "AjIAvKmR",
        "title": "Headless Drupal",
        "text": "Decoupling back-end from front-end offers the ability for teams with specialized knowledge of modern front-end JavaScript frameworks to work in parallel with content management/creation teams, without compromising the integrity of the project. \n\nIn this approach, Drupal is used to deliver content as a service, to be consumed on demand by other applications via web APIs.  Drupal handles the creation and management of structured content (including content workflows and user management), while other client apps flexibly handle the presentation of content and the creation of user experiences targeted at the needs of specific user groups.  \n\nDrupal 8 offers a robust variety of features handling the encoding and serialization of API resources.  Drupal's API can accept HTTP requests and issue HTTP responses in JSON or XML to a client located in a decoupled application.\n\n- The default REST API available out-of-the-box in Drupal 8 Core is REST-Compliant.  \n- JSON API is a stable contributed module included in Drupal core that implements the RESTful API of the same name.\n- GraphQL\n\nThere are, however, some disadvantages associated with the use of Drupal as a decoupled CMS.  This include: \n\n- Introduction of additional points of failure by introducing an additional hosting stack for front-end applications\n- Need to handle client/front-end security\n- Ability to immediately view live results while adjusting content not always available\n- Ability to preview content in \"review status\" requires a separate staging environment\n- Layout management is a developer's concern, not of the editor. \n\n\n"
    },
    {
        "level": 3,
        "id": "ZN07PRiz",
        "title": "GraphQL",
        "text": "A query language specialized for consumer-driven queries (and therefore responses tailored for the consumer)\n\n"
    },
    {
        "level": 3,
        "id": "LRBfqLi8",
        "title": "Application Programming Interface (API)",
        "text": "An API consists of a set of methods and object classes that collectively describe the behavior of a system in response to specific actions by a user or client application. \n\nIt abstracts the actual implementation of that behavior, exposing only the classes or methods that are required by the client to perform a task. \n\n"
    },
    {
        "level": 3,
        "id": "nF5MUK35",
        "title": "Drupal",
        "text": "Drupal is a free and open-source CMS, written in PHP and distributed under the GNU General Public Licence.  It was initially launched in 2001. \n\n"
    },
    {
        "level": 3,
        "id": "PuNVAw2K",
        "title": "Decoupled content management",
        "text": "Delivering pure structured content (rather than fully rendered HTML pages) from a single source to a multitude of systems and front end applications through machine-to-machine interfaces.  \n\nThe content management system is used for its back-end database abstraction layer, decoupling it from other front-end layers. \n\n"
    },
    {
        "level": 1,
        "id": "VtZUwWX5",
        "title": "Anti-patterns",
        "text": "\"Common forces that have contributed to more than one system failure\"\n\n"
    },
    {
        "level": 2,
        "id": "JO0BPDLM",
        "title": "Anti-patterns",
        "text": "An anti-pattern is the opposite of a design pattern: \"a proven flawed technique that will most likely cause some trouble and cost time and money. (...) A priori, it seems like a good idea, but in the end it will most likely cause more harm than good.\"\n\nSome examples of anti-patterns include:\n\n- **God class**: A class that handles too many things, and breaks the application every time someone touches it. \n\n"
    },
    {
        "level": 3,
        "id": "VsoEyAHs",
        "title": "Design Patterns",
        "text": "A design pattern is proven technique that can be sued to solve a specific problem.  \n\nDesign patterns help craft solutions at scale.\n\n"
    },
    {
        "level": 3,
        "id": "GoNTrbVa",
        "title": "Writing high-quality code",
        "text": "Characteristics of high-quality code:\n\n- **Flexibility**\n- **Reusability**\n- **Maintainability**\n- **Readability**: Less code is simpler to visualize, leading to quicker undrestanding.  Use descriptive names for variables and methods. \n- **Simplicity**: Aim to write code that solves the issue at hand, instead of over-engineering it\n- **Testability**\n\n## Code smells:\n\nA code smell is an indicator of a possible problem. For example:\n\n- **Too many comments**: This is often an indication that the code could be split into smaller chunks with proper names, leading to better readability. \n\n- **Long methods**: More than 10 to 15 lines of code suggests a method (1) contains complex logic intertwined in multiple if statements, (2) does too many things, and/or (3) contains duplications of code.\n\n- **Names that are not self-explanatory**: When the name of an element does not immediately suggest what it is about, it is likely that the element could be split into smaller multiple pieces. \n\n"
    },
    {
        "level": 2,
        "id": "VsoEyAHs",
        "title": "Design Patterns",
        "text": "A design pattern is proven technique that can be sued to solve a specific problem.  \n\nDesign patterns help craft solutions at scale.\n\n"
    },
    {
        "level": 3,
        "id": "UdjxEMkP",
        "title": "Interface segregation principle",
        "text": "It is better to have multiple, smaller client-specific interfaces instead of  one large, general-purpose interface. \n\nCohesion - The elements of an interface should align towards a common goal.\n\nSmaller interfaces are easier to re-use, and help expose only the features that are needed for a specific purpose. They are also easier to compose into bigger pieces.   \n\n"
    },
    {
        "level": 3,
        "id": "DRSBoZlD",
        "title": "Code management - good practices",
        "text": "- Never hard-code credentials. Use a secrets file or config file that is .gitignored, or prompt the user.\n\n"
    },
    {
        "level": 1,
        "id": "W7R4OkLD",
        "title": "Technical debt",
        "text": "Technical debt consists of \"all the corners that are cut during the development of a feature or system.\" While it is unavoidable to incur in technical debt in every project, it is important to manage it and to keep it within limits;  otherwise, it will not be possible to pay it back. \n\nSome ways to manage and limit technical debt include: \n\n- Refactoring often\n- Learning to cooperate with all stakeholders involved in the project\n- Using proper governance and management techniques\n\n"
    },
    {
        "level": 2,
        "id": "MSGyGW7x",
        "title": "Refactoring",
        "text": "Refactoring is a process of improving existing code without changing its behavior.  It helps clean the code base of a project and get rid of some technical debt. \n\nRefactoring is particularly important in agile development: Instead of trying to finish the product at once, it is refined and improved over time.  \n\nRefactoring time must be factored into the planning of work (e.g., into product/sprint backlogs if using an Agile approach to project management).\n\n\n\n"
    },
    {
        "level": 3,
        "id": "hAem1VBd",
        "title": "Embracing change",
        "text": "Applications are born to change.  It is important to remain flexible\n\n"
    },
    {
        "level": 3,
        "id": "zgBmscjq",
        "title": "Backward-compatibility principle",
        "text": "Everything that worked in a particular way in the past should still work at least in that particular way after changes are made to a system. \n\n"
    },
    {
        "level": 3,
        "id": "c1CmqLad",
        "title": "Cooperation is essential to manage technical debt",
        "text": "Form time to time, it may be necessary to cut short the usage of best practices in order to meet short-term constraints, such as deadlines, budgets, etc.  To ensure that the resulting technical debt is properly managed, everyone involved must learn to cooperate and to understand each other's reality, and agree on how to allocate time and resources to re-factor existing solutions as soon as possible.  \n\n"
    },
    {
        "level": 3,
        "id": "VpDi7iBz",
        "title": "Extreme Programming (XP)",
        "text": "An agile approach focused on software development whose practices include:\n\n- continuous integration\n- software refactoring\n- test-driven development\n\n"
    },
    {
        "level": 2,
        "id": "c1CmqLad",
        "title": "Cooperation is essential to manage technical debt",
        "text": "Form time to time, it may be necessary to cut short the usage of best practices in order to meet short-term constraints, such as deadlines, budgets, etc.  To ensure that the resulting technical debt is properly managed, everyone involved must learn to cooperate and to understand each other's reality, and agree on how to allocate time and resources to re-factor existing solutions as soon as possible.  \n\n"
    },
    {
        "level": 1,
        "id": "R4yOjNYV",
        "title": "Knowledge assets",
        "text": "\"Statistical organizations deal with knowledge assets that aren't visible in the same way as physical or financial assets.\"\n\n"
    },
    {
        "level": 2,
        "id": "0fj1yp8k",
        "title": "Information, knowledge and wisdom",
        "text": "\"T.S. Eliot's Poem *The Rock* is a favorite among information scientists, for the following two lines:\n\n> Where is the wisdom we have lost in knowledge?\n> Where is the knowledge we have lost in information?\n\nhttp://www4.westminster.edu/staff/brennie/wisdoms/eliot1.htm\n\n"
    },
    {
        "level": 2,
        "id": "f7nY4e4m",
        "title": "Outsourcing knowledge work",
        "text": "To understand what sorts of knowledge work can be outsourced, ask:\n\n- Can the task be easily specified and measured?\n- Is delay between value creation and consumption possible?\n- Can the task be done remotely?\n\n"
    },
    {
        "level": 3,
        "id": "y1Zuq28z",
        "title": "Need for knowledge sharing across organizations",
        "text": "No organization alone has all the knowledge it needs to attain its objectives.  This creates an incentive for reaching across organizational boundaries in search of knowledge. \"The sharing of knowledge across organizations, particularly in the public sector, can lead to innovation, facilitate better problem definition, (...) reduce redundancies, build long-term collaborative capacity, and (...) increase accountability.\"\n\n"
    },
    {
        "level": 1,
        "id": "Tcm6TD6G",
        "title": "Quality and the integration and standardization of statistical production systems",
        "text": "\n**Requirement 11.6** of the \"National Quality Assurance Frameworks Manual for Official Statistics\" calls statistical agencies to define, promote and implement integrated and standardized production systems, including through  the promotion, sharing and implementation of standardized solutions that increase effectiveness and efficacy.  It also calls for them to adopt a statistical business architecture based on international standards and tools, such as GSBPM, GAMSO, CSPA, and SDMX. \n\n\n"
    },
    {
        "level": 2,
        "id": "1P7GAwhy",
        "title": "Sustainable access to source data",
        "text": "To be sustainable, producers of statistics need to have the capacity to regularly access to the necessary input data from external sources.\n\nThis requires:\n- Effective legal data exchange arrangements \n- Adequate incentives and business models\n- Appropriate technical data exchange standards and protocols\n- Good data exchange infrastructure, including connectivity and bandwidth.\n\n"
    },
    {
        "level": 3,
        "id": "QGyxiutK",
        "title": "Key principles of data innovation projects",
        "text": "1. Country ownership\n\nWithout country ownership, and funding, a data innovation project will not survive very long. Decision makers in government and senior NSO officials need to be convinced that the project will help drive the national strategy for the development of statistics and other national policy agendas. \n\n2. Sustainability\n\n3.  Long-term relevance\n\nAdequate planning of data and infrastructure is essential to ensure country ownership, sustainability and long-term relevance. \n\n"
    },
    {
        "level": 3,
        "id": "57Y4WA5S",
        "title": "Common Statistical Business Architecture Principles",
        "text": "- **Holistic approach**: Ensure data, methods, technology and skills are consistent, re-usable and interoperable across multiple functions and business units in the organization.\n\n- **End result focus**: Make the envisioned output the reference starting point for the design of new statistical processes.\n\n- **Enterprise-level value**: Design and implement new or improved statistical business processes aiming to maximize value at enterprise level. Leverage all enterprise capabilities so that the end result is well integrated, measurable and operationally effective. \n\n- **Customer value**: Design statistical services that deliver value to end customers. \n\n- **Sustainability**: Make investments and plans aiming to maximize the long-term continuity, adaptability and growth of core statistical functions.\n\n- **Re-use before designing new**: Re-use all existing capabilities and products before designing new ones.\n\n- **Design new for re-use**: Design new capabilities and products to be re-used, making sure they can be easily standardized, assembled and adapted to accommodate changing demands.\n\n- **Maximization of use of existing data assets**: Leverage all currently existing data before collecting additional data; monitor and reduce respondents' burden over time.\n\n- **Standards**: Adopt open, industry-recognized and international standards where available.\n\n- **Metadata-driven processes**: Design business processes to ensure that their composition, operation and management are metadata-driven and automated as much as possible.\n\n- **Discoverability and accessibility**: Ensure that statistical services are easily discoverable and accessible to enable sharing and re-use.\n\n- ** \"Just-right\" granularity**: Define statistical services at the level of granularity that is most relevant to the business needs.\n\n- **Trust and security**: Conduct all business in a manner that builds trust and confidence in the statistical organization's decision making and practices and its ability to preserve the integrity, quality, security, and confidentiality of the data assets entrusted to it.\n\n- **Service-level agreements**: Clearly define performance expectations for every statistical service, laying out metrics and remedies should agreed-on service levels not be achieved.\n\n- **National and international partnerships**: Collaborate nationally and internationally to leverage and influence statistical and technological developments which support the sharing of statistical services.\n\n\n\n"
    },
    {
        "level": 2,
        "id": "p9p1qFpa",
        "title": "Total Quality Management",
        "text": "Total Quality Management is the systematic and ongoing monitoring and identification of quality issues across an entire organization, as well as the planning and implementation of corrective actions, aimed to maintain and continuously improve over time the quality of the all the products and services delivered to its customers. \n\n"
    },
    {
        "level": 2,
        "id": "np2OKCzl",
        "title": "Quality of statistical information",
        "text": "Quality of statistical information refers to the degree in which statistical products and services satisfy the needs and requirements of their intended users.  \n\nThe measurement of quality of statistical outputs is made operational through the specification of a set of indicators along various dimensions, which are often referred to as a \"Data Quality Assessment Framework\".    \n\nThis dimensions and indicators of such a framework usually include:\n\n- **Relevance**: Extent to which the information content of the data delivered is pertinent to help users attain their own goals more effectively.\n\n  - *Indicators of relevance may include:*\n    - References in media\n    - Number of website hits\n    - Proportion of records pertaining to statistical units outside the target population\n    - Proportion of units in the target population that are not represented in the dataset\n\n- **Accuracy**: The closeness of a statistical data point or data set to the true value or set of values that it is intended to measure.  \n\n  - *Indicators of accuracy may include:*\n    - \n    - Proportion of correct predictions among the total number of cases examined\n    - Average size of (ex-post) prediction errors among the total number of cases examined\n\n- **Reliability**: The degree in which the same overall data generation process or method can be expected to yield the same results in repeated trials.  In situation where multiple revisions of an estimate are published over time, reliability can be thought of as the closeness of the initial estimate to the subsequent estimated values. \n\n  - *Indicators of reliability may include:*\n    - Sample size relative to population size\n    - Standard deviations \n    - Standard error estimated using a resampling method (e.g., bootstrap or jackknife estimation)\n    - Number and size of subsequent revisions\n\n- **Coherence and comparability:**  Degree to which two data points (or two datasets) can be assumed to be the result of the same overall data generation process, thus allowing to meaningfully combine them for analytic purposes.  In other words, for two data points (or data sets) to be deemed coherent and comparable, the assumptions made about the respective processes that generated them should not stand in contradiction to each other in any significant way.  For statistical data, coherence and comparability require in particular the use of common standards with regard to the concepts, definitions, classifications, and methods employed in their collection, processing, and dissemination.\n\n  - *Indicators of coherence and comparability may include:*\n    - Relative size of discrepancies in spatio-temporal trends exhibitied between two or more datasets that provide the same or similar information on the same subject matter\n    - Whether or not to datasets use the same concepts, definitions, and classifications (e.g., same definition of statistical units; common sampling frameworks; same definition of geographic areas and reference time periods; same temporal frequency and geographic granularity; same definition of age groups; etc.) \n\n- **Timeliness**: Extent to which the statistical information pertaining to the measurement of a past phenomenon or event is delivered in time for users to be take meaningful action or make consequential decisions.\n\n  - *Indicators of timeliness may include:*\n    - Gap between publication date and reference period\n\n- **Punctuality:** Degree to which statistical information is delivered according to an initially announced publication schedule.\n\n  - *Indicators of reliability may include:*\n    - Gap between actual and planned publication dates\n\n- **Completeness**: Degree to which the data set includes all the relevant information expected by the user.\n  - *Indicators of completeness may include:*\n    - Number of records that are missing key variables\n    - Total percentage of empty cells\n    - Proportion of statistical units in the target population that are not represented in the data set\n    - Proportion of statistical units in the sample design for which information is available / Response rates\n\n- **Accessibility:** Ease with which users can find and obtain the statistical data in a ready-to-use format and in line with open data standards.\n\n- **Interoperability**:  Ease with which a data set can be ingested and handled by multiple, independent, standard-compliant data processing and analysis systems. \n  - *Indicators of data security and confidentiality may include:*\n    - Use of standard classifications, concepts and definitions\n    - Use of open, machine-readable data formats and serializations\n\n\n- **Clarity:** Extent to which the statistics are presented to users in an understandable manner and in a form that facilitates its proper interpretation and use. \n  - *Indicators of data security and confidentiality may include:*\n    - Availability of complete and easy-to understand reference metadata (including glossary and methodological notes)\n    - Use of meaningful, human-readable variable names\n\n\n- ** Security and confidentiality:** Effectiveness with which the integrity of the data is preserved and the risks of disclosing confidential information about individuals, households and businesses are mitigated.\n\n- **Compliance with applicable legal requirements**:\n\n\nUser satisfaction surveys are one means to collect data on quality indicators.\n\n**Relationship between dimensions of quality of statistical information:**\n\n- **Accuracy vs reliability**:  When there is a systematic bias in the data production process, then increasing the sample size will generally increase reliability but will not improve accuracy.\n\n\n"
    },
    {
        "level": 3,
        "id": "CVQxSUja",
        "title": "Data quality and trust",
        "text": "Ensuring dat quality reinforces the credibility of the producers of statistics and the coordinating agency\n\n"
    },
    {
        "level": 3,
        "id": "OknCCndD",
        "title": "Response rates",
        "text": "Information on unit and item response rates help assess whether the survey results are representative of the target population.\n\nResponse rates need to be measured, reported, and analyzed\n\n"
    },
    {
        "level": 3,
        "id": "e7UgXxck",
        "title": "Data quality assessment roadmap",
        "text": "- Set a timetable for developing and implementing a data quality assessment strategy\n- Describe the data quality principles and policies to be used across the organization. \n- Secure commitment to data quality by senior management and key stakeholders\n- Establish governance framework for data quality, including roles and responsibilities around data quality management\n- Describe main statistical production processes within the organization (e.g., using the phases of the Generic Statistical Business Process Model, or GSBPM, as a reference)\n- Identify and describe the structure, definitions, attributes and inter-relationships of data inputs and outputs of each stage of the main statistical production processes (e.g., using the Generic Statistical Information Model, or GISM).\n- Develop a Quality Assessment Framework for the organization, formulating for the each input and output of every stage of the main statistical production processes:\n    - Relevant quality characteristics\n    - Appropriate quality indicators\n    - Methods and sources of data for compiling quality indicators \n- Provide training and develop guidelines for implementing data quality assessment framework across the organization\n- Compile and report quality indicators on a continuous basis\n- Analyze information from quality indicators\n- Identify necessary actions to improve quality of statistical processes and outputs (e.g., using the Generic Activity Model for Statistical Organizations, or GAMSO)\n\n"
    },
    {
        "level": 1,
        "id": "JJsmi4lC",
        "title": "Fighting misinformation and conspiracy theories",
        "text": "\"It's undeniable that we face ongoing battles against misinformation, on subjects from COVID-19 to climate change, from vaccines to votes.\" What can be done \"to convince people to rethink their positions on fake news and propaganda'?\n\nDiscussing and exposing the common mechanisms behind the spread of misinformation, such as (1) use of a false authority figure, (2) appeal to an individual's anger and prejudices, (3) claim of urgency.  Promote \"active open-minded thinking\". \n\n\"People who are educated to recognize how misinformation spreads are less likely to be duped by it, or to spread it themselves\". \n\n\n\n"
    },
    {
        "level": 2,
        "id": "2pva38Uf",
        "title": "Conspiracy theories",
        "text": "A conspiracy theory is an unsubtantiated belief in the existence of a secret plot between a specific group of actors to manipulate a situation or engage in wrongdoing. It is a belief framework that postulates the existence of an external agent or group who is responsible for some unacceptable state of affairs.\n\nThe conspiratorial mindset provides:\n\n- **Epistemic benefits**: A framework for understanding the world and bringing order to otherwise random events\n- **Existential benefit**: A distraction from facing one's own fears and assuming one's own responsibilities.\n- **Social benefit**: The possibility of joining a community of similarly disaffected thinkers who can validate one another's anxieties and shared worldview. \n \n\"Conspiracy theories are memetic--they mutate easily and take on new forms-- which makes them a perfect fit for [viral spread in] social media platforms.\"\n\nConspiratorial ideas are now habitually deployed / weaponized by political leaders in order to create political tension. \n\n\"Legitimization of conspiracies ... has fundamentally altered the way many of us receive and accept information, so that now many people, without any evidence, view scientific method and fact-based journalism as suspicious, and see once-trusted leaders as nefarious plotters.  The damage to the public trust has been sever and won't be easily healed.\"\n\n"
    },
    {
        "level": 3,
        "id": "ZveYwM4W",
        "title": "Group-think or tribal epistemology",
        "text": "Group think, or \"tribal epistemology\", is a way to describe situations where people evaluate new information based on whether others in their \"tribe\" or community endorse it, instead of through the lens of \"common standards of evidence\".   \n\nHere, the \"tribe\" is a community of similarly disaffected thinkers who can validate one another's anxieties and shared worldview.\n\nGroupthink is often the result of members of a group deciding to agree with the opinion of a few members of the group that are widely recognized as \"experts\".   \n\n\"'Truth,' then, is whatever the tribal rhetoric says it is.\"\n\n"
    },
    {
        "level": 1,
        "id": "QMGTrrZV",
        "title": "Benefits from joining a community of practice",
        "text": "Individuals and organizations who share valuable ideas through a community of practice can build their reputation and trustworthiness.   \n\nThe community of practice provides:\n1. Access to multiple channels of informal cross-group communication \n2. Ability to connect with peers and potential partners from other organizations and occupational communities on a broad range of issues. \n3. The opportunity to obtain relevant \"know-how\" from a variety of perspectives\n\nIndividuals who do the same type of work in different jurisdictions can support each other when facing similar challenges, while individuals who have different roles can gain understanding of different viewpoints and find opportunities for cooperation.\n\nIn other words, the community of practice reduces the cost of knowledge sharing. \n\nUltimately, an effective community of practice leads participants to \"generate improvements in their own organizations, which can then be brought back to the community of practice, creating a cycle of learning and knowledge exchange for participants.\"\n\n\n"
    },
    {
        "level": 2,
        "id": "wX8xRCMT",
        "title": "Meetings of a community of practice",
        "text": "The primary activity of a community of practice usually consists of holding face-to-face or virtual meetings. \n\nPrior to each meeting, coordinators collect data from participants on the main topic for discussion (e.g., via an electronic survey), and analyze it along other publicly available data.\n\nEach meeting starts with a presentation of the preliminary data analysis by the coordinators, followed by a list of suggested questions for discussion. These questions specifically encourage participants to make informal comparisons among each other, with the aim to generate new insights and learning (e.g., \"who uses X approach?\", \"how many staff/resources are available to do Y?\", \"who thinks X is a good practice?\"...)\n\nMeetings allow participants to:\n\n- Discuss differences in their own approaches to address specific challenges\n- Share anecdotes\n- Pose additional questions and pass information to the group\n- Get new ideas and brainstorm\n- Identify and \"bubble up\" good practices\n- Construct an unofficial list of who knows what\n\nIdeally, the meeting is only the initial contact point that generates off-line follow-up discussions.  Ideally, participants \"seek each other out prior to, during, and after the meeting to connect on issues thy have in common or find out more details about what they are doing in response to a particular challenge.\"\n\n"
    },
    {
        "level": 3,
        "id": "z9Y6pN8M",
        "title": "Communities of practice",
        "text": "Social relationships play an important role in people's ability to share and to use knowledge. \n\nCommunities of practice are:\n- \"Groups of people informally bound together by shared expertise and a passion for joint enterprise\" (Wenger and snyder, 2000, p. 139)\n- \"Building blocks of social learning systems\" (Wenger, 2000, p. 229)\n\nThey provide a low-risk way to access and share both explicit know-how (e.g., training in specific skills) and tacit know-how (e.g., values, norms, and behavioral expectations), as well as \"a safe space in which members can openly admit lack of knowledge\", \"empathize with each other's problems\" and \"show pride in their work.\"\n\nCommunities of practice usually cut across organizational boundaries, as people often identify with occupational or professional groups outside their own organization. For example, statisticians, data engineers and data scientists often describe their professional identity in terms of what they do, as opposed to where they work. Moreover, when people interact with others outside their organization, new knowledge emerges and becomes enriched as established facts, theories and hypothesis are examined, applied and tested in different contexts. \n\n"
    },
    {
        "level": 3,
        "id": "2lr4gGNA",
        "title": "Meeting management",
        "text": "Meetings can go off tracks due to unanticipated displays of passion from a team member.\n\n\n"
    },
    {
        "level": 2,
        "id": "z9Y6pN8M",
        "title": "Communities of practice",
        "text": "Social relationships play an important role in people's ability to share and to use knowledge. \n\nCommunities of practice are:\n- \"Groups of people informally bound together by shared expertise and a passion for joint enterprise\" (Wenger and snyder, 2000, p. 139)\n- \"Building blocks of social learning systems\" (Wenger, 2000, p. 229)\n\nThey provide a low-risk way to access and share both explicit know-how (e.g., training in specific skills) and tacit know-how (e.g., values, norms, and behavioral expectations), as well as \"a safe space in which members can openly admit lack of knowledge\", \"empathize with each other's problems\" and \"show pride in their work.\"\n\nCommunities of practice usually cut across organizational boundaries, as people often identify with occupational or professional groups outside their own organization. For example, statisticians, data engineers and data scientists often describe their professional identity in terms of what they do, as opposed to where they work. Moreover, when people interact with others outside their organization, new knowledge emerges and becomes enriched as established facts, theories and hypothesis are examined, applied and tested in different contexts. \n\n"
    },
    {
        "level": 3,
        "id": "Tlb9tDLU",
        "title": "Sharing of experiences, good practices and lessons learned",
        "text": "Success in the use of new technologies and sources of data will depend on a wide variety of contextual and operational factors, as well as on the evolving severity and nature of the impact of the COVID-19 epidemic in each country. \n\nManagers of national and global statistical programmes affected by the crisis require a platform for rapidly sharing experiences and lessons learned as they navigate a new environment characterized by many uncertainties and risks. \n\nThis sharing should focus on the essential aspects of planning, management and implementation of re-organization and adaptation strategies. \n\n"
    },
    {
        "level": 3,
        "id": "4jDVh28g",
        "title": "Characteristics of effective communities of practice",
        "text": "What structures, behaviors, and processes facilitate knowledge sharing in a community of practice? What factors influence an organization's decision to join and actively participate?\n\nTo be effective, a community of practice needs to generate value to participants seeking information on how to address specific challenges they face in their own organizations. \n\nSuccessful communities of practice usually:\n \n- **Are coordinated by a respected outside party**: Rather than being ad hoc, meetings are planned and scheduled in advance, and participants know ahead of time the topical focus for discussion.\n- **Have a small, but sufficient funding stream**: Funding allows for minimal expenses, such as hiring a part time administrator and a part-time analyst, and the production of materials and handouts necessary for the meetings. \n- **Have an agenda driven by their members**: The topics and the format of the meetings are driven by a steering committee and the interests of participating communities\n- **Allow different levels of engagement**: There are no prescriptive rules specifying what will be shared by whom, and participants can decide to fully engage or to passively observer discussions, have one-on-one informal conversations, and see what other share.\n\nIt is important to understand the social networks established through a community of practice:\n\n- Are some actors more central to others?\n- How does social-network centrality influence behavior of participants?\n\n"
    },
    {
        "level": 1,
        "id": "Oyhi83hd",
        "title": "Escalation of commitment",
        "text": "Area of research that has directly examined the ending of organizational programs and how organizations become committed to losing courses of action over time.\n\nDeterminants of escalation:\n- Project determinants (closing costs, salvage value, causes of setbacks) \n- Psychological determinants (difficulties withdrawing from previously rewarded behavior; need for self-justification; cognitive biases \n- Social determinants (hostile audiences; imitation; cultural norms favoring strong leadership). \n- Organizational determinants: project's institutionalization within the organization, economic and technical side-bets (hiring of staff, development of expertise).\n\nProject determinants are more important in the initial stages of an escalation episode; psychological and social determinants are more important in the middle stages; organizational and (again) project determinants become more important at the end. \n\n\n"
    },
    {
        "level": 2,
        "id": "W7T2p8gF",
        "title": "Embracing failure when it happens",
        "text": "People do not take risks for fear of failure. But taking risks is intrinsic to growth and innovation.  We should therefore become familiar with, and embrace, the feelings associated with taking risks, failing, admitting mistakes, and recovering from them, so fear of those feelings is not any more a source of paralysis. \n\n\"The faster you fail, the faster you can recover and learn, leading to successful products. (...) Do your best, learn from your mistakes, and be humble.\"\n\n"
    },
    {
        "level": 1,
        "id": "wJHcEcFj",
        "title": "Multigraph",
        "text": "Unidirected graphs that can have multiple edges between the same nodes. Parallel edges can represent different types of relationships between the nodes.\n\n"
    },
    {
        "level": 2,
        "id": "t4mv3eQD",
        "title": "Network analysis: Undirected graphs",
        "text": "Edges can be traversed in either direction (an edge from A to B is the same as an edge from B to A).\n\nCan have self-loops (reflexive relationship)\n\n\n"
    },
    {
        "level": 3,
        "id": "kvpUkOPt",
        "title": "Graph",
        "text": "In graph theory, a graph is a set of discrete objects (nodes) possibly joined to each other in binary or reflexive relationships (edges).   It is a relational form of organizing and representing discrete data, where nodes and edges can be decorated with additional properties known as \"attributes\".\n\nA binary relationship represents a relationship between two nodes.  A reflexive relationship represents a relationship of a node with itself.\n\nEdges can be thought of as representing pairs of connected nodes (dyads). \n\nGraphs can be used to describe networks (systems of interconnected objects).  \n\n"
    },
    {
        "level": 3,
        "id": "MV7Xi5dH",
        "title": "Network analysis",
        "text": "Exploring quantitative relationships in networks with non-trivial structure.\n\nThe study of complex networks did not start until the late 1800s/ early 1900s due to lack of proper mathematical apparatus (graph theory) and adequate computational tools.\n\n"
    },
    {
        "level": 3,
        "id": "2fx14QVS",
        "title": "Representing ETL jobs as Directed Acyclic Graphs (DAG)",
        "text": "It is often useful to visualize complex ETL data flows using a directed acyclic graph, where each node corresponds to a task (which only needs to be performed once), and arrows represent dependencies between tasks. \n\n"
    },
    {
        "level": 2,
        "id": "kvpUkOPt",
        "title": "Graph",
        "text": "In graph theory, a graph is a set of discrete objects (nodes) possibly joined to each other in binary or reflexive relationships (edges).   It is a relational form of organizing and representing discrete data, where nodes and edges can be decorated with additional properties known as \"attributes\".\n\nA binary relationship represents a relationship between two nodes.  A reflexive relationship represents a relationship of a node with itself.\n\nEdges can be thought of as representing pairs of connected nodes (dyads). \n\nGraphs can be used to describe networks (systems of interconnected objects).  \n\n"
    },
    {
        "level": 1,
        "id": "BaBoinzs",
        "title": "Network analysis: Modularity",
        "text": "Modularity measures the strength of division of a network into clusters (modules). A network that is partitioned into non-overlapping modules is said to have high modularity if the connections between the nodes within modules are dense, while  the  connections between nodes in across modules are sparse.\n\nIt is defined as the fraction of the edges of a network that fall within the given modules, minus the expected fraction if edges were distributed at random.\n\n"
    },
    {
        "level": 2,
        "id": "p6ocJHTv",
        "title": "Network analysis: Degree",
        "text": "The degree of a node in a network is the number of immediate neighbors (adjacent nodes).  It is a non-negative integer number. \n\n"
    },
    {
        "level": 1,
        "id": "h1xZIuYx",
        "title": "Investing in people",
        "text": "\"To effectively reenergize their workforces, organizations need to shift their emphasis from getting more out of people to investing more in them, so they are motivated--and able--to bring more of themselves to work every day.\n\nEmployees reward organizations that treat them humanely, fairly and with dignity.  In contrast, loyalty to the organization erodes not only among employees that are treated inhumanely, unfairly or without respect, but also among their colleagues who witness such behavior.  This can lead to serious talent retention problems. \n\n"
    },
    {
        "level": 2,
        "id": "fOMXYZho",
        "title": "Conflict management",
        "text": "- Depersonalize conflict, focusing on the disagreement issue rather than on the interested parties. \n- Look beyond the merits of an issue and understand the interests, feasrs, aspirations, and loyalties of the factions around it.\n\n"
    },
    {
        "level": 3,
        "id": "k89uWQ1l",
        "title": "Self-management: Change the narrative",
        "text": "- **Reverse lens**: \"What would the other person in this conflict say and in what ways might that be true?\"\n- **Long lens**: \"How will I most likely view this situation in six months?\"\n- **Wide lens**: \"Regardless of the outcome, how can I learn and grow from it?\"\n\n"
    },
    {
        "level": 1,
        "id": "HhsYzH6I",
        "title": "Project management: establishing a rhythm",
        "text": "\"Establishing a rhythm is how teams [members] can maintain awareness of each other's workloads, upcoming decisions, and decision outcomes\"\n\nSchedule specific types of meetings at specific intervals:\n\n- **Weekly action meeting** \n  - *Objective:* Unblock the work and set the team up to get their work done for the next seven days.\n  - *Agenda:*\n    1. Check in - Everyone answers: \"What has your attention right now?\"\n    2. Checklist review - Everyone answers \"Yes/No\" to each checklist item\n    3. Metrics review - The team reviews the metrics to see whether we are on the right path\n   4. Project updates - \"What has changed in the project since last week?\"\n   5. Build agenda - Everyone adds topics to the agenda by calling out a placeholder word or phrase.  The team focuses on topics that will unblock the work in the coming week\n   6. Process agenda - Work through the agenda focusing on unblocking work for everyone\n   7. Checkout - Everyone answers: \"What did you notice?\"\n- **Monthly retrospect meeting** \n  - *Objective:* To look back on the past month and discuss necessary changes in the way the team works together.\n\n"
    },
    {
        "level": 2,
        "id": "9GzzV4Gn",
        "title": "Clarity Paradox: Success as a catalyst for failure",
        "text": "Clarity of purpose >> Success >> More options and opportunities >> Diffused efforts >> Less clarity of purpose\n\nEarly success often leads to \"the undisciplined pursuit of more.\" Thus, \"one simple antidote is the disciplined pursuit of less... purposefully, deliberately, and strategically eliminating the nonessentials... constantly reducing, focusing and simplifying.\"\n\nTo avoid the clarity paradox, we need to keep a constant check on the Sisyphean task of clarifying purpose.  \n\nAt the personal level, it means continuously finding the intersection between the set of activities we are deeply passionate about, those in which we are genuinely talented, and those that meet a significant need in the world. \n\nWe need to continuously 'figure out which ideas from the past are important and pursue those\" while throwing out the rest.  \n\nThis means also a keeping a constant check on WIP (work in progress): \"eliminating an old activity before (adding) a new one\" so \"you don't add an activity that is less valuable than something you are already doing\".\n\n"
    },
    {
        "level": 3,
        "id": "QyoZwcP9",
        "title": "Managing own's time and effort",
        "text": "\"It is no fun to find ourselves with too little time and too many deliverable, rushing headlong into panic mode while the hours ebb dep into the night.\" (Goldfedder 2020)\n\nTrying to manage time is a fools errand when it is objectively impossible to fit everything into the limited amount of time that is available in a day, no matter how efficient one is.  \n\nTime is a limited resource and should be allocated in alignment of one's own personal definition of success.\n\nRecognize and excel in what really counts, and aim for less than perfect in everything else.  \n\n- Decline invites to tactical meetings \n- Reduce your involvement in committees\n- Focus on strengths instead of trying to shore up your weaknesses\n\n"
    },
    {
        "level": 3,
        "id": "qvRxZspp",
        "title": "When there is too much to do",
        "text": "\"when we have too much to do we can freeze. Spinning without traction, we move fast but don't make progress on the things that are creating our stress.  Because when there is so much competing for attention, we don't know where to begin and so we don't begin anywhere\".\n\n"
    },
    {
        "level": 2,
        "id": "gteEMrtG",
        "title": "Transitioning to Agile project management",
        "text": "Agile is radically different from how most organizations operate.  in order to transition to an agile way of doing things, there needs to be a cultural change and a re-alignment of roles and responsibilities.  \n\n**Identify key supporters**\n\nGet the right people on board, including individual contributors form different functional areas, their managers, and a senior executive who can sponsor an agile pilot project.  These should become a team of supporters who see the value in the agile transition and are willing to try it. \n\nDiscuss with them the benefits of embracing an agile mindset. and how the agile principles and values can enhance delivery and predictability in the organization.  \n\n**Implement agile training and couching**\n\nEnsure there are training and couching opportunities for everyone in the organization to learn about the values, principles, and practices of the agile mindset.   \n\n**Focus on organizational culture**\n\nHelp people become comfortable with sharing and collaborating, explaining that when people learn more about what others do, and have an opportunity to collaborate with others outside their fields of expertise, everyone grows their own skills.  \n\n**Select and pilot project**\n\nIn order to transition from traditional to Agile project management, it's a good idea to select a medium-sized pilot project.  This will allow to test the agile fundamentals from start to finish and gather feedback on areas where the organization will need to change current practices. \n\nThe project should run for 4 to 12 weeks, involve all roles and cover all phases of development.  Moreover, it should not be a high-risk project, \n\n**Bottom-up vs. Top-down transformation**\n\nBottom-up transformation is usually faster, but may lead to issues id there is lack of senior management support. Top-down transformation may be challenging when teams do not know much about the agile mindset.\n\n**Building the business case and secure funding **\n\nOne of the main challenging aspects of an agile transformation effort is building the business case and securing funding for a project with flexible scope, which often goes against traditional way of doing things. \n\n"
    },
    {
        "level": 2,
        "id": "QyoZwcP9",
        "title": "Managing own's time and effort",
        "text": "\"It is no fun to find ourselves with too little time and too many deliverable, rushing headlong into panic mode while the hours ebb dep into the night.\" (Goldfedder 2020)\n\nTrying to manage time is a fools errand when it is objectively impossible to fit everything into the limited amount of time that is available in a day, no matter how efficient one is.  \n\nTime is a limited resource and should be allocated in alignment of one's own personal definition of success.\n\nRecognize and excel in what really counts, and aim for less than perfect in everything else.  \n\n- Decline invites to tactical meetings \n- Reduce your involvement in committees\n- Focus on strengths instead of trying to shore up your weaknesses\n\n"
    },
    {
        "level": 3,
        "id": "ChwOmom4",
        "title": "Personal time management in agile",
        "text": "Each person has their own time box (no more than eight hours work per day). Overtime work disrupts teams predictability and is not sustainable.  A team member cannot commit to a delivery date if there is too much work in process, if meetings are too many or too long, or if everyone is working overtime. \n\n"
    },
    {
        "level": 3,
        "id": "2uCsbhU6",
        "title": "Results of bad planning",
        "text": "\"Typical results of bad planning are missed deadlines, unrecognized overhead, and, more often than not, ... desperate acts of last-minute heroism that take on a weirdly optimistic tone (\"I did it and with only three ours of sleep in the last two days!\") \"\n\n\n\n"
    },
    {
        "level": 1,
        "id": "MkZau9Uc",
        "title": "Waterfall project management",
        "text": "The waterfall model assumes the project can be divided in a linear sequence of phases (requirements analysis, design, implementation, testing, deployment, maintenance).  \n\nIt works well in situations where the requirements and task to be performed in order to meet those requirements are clearly defined in advance. \n\n"
    },
    {
        "level": 2,
        "id": "MVQ6AEt4",
        "title": "Project management: Agile vs Waterfall",
        "text": "In practice, a mix of waterfall and agile methodologies is needed. \n\n"
    },
    {
        "level": 3,
        "id": "TBRkKxJ7",
        "title": "Paralysis by analysis",
        "text": "\"When the fear of either making an error, or foregoing a superior solution, outweighs the realistic expectation or potential value of success in a decision made in a timely manner\"\n\n\"When overanalyzing or overthinking a situation prevents progress or decision-making\"\n\n\"The more numerous our options, the more difficult it becomes to choose a single one... and so we end up choosing none at all.\"(P. Bergman, 2010)\n\nParalysis by analysis is common when:\n- Dealing with complex systems and high uncertainty.\n- There is an overload of information from numerous sources \n- There is too little knowledge leads to lack of confidence on the part of team members\n- Vast knowledge and expertise increases the number of options and considerations that appear at every decision point. \n- Excessive focus is placed on perfection and completeness of the analysis phase.\n- The goals and timeframe of the analysis phase are not clearly defined and the expectations surrounding the deliverables are fuzzy.\n- Design and implementation issues are introduced into the analysis phase.\n\nWikipedia: \"In software development, analysis paralysis typically manifests itself through the Waterfall model with exceedingly long phases of project planning, requirements gathering, program design and data modeling, which can create little or no extra value by those steps and risk many revisions.\"\n\n\"Agile software development methodologies explicitly seek to prevent analysis paralysis, by promoting an iterative work cycle that emphasizes working products over product specifications,\"\n\n\n"
    },
    {
        "level": 1,
        "id": "liKE24lR",
        "title": "Regularity/Frequency of data migration tasks",
        "text": "- **One-time migration:**\n  An existing data source is moved once into a target destination, often after performing several transformations and tests through code or scripting.\n- **Nightly integration**:\n  Data from external system is loaded on a regular basis (e.g., every night)  into the target system.  This approach is common in data warehouses. \n- **Near real time integration:**\n    Listener services allow for integration to be triggered the moment data is input into the source system \n- **Hypbrid approach**\n\nNightly and real-time integration require the automation of the ETL task, which assumes that the content and structure of the source system is predictable and that exceptions can be adequately handled. \n\n"
    },
    {
        "level": 2,
        "id": "1cHc8MiO",
        "title": "Reproducibility of processes",
        "text": "The ability to reproduce processes is crucial to generate trust in their outputs.\n\nReproducibility requires immutable data and versioned logic (adoption of functional programming).\n\nReproducibility allows to schedule repeatable processes to auto-execute on specific intervals\n\n"
    },
    {
        "level": 3,
        "id": "v0j3xMCw",
        "title": "Functional programming",
        "text": "A programming paradigm \"that treats computation as the evaluation of mathematical functions\", where the output of each computation depends only on the arguments passed to it (thus isolating logic changes from data/state changes)\n\nThe goal is to break the application logic into smaller functions, where each function is focused on a single task. Functions are then **'composed'** into larger functions.\n\nAdvantages of functional programming:\n\n- Improve modularity: Individual functions can be written and tested in isolation \\(without having to understand external context\\)\n- Enhance reproducibility of results\n- Making code easier to understand\n- Making outputs easier to predict\n\nFunctional programming is part of declarative programming.\n\nSee also (https://en.wikipedia.org/wiki/Functional_programming)\n\n"
    },
    {
        "level": 3,
        "id": "vLPsC8l0",
        "title": "Pure tasks",
        "text": "- **Pure tasks** produce the same result every time they are run.\n- **Overwrite approach**: \"Re-executing a pure task with the same input parameters should overwrite any previous output that could have been left out from a previous run of the same task.\"\n- Tasks can become \"purified\" by breaking them down into smaller tasks, each of which targets a single output. \n\n"
    },
    {
        "level": 1,
        "id": "V9r8N6je",
        "title": "Function expressions in JavaScript",
        "text": "**Function expressions** create functions as a variable.  They are not hoisted -- cannot be invoked before being created. See (https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function#Function_declaration_hoisting)\n\nIn **JavaScript**, functions are \"first-class\" members. It is possible to declare functions with the `var`, `const`, or `let` keywords.  Functions can do the same things that variables can do.  Functions can be added to objects and arrays, sent to functions as arguments, and can be returned from functions.  \n\nNote:\n\n- With the `let` keyword, we can scope a variable to any code block, protecting the value of the global variable\n- The `const` keyword declares a variable that cannot be overwritten. Once declared, its value cannot be changed.\n\n\n"
    },
    {
        "level": 2,
        "id": "WtKaAhds",
        "title": "JavaScript for React",
        "text": "- Arrays\n- Objects\n- Functions\n- Functional JavaScript\n\n"
    },
    {
        "level": 3,
        "id": "h6EOMFkP",
        "title": "Node.js and JavaScript projects",
        "text": "`Node.js` is an asynchronous, event-driven JavaScript runtime environment  designed to build scalable network applications.\n\nNode.js allows to use JavaScript in web-application development, for both server- and client-side scripts.\n\nTo make sure Node.js is intalled:\n`node -v`\n\n## npm\n\n`npm` is a package manager for the Node.js environment that makes it easier  to publish and share source code of Node.js packages, and is designed to simplify installation, updating, and uninstalling of packages.\n\n`package.json` is a file describing a JavaScript project and its dependencies.  When running `npm install` in the folder that contains the `package.json` file, npm will install all the packages listed in the project.  \n\n- `npm init -y` will initialize a new project and create the `package.json` file.\n- `npm install <package name>` will install a package\n- `npm remove <package name>` will remove a package\n\n### yarn\n\n`yarn` is an alternative to `npm`.  \n\n- `npm install -g yarn` will install yarn globally with npm:\n- `yarn` will install dependencies from `package.json`\n- `yarn add <package-name>`\n- `yarn remove <package-name>`\n\n\n\n"
    },
    {
        "level": 3,
        "id": "V8p4H53X",
        "title": "Deconstructing JavaScript Objects",
        "text": "Technique that consists of pulling fields within an object to create local variables for them.\n\n"
    },
    {
        "level": 3,
        "id": "b8UigmQs",
        "title": "React learning roadmap",
        "text": "1. JavaScript for React\n2. Building a user interface with components\n3. Adding logic with props and state\n4. Use of React Hooks - to reuse stateful logic between components\n\n"
    },
    {
        "level": 3,
        "id": "GRNZb1vP",
        "title": "React",
        "text": "React is a JavaScript library designed to build component-based, high-performing, single-page applications (front ends). It allows to write JavaScript code that looks like HTML and can update the browser DOM, using patterns that are readable, reusable and testable.  \n\nReact needs pre-processing to run in a browser (it needs a build tool like Webpack).\n\nThe major advantage of React is its ability to separate data from UI elements.\n\n"
    },
    {
        "level": 1,
        "id": "PLe99ZmC",
        "title": "React hooks",
        "text": "React hooks are a new way of add and share stateful logic across components. When data within the hook changes, they can cause a component to re-render in order to reflect the new data.\n\nHooks contain reusable code logic that is separate from the component tree.\n\nExamples of hooks include:\n\n- `useState`: \n\n  - The value sent to the `useState` function is the default variable of the state variable\n  - The first value of the return array is the state variable\n  - The second value of the return array is a function that can be used to change the state value\n\n\n"
    },
    {
        "level": 2,
        "id": "zC7DLmEh",
        "title": "React components",
        "text": "React components are parts that make up a user interface (e.g., buttons, lists, heading...).  They allow the re-use of the same structure, which can be populated many time with different sets of data.\n\nReact applications are data-driven. Components are \"vessels\" for data. The value of a React application depends on the data that flows through its components.\n\nA component is created by writing a function, which returns the re-usable part of a user interface.\n\n\n\n\n"
    },
    {
        "level": 3,
        "id": "W0uZ0vW4",
        "title": "Function composition and chaining",
        "text": "Composition is the process of putting smaller, pure functions that perform specific tasks back together, to call them in parallel, or compose them into larger functions. \n\nThe 'dot' notation is often used to allow a function to act on the return value of the previous function. It allows to 'pipe' an input value through a sequence of functions.\n\n"
    },
    {
        "level": 3,
        "id": "GfeBsLyO",
        "title": "Bundling in React applications",
        "text": "Network performance is improved by having to load only one dependency in the browser: the \"bundle\"\n\nBy bundling all the dependencies into a single file, it is possible to load everything with a single HTTP request, avoiding additional latency\n\n`Webpack` is one of the leading React bundling tools.  It takes all different files (JavaScript, CSS, JSX...) and turns them into a single file for improved modularity and performance,\n\n"
    },
    {
        "level": 3,
        "id": "mJPzl1Wc",
        "title": "State in React",
        "text": "The state of a React application is driven by data\n\n- Create stateful components\n- Send state down a component tree\n- Send user interactions back up the component tree\n\n"
    },
    {
        "level": 1,
        "id": "NwHayaCP",
        "title": "Liberalism",
        "text": "Liberalism seeks to provide a way out of the continuous oscillation between authoritarianism and individualism.  It aims to establish a social order that is not built on the basis of irrational dogma, but at the same time, allows for a minimum of stability and order necessary for the preservation and functioning of the state. \n\n"
    },
    {
        "level": 2,
        "id": "GH8W0S4g",
        "title": "Locke",
        "text": "Locke opposed both individualism and the unmitigated submission of the individual to an absolute authority.  According to Russell, this eventually led to doctrines centered around the glorification of the state.\n\n"
    },
    {
        "level": 3,
        "id": "uIHekGyz",
        "title": "Individual freedom vs social cohesion",
        "text": "In human history, as in the history of philosophy, the pursuit of individual freedom has been always in conflict with the quest for social cohesion \n\n- In ancient Greece, social bonds where established, in varying degrees, through the loyalty and duties _of citizens_ towards the state. \n- After the Greeks where conquered by Rome, a more individualistic ethic emerged (e.g., the stoics identified virtue not so much in the relationship between citizens and the State, but between the soul and God).  \n- With Christianity, the idea became more widespread that an individual's duties towards God take precedence over those towards the state. \n- Liberalism provided for a clear separation between the public and the private spheres.  \n- This separation let, on one hand. to romanticism and individualism, and on the other to doctrines centered around the glorification of the state.\n\n\n"
    },
    {
        "level": 3,
        "id": "rJKUGkhR",
        "title": "Origins of Philosophy as a discipline",
        "text": "- Philosophy originated as a discipline independent of theology in Greece around the **6th century BC**, with Thales. In its origins, philosophy and science were not different from each other.\n- With the rise of Christianity and the **end of the Roman empire**, philosophy merged back into theology \n- Philosophy's second \"big era\" between the **11th and 14th centuries** under the rule of the catholic church, and ended with the Reformation.\n- It's third era, from the **17th century to date**, is more influenced by science and secular points of view, although traditional religious influences are still present. \n\n"
    },
    {
        "level": 2,
        "id": "uIHekGyz",
        "title": "Individual freedom vs social cohesion",
        "text": "In human history, as in the history of philosophy, the pursuit of individual freedom has been always in conflict with the quest for social cohesion \n\n- In ancient Greece, social bonds where established, in varying degrees, through the loyalty and duties _of citizens_ towards the state. \n- After the Greeks where conquered by Rome, a more individualistic ethic emerged (e.g., the stoics identified virtue not so much in the relationship between citizens and the State, but between the soul and God).  \n- With Christianity, the idea became more widespread that an individual's duties towards God take precedence over those towards the state. \n- Liberalism provided for a clear separation between the public and the private spheres.  \n- This separation let, on one hand. to romanticism and individualism, and on the other to doctrines centered around the glorification of the state.\n\n\n"
    },
    {
        "level": 3,
        "id": "SLgwJQDz",
        "title": "Conflict between church and state in the Middle Ages",
        "text": "In the conflict between church and state that ensued between the end of the 5th century until middle of the 11th century, the former prevailed. The established philosophy of this era played a key role in this, since although the secular centers of power kept the monopoly of violence and were not bound by a notion of legality, the authority of kings and barons of Germanic descent was dependent on the loyalty and support of their feudal aristocracies, while the church had the monopoly on education and, more importantly, *was believed* to have the keys to everyone's eternal salvation or damnation.\n\n\n\n"
    },
    {
        "level": 3,
        "id": "V2qxeWls",
        "title": "Why study philosophy?",
        "text": "Historian's perspective:\n\n- People's behavior is at least in part determined by their notions of good and evil. \n- \"To understand an era or a people, one has to understand their philosophy.\"\n- There is a two-way causality between human history and the history of philosophy; similarly, one's life situation determines one's own philosophy, and vice-versa.\n\nPersonal perspective:\n\n- Philosophy tries to teach us how to live without being paralyzed by the absence of certainty.\n\n"
    },
    {
        "level": 1,
        "id": "bawDrRBv",
        "title": "Renaissance and Reformation",
        "text": "The Renaissance and the Reformation destroyed the unity of Christianity around the pope's authority.\n- New knowledge about antique cultures became more widespread\n- Copernicus' astronomy gave Man and the Earth a different place in philosophy\n- The belief in eternal regularity was replaced by scientific inquiry, subjectivism and moral relativism\n- With Machiavelli, politics was understood as the naked pursuit of power.\n- The Reformation was a revolt of the Nordic kings and peoples against the pope's dominance.\n- The authority of the pope was countervailed not by one secular emperor, but by a multitude of nation states (and the strengthening of social bonds within them)\n- In the new protestant philosophy, there is no longer a single earthly intermediary between the soul and God\n- The truth does not come from an authority, but is grasped by each individual through reason\n- This led to the advent of anarchism, mysticism. \n- There is no one but many \"protestant philosophies\".\n- Slowly, the seeds are planted for a re-emergence of individualism and pluralism.\n\n"
    },
    {
        "level": 2,
        "id": "kSY5JM8j",
        "title": "Origins of modern philosophy: Descartes and subjectivism",
        "text": "Modern philosophy begins with Descartes, who attempts to derive the knowledge of the external world from the certain knowledge of the existence of one's self and one's own thoughts.\n\nThe resulting philosophical subjectivism--whose origins can be traced back to the protestant opposition to the authority of the pope--goes hand in hand with the emergence of liberalism and a more general opposition to all kinds of secular government and the gradual rise of political anarchism.\n\n"
    },
    {
        "level": 1,
        "id": "tnUhXJcI",
        "title": "The decade of action",
        "text": "In spite of progress made so far in many places, action towards achieving the SDGs by 2030 is not fast or comprehensive enough.  The **Decade of Action** launched by the UN Secreatry-General in September 2019 calls for accelerating sustainable solutions to the world\u2019s biggest challenges on three levels: \n\n- **Global action**: Secure leadership and resources\n- **Local action**: Design and implement policies, budgets, institutions and regulatory frameworks of governments, cities and local authorities\n- **People action**: Mobilize youth, civil society, the media, the private sector, unions, academia and other stakeholders\n\n"
    },
    {
        "level": 2,
        "id": "GQbIN4UB",
        "title": "Role of multi-stakeholder partnerships",
        "text": "Multi-stakeholder partnerships are key to **mobilize support and action** around the 2030 Agenda for Sustainable Development.\n\n*SDG Targets on Multi-stakeholder partnerships:*\n\n- 17.16 Enhance the global partnership for sustainable development, complemented by multi-stakeholder partnerships that mobilize and share knowledge, expertise, technology and financial resources, to support the achievement of the sustainable development goals in all countries, in particular developing countries\n- 17.17 Encourage and promote effective public, public-private and civil society partnerships, building on the experience and resourcing strategies of partnerships\n\nThe [Partnerships for SDGs online platform](https://sustainabledevelopment.un.org/partnerships/) is a global registry of voluntary commitments and multi-stakeholder partnerships maintained by the United Nations to facilitate the global engagement of all stakeholders in the implementation of the 2030 Agenda.\n\n"
    },
    {
        "level": 3,
        "id": "0zGb0Ntw",
        "title": "The 2030 Agenda for Sustainable Development",
        "text": "The 2030 Agenda for Sustainable Development is a call for action by all the member states of the United Nations to end poverty and other deprivations, improve health and education, empower women and girls, reduce inequalities, spur economic growth, strengthen universal peace in larger freedom, realize the human rights of all, tackle climate change, and preserve the environment for current and future generations.  \n\nIt recognizes the eradicating poverty as the greatest global challenge and as an indispensable requirement for sustainable development. \n\nThrough 17 Sustainable Development Goals and 169 targets, all member states committed to take bold and transformative steps to shift the world onto a sustainable and resilient path and to leave no one behind,\n\nThe 2030 Agenda cover 5 Areas of action:\n\n- **People:** End poverty and hunger, and ensure everyone can fulfill their potential in dignity and equality\n\n- **Planet:** Protect the planet from degradation, so it can support the needs of the present and future generations.\n\n- **Prosperity:** Ensure everyone's ability to enjoy prosperous and fulfilling lives and economic, social and technological progress in harmony with nature.\n\n- **Peace:** Foster peaceful, just and inclusive societies, free from fear and violence. \n\n- **Partnership:** Mobilize global solidarity among all people, countries, and stakeholders, focused particularly on the needs of the poorest and most vulnerable\n\n"
    },
    {
        "level": 2,
        "id": "5u1sLb2p",
        "title": "Objectives of data innovation for Sustainable Development",
        "text": "- Data innovation projects should enable countries to *regularly* produce better, more timely and more disaggregated data to inform policies and decisions that contribute to achieving the 2030 Agenda for Sustainable Development\n- Measures of success: Data is available and openly accessible online to policy and decision makers in easy-to-use formats and presentations.\n  - Frequency (at least once a year)\n  - Time lag (less than 2 years)\n  - Geographic coverage (national coverage)\n  - Geographic disaggregation (at least 3rd level)\n  - Coverage of specific population groups (women, disabled, youth, elderly)\n\n"
    },
    {
        "level": 3,
        "id": "jt99xW61",
        "title": "Objectives of Data4Now country projects",
        "text": "The Data4Now initiative aims to provide timely and disaggregated information needed by policy and decision makers to better design development strategies and deliver public policies to achieve the 2030 Development Agenda.  \n\nIt mobilizes resources and partnerships to support countries in the use of innovative technologies, data, and methods to obtain insights on key aspects of sustainable development, such as poverty, food security, education, health, disaster-risk resilience, etc.\n\nIts addresses all aspects of the data lifecycle, from data integration and data engineering to the use of data science methods and data communication and visualization tools.\n\nCollaboration in multi-functional teams covering different areas of expertise allows participating countries to leverage synergies and interlinkages between different domain areas.  For instance: \"How are drought patterns and commodity prices correlated with crop yields, poverty estimates, and population movements?\"\n\nThe initiative promotes the systematic use of sound data quality frameworks and data ethics principles in data innovation projects. \n\n"
    },
    {
        "level": 3,
        "id": "EojLU88D",
        "title": "Global SDG indicator framework",
        "text": "The 17 goals and 169 targets of the 2030 Agenda for Sustainable Development are monitored and reviewed at the global level through an indicator framework developed by the Inter-agency and Expert Group on SDG Indicators. \n\nThe Global SDG indicator framework was initially agreed upon at the 48th session of the United Nations Statistical Commission in March 2017, and adopted by the General Assembly on 6 July 2017 in its [resolution on the Work of the Statistical Commission pertaining to the 2030 Agenda for Sustainable Development (A/RES/71/313)](https://undocs.org/A/RES/71/313).  \n\nAnnual refinements to the Global SDG Indicator Framework, as well as more comprehensive reviews, are submitted by the IAEG-SDGs to the United Nations Statistical Commission for its approval.  \n\nThe 2020 comprehensive review included 36 major changes to the framework in the form of replacements, revisions, additions and deletions to the framework.  The next comprehensive review is scheduled to take place in 2025.\n\n\n"
    },
    {
        "level": 3,
        "id": "oTbmWlt2",
        "title": "Mainstreaming data innovation projects",
        "text": "It is crucial to ensure that the objectives and scope of national data innovation projects fit with the priorities of National Strategies for the Development of Statistics, with the regular work programme of NSOs and with the overall institutional setting of the National Statistical system.\n\nFor instance, there has to be a clear link between project outputs and SDG reporting platforms and VNR processes to ensure sustained demand from policy makers and other key stakeholders.\n\n"
    },
    {
        "level": 2,
        "id": "0zGb0Ntw",
        "title": "The 2030 Agenda for Sustainable Development",
        "text": "The 2030 Agenda for Sustainable Development is a call for action by all the member states of the United Nations to end poverty and other deprivations, improve health and education, empower women and girls, reduce inequalities, spur economic growth, strengthen universal peace in larger freedom, realize the human rights of all, tackle climate change, and preserve the environment for current and future generations.  \n\nIt recognizes the eradicating poverty as the greatest global challenge and as an indispensable requirement for sustainable development. \n\nThrough 17 Sustainable Development Goals and 169 targets, all member states committed to take bold and transformative steps to shift the world onto a sustainable and resilient path and to leave no one behind,\n\nThe 2030 Agenda cover 5 Areas of action:\n\n- **People:** End poverty and hunger, and ensure everyone can fulfill their potential in dignity and equality\n\n- **Planet:** Protect the planet from degradation, so it can support the needs of the present and future generations.\n\n- **Prosperity:** Ensure everyone's ability to enjoy prosperous and fulfilling lives and economic, social and technological progress in harmony with nature.\n\n- **Peace:** Foster peaceful, just and inclusive societies, free from fear and violence. \n\n- **Partnership:** Mobilize global solidarity among all people, countries, and stakeholders, focused particularly on the needs of the poorest and most vulnerable\n\n"
    },
    {
        "level": 3,
        "id": "3QR1WCvN",
        "title": "Sustainable development",
        "text": "Sustainable development is \"development that meets the needs of the present without compromising the ability of future generations to meet their own needs\". \n\n\n"
    },
    {
        "level": 3,
        "id": "6SXNQUaG",
        "title": "Inter-linkages across SDGs",
        "text": "The goals and targets of the 2030 Agenda constitute an \"integrated and indivisible\" whole. All the SDGs are interconnected, and each one of them is crucial for the well-being of individuals and societies.\n\nCross-sectoral coordination requires different actors to understand the complex interdependencies across different goals and targets that result form a multitude of trade-offs and synergies among the economic, social and environmental dimensions of sustainable development.\n\nIt is crucial to understand how does making progress made on a particular target reinforce or thwart progress on other targets. Holistic sustainable development solutions are those that maximize synergies and minimizing mutually off-setting effects between different SDGs.\n\n\n\n\n"
    },
    {
        "level": 1,
        "id": "ClFwmtgA",
        "title": "Data visualization",
        "text": "Exploring data and analytic results in visual form helps identify and communicate conclusions in ways that are easily understood, shared and acted upon.\n\n"
    },
    {
        "level": 2,
        "id": "3c7dZcB6",
        "title": "Communicating results of data innovation projects",
        "text": "**Risk:**\n\n- Outputs from data innovation projects do not reach their intended users, and remain unused by policy and decision makers\n- Key insights and caveats of project results are not presented in formats that are easy to understand by different types of technical and non-technical users \n- Potential users are not aware of project outputs and/or their value to inform policy and decision making\n\n**Mitigation:**\n\n- Communicate clearly and frequently any limitation of the source data and of the applicability of outputs, with a view to avoiding the building of unrealistic expectations\n- Explain possible applications and demonstrate the value of resulrs at an early stage\n- Develop a results communication strategy aimed to maximize the reach of the outputs and their impact. This strategy may include, among other things:\n  - A **social media campaign** to highlight main results and outputs, and direct user to data and resources to understand the underlying methodologies. \n  - **Online dissemination of results** through the official data dissemination platforms of the NSO and participating government agencies, including: \n    - Downloadable **datasets**\n    - Online **maps and visualizations**\n    - Data stories / **narratives** that put data outputs in context and make them easier to understand for policy and decision makers\n  - Production and distribution of project **reports** in multiple formats\n  - User outreach **events**\n\n"
    },
    {
        "level": 3,
        "id": "wFd1FvbC",
        "title": "Ensuring national ownership of data innovation projects",
        "text": "**Risk**: \nExternal partners may be perceived by local teams as exerting too much ownership over the process\n\n**Mitigation**: \n- A specific entity within each country (usually the NSO) should be identified and recognized as the **owner** of every data innovation project. \n- This entity should be responsible to ensure the quality of outputs and should be willing to commit staff and invest resources in the project. \n- The main **focal point** for every country project should be within that entity, and all inquiries should be referred to that focal point first. \n- The first publication of all results should be done by the country, and publications by external partners should make reference to national publications\n\n\n\n"
    },
    {
        "level": 3,
        "id": "Eymkhg9U",
        "title": "Data Communication",
        "text": "\"The goal of effective data communication is to ensure that data is transmitted, decoded, and understood accurately, and acted upon.\"  Describing different spation-temporal trends, identifying patterns through storytelling supported with visual aids is an effective way of communicating data to users\n\n"
    },
    {
        "level": 2,
        "id": "iJY1iH4F",
        "title": "Geospatial data analysis and visualization",
        "text": "Geospatial data analysis and map visualizations can help identify trends, patterns and relationships between different data sets over geographic space.  The analysis and visualization of geospatially disaggregated data \"enhances the ability to understand and respond to place-based factors\" affecting the phenomenon under study.  It also can lead to \"new questions or ideas about relationships in the data that can be further explored with additional methods\". \n\n"
    },
    {
        "level": 3,
        "id": "U3elusdN",
        "title": "Google Earth Engine",
        "text": "Combines a large catalogue of satellite imagery and geospatial datasets with GIS analysis capabilities to detect changes, map trends and quantify differences on Earth surface\n\n"
    },
    {
        "level": 1,
        "id": "URGTzbbu",
        "title": "Measures by WHO and public health authorities to contain COVID-19",
        "text": "WHO and public health authorities across the world are taking measures to contain the COVID19 epidemic.  All sectors of society, including members of the global and national statistical systems, need to act promptly and in a coordinated manner to respond effectively to the crisis and mitigate its human and economic impacts, as well as to usher a rapid and sustainable recovery.\n\n"
    },
    {
        "level": 2,
        "id": "jj13yRW9",
        "title": "COVID19 contingency plans in statistical organizations",
        "text": "- Determine how existing statistical activities, processes and programmes are being affected by the crisis\n- Find alternative means to reach existing data sources (e.g., computer-assisted self interviews with the use of internet and smart phone, or computer assited telephone interviews), or alternative data sources when existing ones are no longer accessible (e.g., switch to use of administrative sources and webscraping).\n- Develop a plan to switch on-site and field processes to a remote/telecommuting setting:\n    - Processes that can be performed remotely by staff using existing infrastructure (e.g., repurposing CAPI software and equipment to support CATI data collection)\n    - Processes that could be performed remotely after digitizing and/or migrating them to cloud environments (e.g., maintenance of public data dissemination portals)\n    - On-site and field processes that would require establishing mechanisms for secure remote access to central databases and systems in order to be performed out of premises (e.g., processing and analysis of confidential microdata records).\n- Identify priority data needs of governments and other stakeholders to respond to the COVID19 crisis\n\n\n"
    },
    {
        "level": 3,
        "id": "mRUJpSCa",
        "title": "Impact of COVID19 pandemic on national statistical offices",
        "text": "The measures to contain the spread of the epidemic in many countries include the requirement for very large parts of the population to stay at home and avoid social contact.  There is a substantial risk that the global health emergency will continue disrupting the normal operations of all sectors of society over several months. \n\nThis is disrupting statistical operations around the world, as the staff of statistical organizations are unable to go to their offices or to the field in order to perform their regular tasks.  The crisis is also making it more difficult to engage with the broader statistical community, as workshops, seminars, expert forums, inter-agency coordination meetings and other public events are cancelled. \n\nThe impact on statistical operations include:\n- Disruptions in data collection\n- Disruption of data processing and analysis workflows\n- Delay in publication of statistical outputs\n- Suspension of user engagement events and staff training activities\n\n\n\n"
    },
    {
        "level": 3,
        "id": "SGskQjpC",
        "title": "Contingency planning needs to be tailored to national circumstances",
        "text": "The response of each National Statistical Office to the COVID-19 crisis needs to be tailored to its own particular circumstances, including the nature and severity of the disruptions caused by the crisis, its existing infrastructure, technical capacities and resources, and other institutional, operational, economic, and socio-cultural factors. \n\n"
    },
    {
        "level": 3,
        "id": "qvOtxtiK",
        "title": "Continuity of statistical programmes: Role of new technologies",
        "text": "It is imperative to leverage innovative technologies and approaches to ensure the continuity of censuses, household surveys, and other major statistical programmes.  This includes making use, to the fullest extent possible, of mobile connectivity, cloud computing, smart mobile devises, and other technological innovations that offer alternative means to ensure that activities around the capturing, validation, processing and dissemination of census and survey data can go on in the new environment of limited mobility of staff and of the population at large. \n\n"
    },
    {
        "level": 3,
        "id": "mRaNOol6",
        "title": "Complexity of census programmes",
        "text": "Population and housing census programmes are complex data collection operations comprising a series of many interrelated activities. They require contacting and collecting information on the whole population of a country within a limited period of time. \n\n"
    },
    {
        "level": 2,
        "id": "mRUJpSCa",
        "title": "Impact of COVID19 pandemic on national statistical offices",
        "text": "The measures to contain the spread of the epidemic in many countries include the requirement for very large parts of the population to stay at home and avoid social contact.  There is a substantial risk that the global health emergency will continue disrupting the normal operations of all sectors of society over several months. \n\nThis is disrupting statistical operations around the world, as the staff of statistical organizations are unable to go to their offices or to the field in order to perform their regular tasks.  The crisis is also making it more difficult to engage with the broader statistical community, as workshops, seminars, expert forums, inter-agency coordination meetings and other public events are cancelled. \n\nThe impact on statistical operations include:\n- Disruptions in data collection\n- Disruption of data processing and analysis workflows\n- Delay in publication of statistical outputs\n- Suspension of user engagement events and staff training activities\n\n\n\n"
    },
    {
        "level": 3,
        "id": "nVgnq2cz",
        "title": "Impact of COVID-19 on census programmes",
        "text": "The COVID-19 crisis creates unprecedented and sudden challenges for countries conducting population and housing censuses and other major statistical operations, disrupting data collection, processing, analysis and dissemination activities carried out by National Statistical Offices, forcing them to rapidly develop and adopt alternative ways of implementing them.\n\n"
    },
    {
        "level": 3,
        "id": "rps4c5Jo",
        "title": "Impact of population mobility restrictions on data collection programmes",
        "text": "In an effort to contain the spread of the COVID-19 epidemic, many governments are imposing severe restrictions on the mobility of the population, disrupting field data collection operations and threatening the ability of National Statistical Offices to deliver high-quality, timely and cost-effective statistical outputs. \n\nThis results in the urgent need to replace current field data collection operations that rely on face-to-face interviews with alternative remote data collection methodologies, such as telephone personal interviewing or paper or web-based self-reporting methods.  \n\nTo respond to this challenge, many countries need to build quickly the necessary capacity to accelerate the implementation of fully digital data collection technologies instead of traditional paper-based methods.\n\n"
    },
    {
        "level": 1,
        "id": "voZHlIkR",
        "title": "Use of Big Data in estimating local poverty statistics",
        "text": "- UN Global Pulse developed a proxy indicator of poverty in Uganda based on the roofing materials as captured in satellite images for a particular district of Gulu in 2012. [link](https://www.unglobalpulse.org/project/measuring-poverty-with-machine-roof-counting/)\n\n- Consumption expenditure and asset wealth were estimated using high-resolution survey and satellite data-based imagery from five African countries (i.e., Nigeria, Tanzania, Uganda, Malawi and Rwanda).  Although the estimation was calculated at the country level, the country was considered part of a larger area (i.e., the African Region). Jean, et al. reported in 2016 that the data used in this estimation are the nighttime lights and daytime satellite images which are classified as Big Data. \n\n"
    },
    {
        "level": 2,
        "id": "vWWASKo8",
        "title": "Additional geo-referenced information used in poverty estimation",
        "text": "- Points of service delivery (e.g., schools, health centers, boreholes...) and their attributes (e.g., number of beds in hospitals; number of health personnel)\n- Networks of infrastructure (e.g., roads, electricity, water...) and their attributes (e.g., condition/quality of roads)\n- Natural features (e.g., elevation, agroclimatic characteristics...)\n\n"
    },
    {
        "level": 3,
        "id": "bs0lGShz",
        "title": "Geo-referenced data",
        "text": "Geo-referenced data provides identifiers of geospatial location for each individual observation\n\nDefining common geographic identifiers allows different datasets to be linked and analyzed together. \n\n"
    },
    {
        "level": 1,
        "id": "47pZOC1r",
        "title": "Product vs. Process Ownership",
        "text": "\"Owning a process\" requires a different perspective than the more familiar \"owning a product\"\n\n"
    },
    {
        "level": 2,
        "id": "3ETVkGDf",
        "title": "Total cost of ownership",
        "text": "The \"total cost of ownership\" is an estimate of the direct cost of developing and applying a particular system.  It includes:\n\n- Software/hardware costs\n- Operational costs (including maintenance of physical infrastructure)\n- Personnel costs (including hiring and training costs of both permanent and temporary staff)\n\n"
    },
    {
        "level": 3,
        "id": "U3iX23IM",
        "title": "National ownership of data innovation projects",
        "text": "National ownership of data innovation projects requires local staff to have the ability and confidence to make the right choices regarding sources, technology and methods in real situations.\n\nCapacity development projects try to make learning easier for NSO staff by prearranging information, sorting it into modules, categories and themes, However, they achieve the opposite if they take away the opportunity to build meaningful connections to their needs and context.  \n\n"
    },
    {
        "level": 1,
        "id": "HZvkseXI",
        "title": "Delivering methodology as a blackbox",
        "text": "Some aspects of data innovation projects are highly technical.  There is a risk of \"delivering methodology as a black box\", making it impossible for local teams to maintain and update the project results in the long run.\n\n"
    },
    {
        "level": 2,
        "id": "1j2ClWxr",
        "title": "Understanding the how and the why",
        "text": "Only after we understand why and how new technologies and methods work, are we able to tweak them for our own needs.\n \n"
    },
    {
        "level": 3,
        "id": "DLdh6sOz",
        "title": "Nothing is more practical than a good theory",
        "text": "Facts need to \"hang together on a latticework of theory\" in order to provide  insights that can be used to systematically solve real-world problems.\n\n"
    },
    {
        "level": 1,
        "id": "UdC6b3Tq",
        "title": "Work in Progress (WIP)",
        "text": "Work in Progress is work that has been started but is not yet finished.  The inventory of work in progress is a critical variable agile management.  The focus should be on reducing idle work and establishing a steady workflow.  \n\nThe cost of change is higher when there is too much work in progress. \n\nThe benefits of working in smaller batches include:\n\n- Smaller amounts of work are waiting to be processed \n- Reduced flow variability\n- Accelerated feedback\n- Less impact of errors\n- Reduced complexity\n- Increased focus and sense of responsibility\n\n"
    },
    {
        "level": 2,
        "id": "c6AuFSdb",
        "title": "Idle work vs idle workers",
        "text": "Finding and eliminating bottlenecks in the workflow is more effective than trying to keep everyone 100% busy.    Idle work is far more wasteful and economically damaging than idle workers.  Work delays (which keep value from being delivered to the customer) can be more expensive than the cost of having a buffer of some sub-utilized staff / team members at any point in time (and according to queue theory, delays grow exponentially once available resources and staff reach 100% utilization).  \n\n"
    },
    {
        "level": 2,
        "id": "IZQ63Vb3",
        "title": "An agile approach to measuring progress",
        "text": "The agile way of measuring progress is by delivering working, validated assets to the customer, not by whether we are proceeding according to a per-defined plan. \n\nFinishing on time and on budget according to a per-specified plan is not a measure of success; the true measure of success is the amount of customer-valuable work that is delivered (value-centric delivery)\n\n\"A project in production can be a failure.  Delivering a product does not guarantee success.\"\n\n"
    },
    {
        "level": 3,
        "id": "F3sZj5cS",
        "title": "Early and continuous integration",
        "text": "Continuous integration is the practice of merging all developers' working copies of the code base to a shared main branch several times a day.\n\nIntegration and testing activities are crucial to verify whether components being developed in multiple parallel tracks meet customer expectations.   Continuous integration is critical to validate assumptions about how different components interface with one another and address any issues early on.\n\n\n"
    },
    {
        "level": 2,
        "id": "FCEfQuXa",
        "title": "Sustainable pace of work",
        "text": "People should be able to work at a pace that they can continue for an extended period of time\n\n"
    },
    {
        "level": 1,
        "id": "MztLTtOj",
        "title": "Cone of uncertainty",
        "text": "At the beginning of a system development project, the team has less information upon which to make decisions, yet that is when some of the most consequential decisions need to be made.  Early decisions about a system's scope and component structure have a major impact on its whole evolution, and are the hardest to reverse at a later stage. \n\n"
    },
    {
        "level": 2,
        "id": "nch0nJ7t",
        "title": "Project risks",
        "text": "**Execution risk**: Risk that designated activities won't be carried out properly\n\n**White space risk**: Risk that some required activities won't be identified in advance\n\n**Integration risk**: Risk that disparate project activities won't come together at the end of the project\n\n"
    },
    {
        "level": 1,
        "id": "GUadXwe7",
        "title": "Statistical organizations' threats and challenges",
        "text": "Threats\n\n- Rigid processes and methods\n  - Process and methodology changes are time consuming and expensive \n  - Inability of statistical frameworks, standards and classifications to remain relevant to modern information needs\n- Inflexible aging technology environments and legacy systems\n- Quickly changing information needs\n- Pressure to reduce provider burden \n- Inability to attract and retain critically skilled staff in a competitive market\n  - The skill-sets that underpin statistical organizations are becoming increasingly valuable in the wider market, making it difficult for statistical organizations to compete to attract and retain these skills.\n\nChallenges:\n\n- Make richer use of existing information sources\n- Increase use of administrative data and alternative data sources\n- Enhance data interoperability across systems\n\n"
    },
    {
        "level": 2,
        "id": "TsYS8WO6",
        "title": "Challenges of introducing new technologies in response to COVID-19",
        "text": "National Statistical Offices are being challenged to introduce telephone-based interviewing and web-based self-reporting techniques at once for many critical data collection operations--such as population and housing, agricultural, and economic censuses, as well as household, business and other types of surveys. In many cases, they need to do it without the benefit of prior experience and with very limited time to conduct detailed analysis and testing of the different alternatives.   \n\nThe introduction of these new technologies is risky and can be expensive. To make an informed decision on the type of technologies best suited to mitigate disruptions to data collection programmes, National Statistical Offices need to take into account their existing infrastructure, technical capacities and resources. \n\n"
    },
    {
        "level": 3,
        "id": "5gf8Ujfa",
        "title": "Testing of new IT systems for data collection",
        "text": "Statistical organizations are under huge pressure to accelerate the process of procuring, developing and deploying new IT systems in order to respond to the challenges of the COVID-19 crisis. \n\nHowever, in order to prevent further disruptions in critical statistical operations, the introduction of new IT systems for data collection, processing and dissemination requires that these systems be thoroughly tested with respect to:\n\n- Functionality\n- Usability\n- Integration\n- Accessibility\n- Security\n- Reliability / stress testing\n\n"
    },
    {
        "level": 3,
        "id": "A23mYrDu",
        "title": "Challenges of remote interviewing and self-reporting",
        "text": "Even National Statistical Offices that have started using electronic data collection approaches, such as computer-assessed personal interviews (CAPI), still rely heavily on personal interviews, Some of the challenges in moving away from face-to-face interviews to remote interviews and self-reporting methodologies include:\n\n- Implementing new mechanisms to identify, contact, authenticate and communicate with respondents\n- Establish mechanisms to geo-locate responses obtained via remote interviewing and self-reporting.\n- Procure/develop and test IT systems to support computer-assisted telephone interviews and online self-reporting portals\n- Implement mechanisms to support respondents in completing online questionnaires\n- Implement mechanisms to support and oversee telephone-based interview workflows\n- Ensure secure remote access to IT systems and secure data exchange.\n\n"
    },
    {
        "level": 2,
        "id": "ENwGKFSE",
        "title": "Modernization of statistical operations",
        "text": "Current methods for data collection and dissemination are not keeping pace with information technologies and increasingly interlinked information systems.\n\n"
    },
    {
        "level": 2,
        "id": "exFglBfe",
        "title": "Challenges in using mobile phone records",
        "text": "- Mobile phone data are typically biased and not representative of the entire population of interest, due to factors such as unequal phone penetration and differences in market share by various carriers\n- Working with mobile phone records requires additional privacy protection measures. \n\n"
    },
    {
        "level": 2,
        "id": "A23mYrDu",
        "title": "Challenges of remote interviewing and self-reporting",
        "text": "Even National Statistical Offices that have started using electronic data collection approaches, such as computer-assessed personal interviews (CAPI), still rely heavily on personal interviews, Some of the challenges in moving away from face-to-face interviews to remote interviews and self-reporting methodologies include:\n\n- Implementing new mechanisms to identify, contact, authenticate and communicate with respondents\n- Establish mechanisms to geo-locate responses obtained via remote interviewing and self-reporting.\n- Procure/develop and test IT systems to support computer-assisted telephone interviews and online self-reporting portals\n- Implement mechanisms to support respondents in completing online questionnaires\n- Implement mechanisms to support and oversee telephone-based interview workflows\n- Ensure secure remote access to IT systems and secure data exchange.\n\n"
    },
    {
        "level": 3,
        "id": "87In6UYV",
        "title": "Selecting alternative data collection approaches - time considerations",
        "text": "Time is a key factor in selecting alternative data collection approaches to ensure the continuity of statistical operations:\n\n- Estimated time necessary to procure/develop, test and deploy technical solutions \n- Estimated time that needs to be spent in training of staff in new skills and the use of new technologies\n\n"
    },
    {
        "level": 3,
        "id": "vIDWWZjJ",
        "title": "Communication strategy with respondents",
        "text": "It is crucial to design and launch as quickly as possible a contact and communication strategy towards respondents in the target population, aimed to maximize high response rates in a new web-based or telephone-based data collection setting.  This include communications soliciting households to complete online questionnaires or to be interviewed by telephone, sending reminders and follow-ups in case of non-response.\n\n"
    },
    {
        "level": 1,
        "id": "LjcB50KJ",
        "title": "Code switching",
        "text": "Code-switching: Ability to rapidly and unconsciously go back and forth between different languages. \n\n\"Different languages ... embody different worldviews and different ways of organizing the world around us.\"\n\nLanguage can subtly affect people's visual perception and sense of time.   For example, bilinguals seem to \"think differently about time, depending on the language context in which they are estimating the duration of events.\"\n\n- English, Swedish and Spanish speakers usually visualize the future as in front of us, and the past as behind us. \n- For Aymara speakers, the future is behind, the past is ahead. \n- For Mandarin speakers, the future is down, the past is up.\n\n\n"
    },
    {
        "level": 2,
        "id": "Upsh1o2P",
        "title": "Perception: Top-down vs Bottom-up",
        "text": "Our perception of the world is \"a virtual reality constructed inside our heads\" \"Perceptions ... are more about what the brain expects to encounter than what is truly there.\"\n\nThe human brain is not just a passive receiver of information, but is constantly making predictions about the world.  When sensory input does not match up with accumulated experience, the brain will often disregard the information it receives from the senses. \n\nThere are two types of perception: \n\n- **Top-down perception**: When we experience the world as the brain expects it to be\n- **Bottom-up perception**: When the brain faithfully represents sensory input it receives. \n\n\"Psychologies Gary Lupyan of the University of Wisconsin and Andy Clark of the University of Edinburgh argue that top-down perception ... is the brain's default method for engaging with the world.  [Only] when the brain isn't confident about its expectations...it depends much more on bottom-up perception.\"\n\nPerception-as-prediction influences all sorts of daily experiences\", from listening to music (music with familiar, predictable patterns is more pleasant) to our interpersonal relationships (we are constantly making predictions about what others will say or do, and only when they behave contrary to our expectations they catch our attention).\n\nLanguage creates expectations that influence our perception of the world.  \n\n"
    },
    {
        "level": 3,
        "id": "SifabErO",
        "title": "Cognitive overload",
        "text": "In cognitive psychology, cognitive load refers to the used amount of working memory resources. (Wikipedia)\n\n- Intrinsic: Associated with a specific topic\n- Extraneous: Related to how information or tasks are presented to a learner\n- Germane:  Related to the creation of a permanent store of knowledge, or a schema.\n\n"
    },
    {
        "level": 1,
        "id": "ifY62dRd",
        "title": "Network formation: preferential attachment",
        "text": "Mechanism that leads to complex network formation, whereby nodes with more edges get even more edges, forming large hubs in the core, surrounded by poorly connected nodes in the periphery. \n\n"
    },
    {
        "level": 2,
        "id": "4xQthRYr",
        "title": "Simple vs Complex networks",
        "text": "- Simple networks have regular nodes and edges. \n- Complex networks have a non-trivial structure that often results from decentralized processes with no central control.\n\n"
    },
    {
        "level": 2,
        "id": "Mv4Ukj5Q",
        "title": "Network formation: transitive closure",
        "text": "Connect two nodes together if they are already connected to a common neighbor\n\n"
    },
    {
        "level": 1,
        "id": "bit4HVlF",
        "title": "Network model of knowledge management",
        "text": "A network model facilitates the management, creation, updating and diffusion of knowledge through digital cross-referencing.\n\n"
    },
    {
        "level": 2,
        "id": "o2aK159Q",
        "title": "Slip box",
        "text": "- The slipbox is a simple external system to organize one's thought, ideas, and collected facts.\n- To be effective, it has to be embedded in one's overarching workflow (daily routine)\n- Video of Prof. Niklas Luhmann: https://www.youtube.com/watch?v=qRSCKSPMuDc&feature=youtu.be&t=37m30s\n\n"
    },
    {
        "level": 3,
        "id": "f6AE5k9M",
        "title": "Types of notes",
        "text": "1. **Fleeting notes**: Sever to capture \"raw\" ideas we come across. They are stored in one place for later processing.  They are only useful if reviewed and turned into proper notes within a day or so.\n2. **Literature notes**: Capture bibliographic details and brief description of sources.\n3. **Permanent notes**: Serve to develop ideas based on fleeting notes and literature notes.  Written in precise, clear, and brief full sentences. They can be understood even outside the context they were taken from.\n4. **Project notes**: Are only relevant to one particular project and can be discarded or archived after the project is finished\n\nTypical mistakes:\n- Treat every note as if it belongs to the \"permanent\" category\n- Collect notes only related to specific projects\n- Just collecting unprocessed fleeting notes\n\n"
    },
    {
        "level": 1,
        "id": "6SkgzEAZ",
        "title": "Iterative development",
        "text": "Teams usually get things wrong before they get them right, and initial results are often poor before they are good.  The number of iterations is not fixed from the beginning. but guided by feedback received after each iteration. \n\n"
    },
    {
        "level": 2,
        "id": "bpltunOR",
        "title": "Transparency in scrum",
        "text": "All relevant information must be available to the people involved in creating the product.  Transparency makes inspection possible, which is needed for adaptation.  Transparency also establishes trust. \n\n"
    },
    {
        "level": 2,
        "id": "onHn6c4S",
        "title": "Iterative abstraction / re-specification",
        "text": "- Innovation requires to combine and re-combine ideas, liberating them from their original context by means of an iterative process of abstraction and re-specification. (Ahrens, 2017, p. 123).\n- Abstraction from concrete situations and re-specification allows to apply ideas from one practical context into another.\n\n"
    },
    {
        "level": 3,
        "id": "5WJqHcE5",
        "title": "Abstraction and scalability",
        "text": "Complexity increases when NSOs seek to mainstream data innovations beyond pilot projects run by a small team.  \"This makes enforcing ETL best practices, upholding data quality, and standardizing workflows increasingly challenging.\" \n\nIdentifying and automating common ETL patterns into standard workflows allows to leverage the power of abstraction in order to address scalability challenges.\n\nThis requires both governance and re-usable technologies\n- repositories (git)\n- notebooks (Jupyter)\n- containers (Docker)\n\n"
    },
    {
        "level": 1,
        "id": "akoVqiRK",
        "title": "Cloud computing for business continuity and disaster recovery",
        "text": "In recent years, cloud computing has become increasingly used by statistical organizations that do not have the hardware or personnel necessary to fully deploy software applications onsite, or that need to test new software tools in the context of pilot data-innovation projects.\n\nToday, cloud computing stands out as a key element of a business continuity and disaster recovery plan for statistical organizations, particularly in the face of the disruption national and global statistical systems caused by the COVID-19 crisis. \n\nIn order to leverage cloud computing solutions for disaster recovery and business continuity, statistical organizations require an IT architecture focused on \"automating as many processes as possible in the event of disaster, ensuring that computing resources are switched over quickly to a stable backup and remain operational.\" (\n\n\n"
    },
    {
        "level": 2,
        "id": "smZXjmav",
        "title": "On-site infrastructure",
        "text": "Many statistical organizations still use server infrastructure located in their own premises to support key functions, such as collecting source data from information providers, giving staff access to data management and data analysis software, and delivering statistical outputs to users.\n\nIf staff is not able to access the software tools and data needed to perform their work, statistical organizations face major disruption in essential workflows. \n\n"
    },
    {
        "level": 1,
        "id": "FOPuLnHI",
        "title": "ETL principles",
        "text": "Principles of good ETL pipelines:\n\n- Partition data tables\n- Load data incrementally\n- Use imutable data tables - so queries return the same result when run against the same business logic and time range\n- Parameterize backfilling logic\n- Run early and frequent data checks: Write data into a staging table first, validate data quality, and only then push to final production table\n- Build alerts and monitoring system\n\n\n"
    },
    {
        "level": 2,
        "id": "Jwlw5emQ",
        "title": "A persistent and immutable staging area",
        "text": "By accumulating and persisting all source data in a stating area (keeping it forever unchanged) one can have shorter retention policy on derived tables, \"knowing that it\u2019s possible to backfill historical data at will.\"\n\n"
    },
    {
        "level": 2,
        "id": "5W8MgJOp",
        "title": "Data Partitioning",
        "text": "**Data Partitioning** - breaking up data into independent, self-contained chunks, instead of storing in a single table or file.\n\nIt is \"a practice that enables more efficient querying and data backfilling\" (Chang, 2018b)\n\n"
    },
    {
        "level": 1,
        "id": "oRq6vy7w",
        "title": "Re-purposing CAPI infrastructure to conduct CATI data collection",
        "text": "National Statistical Offices that have in place a data collection programme based on computer-assisted personal interviews (CAPI) may consider re-purposing the existing software and hardware infrastructure to support computer-assisted telephone interviews (CATI) instead.  This would allow to leverage the existing devices (tablets, personal digital assistants, smart phones or portable computers) as well as their specialized software, including their ability to instantly transmit data over mobile data networks.  \n\nHowever, this re-purposing is not trivial.  For instance, it requires to integrate CATI operations with existing digital mapping and operational management applications built under the assumption that enumerators/interviewers would be entering the data on the same location as the respondent. As interviewers will now be entering the information from a remote location, this creates new challenges for the automatic geo-coding of questionnaire responses and for the supervision of the interview process. \n\n"
    },
    {
        "level": 2,
        "id": "UNMCPlDR",
        "title": "Survey planning",
        "text": "Survey planning should specify:\n\n- Goals and objectives\n- Potential users\n- What decisions the survey is designed to inform\n- What will be the key variables to be estimated \n- What statistical outputs (tabulations and analytic results) will be produced\n- Related and previous surveys\n- Steps taken to prevent unnecessary duplication with other sources\n\n"
    },
    {
        "level": 3,
        "id": "uWf1bH6G",
        "title": "Survey design",
        "text": "- Target population\n- Sampling plan\n- Data collection instrument and methods\n- Realistic time table\n- Cost estimate\n\n"
    },
    {
        "level": 3,
        "id": "XJblYe5c",
        "title": "Survey documentation",
        "text": "The documentation of a survey should help users understand how to:\n\n- Properly analyze the data\n- Replicate the results\n- Evaluate the results\n\n"
    },
    {
        "level": 3,
        "id": "dmERvqVE",
        "title": "Data protection",
        "text": "Data protection refers to:\n\n- Safeguards to ensure that data are handled in a way that avoids disclosure thorughout all the stages of the statistical production process\n- Compliance with confidentiality pledge to respondents\n\n"
    },
    {
        "level": 3,
        "id": "nSlJm34m",
        "title": "Notifying potential survey respondents",
        "text": "Potential survey respondents need to be notified of:\n\n- Survey rationale: Why is the information being collected, and how it will be used?\n\n- Authority: Whether the responses are voluntary or mandatory.\n\n"
    },
    {
        "level": 1,
        "id": "2j4pK36q",
        "title": "Key stakeholders in data innovation projects",
        "text": "In every data innovation project, it is very important to identify from the beginning a broad range of actual and potential stakeholders who may become involved in various phases of the project. This includes:\n\n- **Data providers** (line ministries, land administration, mobile phone companies, industry regulators, business associations, local governments, civil society organizations, space agencies, tech companies...)\n- **Technology providers** (e.g., national data centers, intl. organizations, private sector)\n- **Knowledge and expertise providers** (e.g., international organizations, research institutes, universities...)\n- **Funding providers** (e.g., ministry of planning/finance, intl. donors)\n- **Data users** (e.g., line ministries, parliament, local governments, international organizations, civil society organizations, intl. donors)\n\nIt is also crucial to establish from the beginning both institutional and personal links with these stakeholders, and to involve them early on in the planning and execution of project activities.\n\n"
    },
    {
        "level": 2,
        "id": "4RDMe98t",
        "title": "Technical experts",
        "text": "Data innovation projects need to identify and bring on board technical experts who are deeply knowledgeable of the data inputs, technologies, and methods being pursued, and with ample practical experience in their implementation in the field.\n\nIt is particularly important to partner early on with local experts (e.g., form UN Country Teams, World Bank, government agencies and local universities and think tanks), in order to stimulate interest in the project outcomes and learn more about country needs and priorities. \n\n\n\n"
    },
    {
        "level": 2,
        "id": "UejoKjiC",
        "title": "User engagement in data innovation projects",
        "text": "It is important to build strong ties of collaboration between technical team and the user community. Representatives from key user groups need to be invited to be part of all project activities, from the planning stage on.  \n\n"
    },
    {
        "level": 3,
        "id": "j5lKQ5uZ",
        "title": "Monitoring use of metadata",
        "text": "It is crucial to monitor whether metadata provide the information users and applications need in order to find, access and use content. \n\n- A shrinking user base is an indication that the metadata may not be fit for purpose\n- A widening audience brings new user needs to the fore, which are not always easy to foresee or event o detect. \n\nUsers need to be involved from the beginning in the metadata creation process, in order to ensure that the quality of metadata corresponds to their expectations. \n\n"
    },
    {
        "level": 3,
        "id": "Dyg391KX",
        "title": "Users of statistical data and their needs",
        "text": "Statistical organizations must identify their key audiences / user groups and regularly assess and prioritize their data and information needs. \n\nThe extent to which the needs of key user groups are met is the ultimate measure of every success of statistical organization, and thus needs to be monitored continuously and acted upon.\n\nThe Information and audience model of the UN Economic Commission for Africa identifies various user communities and their data requirements.   Broader audiences require data and information that are more aggregated and summarized (e.g., headline indicators and policy briefs). On the other hand, narrower/specialized audiences require detailed information and disaggregated data. \n\nTypes of audiences:\n\n- Decision makers \n  - Leaders from governmental, international, business, and civil society organizations\n  - Public officials, managers\n- General interest users\n  - The general public\n  - Students\n- Specialists\n  - Analysts\n  - Researchers\n  - Regulators\n\n"
    },
    {
        "level": 2,
        "id": "38O0h16q",
        "title": "Project champions",
        "text": "To be successful, data innovation projects need to identify champions willing to put their names behind them, who can mobilize resources and institutional support, as well as facilitate collaboration across different organizations.\n\nNational champions are especially crucial to help broker collaboration with key government agencies and partners. \n\n"
    },
    {
        "level": 1,
        "id": "rYgSpwzc",
        "title": "Designing for production",
        "text": "Today's data innovation projects require dealing with the complexity of modern production environments, including issues like:\n\n- data virtualization\n- containerization\n- load balance\n- service discovery\n\nA system is said to be **\"production ready\"** when it is ready to be deployed, and to be run by independently by the operations team, facing real-world users and without the intervention of the development team. \n\nDesigning for production means elevating production issues (such as security, runtime control, logging and monitoring, and connectivity) to be \"first class concerns\".  It considers operations teams to be primary users of the system, and seeks to address their needs from the beginning and not as an afterthought. \n\n\"Software delivers its value in production (...) everything before production is prelude\" (Nygard, 2018, p.6)\n\n\n\n"
    },
    {
        "level": 2,
        "id": "QlmZERR1",
        "title": "Caching",
        "text": "Caching can reduce the load on a database server and shorten response times. Some caching good practices include:\n\n- The caches of all application-level caches should have configurable maximum memory usage\n- The hit rates of cached items should be regularly monitored\n- Responses that are easy to generate should not be cached\n- Every cached item should have an invalidation strategy\n\n"
    },
    {
        "level": 1,
        "id": "2XSNmikH",
        "title": "Consistent presentation of data",
        "text": "- Use common labels and definitions across multiple data sets originating from different sources:\n  - If two things have the same label, they must correspond to the same thing\n  - If two things are not the same, they should be labeled differently\n\n"
    },
    {
        "level": 2,
        "id": "R8Mhs6zf",
        "title": "UNdata requirements",
        "text": "**Objective**: Deliver high-quality, relevant and accessible statistical data and analytics to users\n\n**How?**\n\n- Chose the most authoritative, robust, actionable statistical data from the vast universe of possible sources across the UN System and partner organizations\n- Make user interfaces and applications simple and template-driven, explicitly matched to the users' cognitive processing profiles\n- Continuously monitor the accuracy and trustworthiness of the data and analysis provided through the platform\n- Monitor the impact on policy decision making to justify staffing and ongoing expenditures\n- Data must be intuitive and obvious to the data users, not only to the data providers\n- The data's structures and labels should mimic users' though process and vocabulary\n- Users should be able to slice and combine the data in many different ways\n- Applications to access the data should be easy to use\n- Query results should be returned to the users with minimum delay\n- Information should be presented in a consistent manner\n- The system should be adaptable over time\n- Information assets must be protected\n\n"
    },
    {
        "level": 3,
        "id": "E64l7v9D",
        "title": "Monolithic vs decoupled statistical platforms",
        "text": "In the past, statistical data platforms often relied on monolithic architectures, in which a tightly-coupled software system governed all elements of data management, from database access to the rendering data tables and visualizations to the client (and everything in between).  \n\nHowever, the spread of new devises and applications has encouraged many statistical organizations to implement modern server-to-server data exchange and dissemination technologies, in order to flexibly serve the data from multiple back-end systems to an array of customized front-end applications. \n\n"
    },
    {
        "level": 1,
        "id": "qwWcGzwz",
        "title": "De jure vs de facto standards",
        "text": "- **De jure standards** - Standards that \"are usually recognized at international or national level, and ... have achieved a formal approval by means of a standardization process and a standards body.\"\n- **De facto standards** - Standards that \"are adopted by a community because they are useful, simple, extensible, flexible, etc.\"\n\n\"standards only exist as long as they are endorsed by a user community\"\n\n"
    },
    {
        "level": 2,
        "id": "HIjWoYIE",
        "title": "Diffusion of innovations",
        "text": "In order to be adopted, an innovation must be perceived as being simple to use.  Complexity raises the cost of adopting an innovation. \n\nEverest Rogers (*Diffusion of Innovations*) proposes a model to explain the factors that determine whether and how fast an innovation is adopted by society.\n\n"
    },
    {
        "level": 1,
        "id": "WQo5kSrX",
        "title": "Chief Data Officer",
        "text": "- Accountable for the quality of an organization's data\n- Responsible for facilitating data access\n- Tasked with building an efficient data governance\n\n"
    },
    {
        "level": 2,
        "id": "DjuehXHJ",
        "title": "Data security",
        "text": "Data security refers to the safeguards that are in place to ensure that no data is lost, and tha it is not used for unauthorized purposes.  This includes, inter alia:\n\n- User identification and authentication\n- Encryption of data for transmission\n- Access controls to data storage systems\n- Security audits\n\n"
    },
    {
        "level": 1,
        "id": "3BC8Oyhi",
        "title": "Data dissemination policy",
        "text": "Data dissemination is \"the ultimate objective of a statistical system\", and statistical organizations need to develop and implement a coherent data dissemination policy that enables them to \"supply the right data to the right audience in the right format\".   Such data dissemination policy should include:\n\n- The **portfolio of statistical products** that will be targeted at different **user groups**\n- The **media** in which statistical data and information are delivered (e.g., paper-based or digital products and services). 4\n- The **distribution channels** for delivery aof those statistical products an services\n- The appropriate **timing** for release of statistical products\n- The model of communication (e.g., one-to-one vs. one-to-many)\n- The use of **data visualization and communication tools** (e.g., story telling)\n\nThe data dissemination policy needs to be regularly reviewed an updated, to respond to changes in user needs, technology, etc.  To this end, statistical organizations should **monitor the effectiveness of their data dissemination policy** through a set of well defined indicators (e.g., use statistics of online products derived from Google analytics). \n\n"
    },
    {
        "level": 2,
        "id": "CsAV96wd",
        "title": "Proliferation of data portals",
        "text": "A common problem is the proliferation and lack of integration of multiple data portals with similar functionalities. This leads to duplication of work, as well as to confusion among users.  \n\n"
    },
    {
        "level": 2,
        "id": "DgHoUEzL",
        "title": "Statistical data reporting",
        "text": "Reporting by statistical agencies to international monitoring agencies often follows specific reporting requirements and formats.\n\nThe International Monetary Fund's General Data Dissemination System (GDDS) Special Data Dissemination system (SDDS) are standards for National, regional and international reporting.\n\n"
    },
    {
        "level": 2,
        "id": "EcF1VIBW",
        "title": "Public right to access official statistics",
        "text": "\nThe public at large should have access to the same information reported to regional and international agencies\n\n"
    },
    {
        "level": 1,
        "id": "ZiwyShrM",
        "title": "Coordination of National Statistical System",
        "text": "The National Statistical Office plays a key role in coordinating the work of different stakeholders involved in the production, dissemination and use of official statistics in a country, so that their individual and collective actions contribute to deliver high-quality, trusted data that meet the information requirements of all sectors of society.\n\nProper coordination of the NSS requires:\n\n- Clarity with regard to the roles and responsibilities of the various NSS members\n- Mechanisms for sharing knowledge and information, delegating responsibilities, and leveraging comparative advantages among different actors at the local, national, regional and international levels\n- Mechanisms to allow actors from different organizations to collaborate in planning and implementing joint activities around the collection, production, analysis, dissemination and use of official statistics.\n\n\n"
    },
    {
        "level": 2,
        "id": "e1cZF4Rq",
        "title": "Coordination of statistical capacity development in the UN System",
        "text": "The Statistics Division of DESA is the Secretariat of the United Nations Statistical Commission, and as such, its main counterparts at the national level are National Statistical Offices.  \n\nIn addition to this direct channel of communication with NSOs, the Statistics Division regularly engages with the Committee of Coordination of Statistical Activities, the Development Coordination Office, and the UN Resident Coordinator Offices to ensure that its statistical capacity development activities are aligned with those of other UN Agencies at the national, regional and global levels. \n\n"
    },
    {
        "level": 3,
        "id": "JXgQD7eG",
        "title": "Development Coordination Office and Resident Coordinators",
        "text": "How can DCO help in coordination of data work at the country level?\n\n- Providing training to data officers (possible collaboration with DESA)\n- Maintaining a calendar of data-related activities at national level\n- Assess data capabilities of UN country teams through a data score card\n\n\n\n"
    },
    {
        "level": 1,
        "id": "t36XnVjf",
        "title": "Data security: the \"weakest link\" problem",
        "text": "'Security measures are often thwarted by the \"weakest link\" problem: If just one person responds to an attack, it may succeed.'\n\n"
    },
    {
        "level": 2,
        "id": "odZoIUPn",
        "title": "Data security: Preventing phishing",
        "text": "\"Phishing accounts for 90% of all data breaches--but an estimated 30% of fraudulent emails are opened nonetheless.\"\n\nNeed to encourage a reflective, analytic approach to cyber security, instead of simply a rules-based training that may result in automatic, careless decision making. \n\n\"Pause if an email requests action; consider the nature, timing, purpose, and appropriateness of the request; and consult a third party about any suspicions.\"\n\n\n\n\n"
    },
    {
        "level": 1,
        "id": "zBMhHUQc",
        "title": "Apache Hadoop",
        "text": "Open source framework for distributed storage and processing of large datasets across clusters of computers. \n\nIt is often used in the implementation of data lake architectures. \n\nHadoop consists of four main modules:\n\n1. *Hadoop Distributed File System (HDFS)* \u2013 A distributed file system with high fault tolerance and native support of large datasets.\n2. *Yet Another Resource Negotiator (YARN)* \u2013 Used to manage and monitor cluster nodes and resource usage, and to schedule jobs and tasks.\n3. *MapReduce* \u2013 An implementation of the MapReduce programming model for large-scale parallel computation on data. The **map task** takes input data and converts it into a dataset that can be computed in key value pairs. The output of the map task is consumed by **reduce tasks** to aggregate output and provide the desired result.\n4. *Hadoop Common* \u2013 Common Java libraries\n\n"
    },
    {
        "level": 2,
        "id": "sLFsKmFz",
        "title": "Computer cluster",
        "text": "A network of computers that are connected and work together to accomplish the same task. \n\nWikipedia: \"A computer cluster is a set of loosely or tightly connected computers that work together so that, in many respects, they can be viewed as a single system. Unlike grid computers, computer clusters have each node set to perform the same task, controlled and scheduled by software.\"\n\nComputer clusters provide for improved performance, availability and scalability.\n\n"
    },
    {
        "level": 1,
        "id": "uVrsmP2n",
        "title": "Synchronous tasks",
        "text": "Instructions are executed immediately in order.  While each operation is being executed, nothing else is happening. \n\n"
    },
    {
        "level": 2,
        "id": "N24sVqGE",
        "title": "Asynchronous tasks",
        "text": "Their execution does not block the main thread.  \n\n"
    },
    {
        "level": 3,
        "id": "YwXkKMRo",
        "title": "JavaScript promises",
        "text": "Promises help make sense of asynchronous behavior in JavaScript.  \n\nA promise is an objects that represents whether an operation is pending, has been completed, or has failed.\n\n-  Simple promises are handled with `fetch` and `.then()`\n- `Asynch` / `Await` is a special syntax to work with promises in a more comfortable fashion.  \n\n"
    },
    {
        "level": 3,
        "id": "KG4OwoAJ",
        "title": "Recursion",
        "text": "A technique that involve creating a function that recalls itself.\n\n- Often can be used instead of a loop\n- Works well with asynchronous processes (functions can recall themselves when they are ready).\n\n"
    },
    {
        "level": 1,
        "id": "RpDkqWDR",
        "title": "Babel - Tool for JavaScript compilation",
        "text": "Babel converts modern JavaScript code to mode widely compatible code before running it in the browser.  It makes possible to use the latest features of JavaScript right away.\n\nSee (https://babeljs.io/repl)\n\n\n"
    },
    {
        "level": 2,
        "id": "UhAj73Yn",
        "title": "React source code",
        "text": "Source code is the set of files that belong to a project that don't run in the browser. \n\n"
    },
    {
        "level": 1,
        "id": "W5dQVr6P",
        "title": "Tagging",
        "text": "Tagging:  Adding a simple label or series of labels to a document or information resource, in order to classify it and enable users to find documents  of a specific type or related to specific topics. \n\nTagging can be done manually - where a person must reach each document and assign the relevant tags - or automatically - where a computer crawls the contents of the document and automatically suggests tags based upon keywords and other information.\n\n"
    },
    {
        "level": 2,
        "id": "Ben2OQZe",
        "title": "Document Management Lifecycle",
        "text": "Although it\u2019s common to view methodological documents as static items fulfilling a purpose at a single point in time (the time of publication). \n\nStatistical offices often treat the publication of manuals, handbooks, and other methodological documents as the primary objective, while managing the drafting process itself, or the process of accessing and using its contents, is given little thought.\n\nIncreasingly, however, methodological documents are dynamic knowledge assets, whose content needs to be continuously updated and made available through an evolving list of channels, following innovations in Information and Communication Technology. \n\nMost methodological documents share a common end-to-end lifecycle, fulfilling specific needs at each stage.\n\n1. Drafting / Creation\n\n2. Classification/categorization - determining and tagging the contents of the document so that it may be stored appropriately;\n\n3. Storage - migrating the document to a secure storage repository, from which it may be retrieved as needed;\n\n4. Use - Performing any tasks and actions that are required to ensure the document fulfills its intended purpose;\n\n5. Archiving and/or destruction - ensuring that inactive and/or expired documents are either archived or properly destroyed in order to reduce clutter and manage risks.\n\n"
    },
    {
        "level": 1,
        "id": "UXewQVNc",
        "title": "Reading and note-taking",
        "text": "- Reading and thinking are the main task. \n- The goal is to understand and come up with new ideas. \n- The notes are just the tangible outcome.\n\n\"The ability to express understanding in one's own words is a fundamental competency\", the same as the ability \"to distinguish the important bits of a text from the less important ones.\" (Ahrens, 2017, p. 54). \n\n"
    },
    {
        "level": 2,
        "id": "ZyufNNxO",
        "title": "Smart note-taking",
        "text": "According to Ahrens (2017), smart note taking requires:\n- Reading a text with questions in mind and try to relate it to other possible approaches\n- Spotting the limitations of a particular approach\n- Seeing what is *not* mentioned\n- Interpreting particular information within the bigger frame or argument of the text\n- Thinking hard about how the main ideas of the text connect with other ideas from different contexts:  *\"Notes are only as valuable as the ... reference networks they are embedded in.\"* (Ahrens, 2017, p. 108).\n\n"
    },
    {
        "level": 3,
        "id": "FlUbxxop",
        "title": "Be flexible - don't cling to a fixed idea",
        "text": "- Don't cling to an idea if another, more promising gains momentum\n- Follow your interest and always take the path that promises the most insight\n- The more you become interested in something, the more likely it is that you will generate insights from reading, taking notes, and writing about it\n\n"
    },
    {
        "level": 1,
        "id": "jwQ8qCBg",
        "title": "Loss of expertise through staff attrition",
        "text": "**Risk**: \n\nLoss of expertise through staff attrition or turnover\n\n**Mitigation**: \n\n- Maintain focus on institutional capacity rather than on individuals \n- Promote knowledge sharing and team collaboration (e.g., working in pairs and peer reviews)\n\n"
    },
    {
        "level": 2,
        "id": "QRWAHOzF",
        "title": "Working in pairs to ensure sustainable skill building",
        "text": "In the face of high staff turnover rates, organizations face the challenge of retaining over the long term the skills acquired during training activities and hands-on implementation activities. \n\nTo address this challenge, a good practice is to ensure that in every activity related to the implementation of a project feature, at least two team members take the lead and are jointly responsible for the deliverable, and that they work via pairing and review each other's work,\n\n"
    }
]